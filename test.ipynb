{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test llm.py (Llama3 8b, mistral 7b v0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/rech/huiyuche/TREC_iKAT_2024/src/')\n",
    "from llm import LM\n",
    "import importlib\n",
    "importlib.reload(sys.modules['llm'])\n",
    "from llm import LM\n",
    "import gc\n",
    "import torch\n",
    "from accelerate.utils import release_memory\n",
    "\n",
    "def flush():\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rech/huiyuche/envs/trec_ikat/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:13<00:00, 44.37s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the custom cache directory\n",
    "cache_dir = \"/data/rech/huiyuche/huggingface\"\n",
    "\n",
    "# empty the memory\n",
    "flush()\n",
    "# or\n",
    "# release_memory()\n",
    "\n",
    "# llm = LM(\n",
    "#     model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     tokenizer_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     padding_side=\"left\",\n",
    "#     dtype=\"bf16\",\n",
    "#     device_map= \"auto\",\n",
    "#     use_flash_attention_2=False,\n",
    "#     access_token=None,\n",
    "#     cache_dir=cache_dir,\n",
    "#     accelerator = None\n",
    "# )\n",
    "llm = LM(\n",
    "    model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    tokenizer_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    padding_side=\"left\",\n",
    "    dtype=\"bf16\",\n",
    "    device_map= \"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    #use_flash_attention_2=False, (deprecated)\n",
    "    access_token=None,\n",
    "    cache_dir=cache_dir,\n",
    "    accelerator = None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.205310344696045"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bytes_to_giga_bytes(bytes):\n",
    "    return bytes / 1024 / 1024 / 1024\n",
    "\n",
    "bytes_to_giga_bytes(torch.cuda.max_memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(llm.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"what is the capital of France?\"},\n",
    "]\n",
    "response = llm.hf_llm_generate(\n",
    "    messages,\n",
    "    temperature = 1,\n",
    "    top_p = 0.9,\n",
    "    max_new_tokens = 256,\n",
    "    do_sample = True,\n",
    "    num_beams = 1,\n",
    "    num_return_sequences = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The capital city of France is Paris. Paris is the most populous city in France and is known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe. It is also home to numerous world-renowned universities, museums, galleries, and theaters, making it a global cultural hub. Paris is located in the north-central part of France and is surrounded by the Seine River.', 'The capital city of France is Paris. Paris is one of the most famous and visited cities in the world, known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe. Paris is also the political, cultural, and economic center of France.']\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [{\"role\": \"user\", \"content\": \"Who are you?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"what is the capital of Germany?\"}],\n",
    "]\n",
    "outputs = llm.hf_llm_generate_via_pipline(\n",
    "    messages,\n",
    "    temperature = 0.1,\n",
    "    top_p = 1,\n",
    "    max_new_tokens = 256,\n",
    "    do_sample = True,\n",
    "    num_beams = 1,\n",
    "    num_return_sequences = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate creative text, and assist with various tasks.\"}]}, {'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate text, and assist with various tasks.\"}]}], [{'generated_text': [{'role': 'user', 'content': 'what is the capital of Germany?'}, {'role': 'assistant', 'content': ' The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin is home to many famous landmarks, including the Brandenburg Gate, the Reichstag Building, and the Berlin Wall Memorial.'}]}, {'generated_text': [{'role': 'user', 'content': 'what is the capital of Germany?'}, {'role': 'assistant', 'content': \" The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin has been the capital of Germany since the country's reunification in 1990.\"}]}]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate creative text, and assist with various tasks.\",\n",
       "  \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate text, and assist with various tasks.\"],\n",
       " [' The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin is home to many famous landmarks, including the Brandenburg Gate, the Reichstag Building, and the Berlin Wall Memorial.',\n",
       "  \" The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin has been the capital of Germany since the country's reunification in 1990.\"]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[alternative[\"generated_text\"][-1][\"content\"] for alternative in output] for output in outputs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    logging\n",
    ")\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "# Specify the custom cache directory\n",
    "cache_dir = \"/data/rech/huiyuche/huggingface\"\n",
    "\n",
    "\n",
    "\n",
    "# with init_empty_weights():\n",
    "#     model = AutoModelForCausalLM()\n",
    "\n",
    "# model = load_checkpoint_and_dispatch(\n",
    "#     model,\n",
    "#     checkpoint=cache_dir + \"/models--meta-llama--Meta-Llama-3-8B-Instruct\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# print(model)\n",
    "\n",
    "\n",
    "# accelerator = Accelerator()\n",
    "# print(accelerator.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mistral 7b v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\", cache_dir=cache_dir\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\", cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama3 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\", cache_dir=cache_dir\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", cache_dir = cache_dir, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rankllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [06:06<00:00, 183.17s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:04<00:00, 62.43s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.1578)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def get_model(peft_model_name):\n",
    "    config = PeftConfig.from_pretrained(peft_model_name)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, num_labels=1)\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_name)\n",
    "    model = model.merge_and_unload()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "model = get_model('castorini/rankllama-v1-7b-lora-passage')\n",
    "\n",
    "# Define a query-passage pair\n",
    "query = \"What is llama?\"\n",
    "title = \"Llama\"\n",
    "passage = \"The llama is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the pre-Columbian era.\"\n",
    "\n",
    "# Tokenize the query-passage pair\n",
    "inputs = tokenizer(f'query: {query}', f'document: {title} {passage}', return_tensors='pt')\n",
    "\n",
    "# Run the model forward\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    score = logits[0][0]\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [04:52<00:00, 146.37s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:04<00:00, 62.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def get_model(peft_model_name):\n",
    "    config = PeftConfig.from_pretrained(peft_model_name)\n",
    "    base_model = AutoModel.from_pretrained(config.base_model_name_or_path, \n",
    "                                           cache_dir=cache_dir, \n",
    "                                           device_map=\"auto\")\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_name)\n",
    "    model = model.merge_and_unload()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "model = get_model('castorini/repllama-v1-7b-lora-passage')\n",
    "print(model)\n",
    "# Define query and passage inputs\n",
    "# query = \"What is llama?\"\n",
    "# title = \"Llama\"\n",
    "# passage = \"The llama is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the pre-Columbian era.\"\n",
    "# query_input = tokenizer(f'query: {query}</s>', return_tensors='pt')\n",
    "# passage_input = tokenizer(f'passage: {title} {passage}</s>', return_tensors='pt')\n",
    "\n",
    "# # Run the model forward to compute embeddings and query-passage similarity score\n",
    "# with torch.no_grad():\n",
    "#     # compute query embedding\n",
    "#     query_outputs = model(**query_input)\n",
    "#     query_embedding = query_outputs.last_hidden_state[0][-1]\n",
    "#     query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=0)\n",
    "\n",
    "#     # compute passage embedding\n",
    "#     passage_outputs = model(**passage_input)\n",
    "#     passage_embeddings = passage_outputs.last_hidden_state[0][-1]\n",
    "#     passage_embeddings = torch.nn.functional.normalize(passage_embeddings, p=2, dim=0)\n",
    "\n",
    "#     # compute similarity score\n",
    "#     score = torch.dot(query_embedding, passage_embeddings)\n",
    "#     print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jsonl file check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of lines in the file is:  116838987\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counter = 0\n",
    "with open(\"/data/rech/huiyuche/TREC_iKAT_2024/data/collections/ikat_23/cluweb22B_ikat_v2.jsonl\", \"r\") as f:\n",
    "    # count the total number of lines in the file\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        counter += 1\n",
    "        line = f.readline()\n",
    "\n",
    "print(\"the total number of lines in the file is: \", counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/rech/huiyuche/TREC_iKAT_2024/data/collections/ikat_23/cluweb22B_ikat_v2.jsonl\", \"r\") as f:\n",
    "    # count the total number of lines in the file\n",
    "    line = f.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "with open(\"/data/rech/huiyuche/cast23_collection.tsv\", \"r\") as f:\n",
    "    # count the total number of lines in the file\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        counter += 1\n",
    "        line = f.readline()\n",
    "\n",
    "print(\"the total number of lines in the file is: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"clueweb22-en0004-50-00485:0\", \"contents\": \"RACISM AND THE POLICE: The Shooting of Michael Brown in Ferguson - Sociology Toolbox RACISM AND THE POLICE: The Shooting of Michael Brown in Ferguson Todd Beer on August 18, 2014 *For a look at the patterns that have emerged in the police shootings of unarmed black citizens since the shooting of Michael Brown in Ferguson click here. . . . Systematic racism has been made evident again in the shooting of an unarmed young Black man, Michael Brown, by a police officer in Ferguson, Missouri.  Pulling stories directly from recent news headlines is one way to get students’ attention and demonstrate the abundant relevance of the sociological perspective.  The New York Times has a timeline of the events that serves as a useful starting point (from the mainstream media) to share the events with students that may have not kept up with the story. The community of Ferguson, Missouri (the site of the shooting) has responded with on-going mass protests. Ferguson cannot be understood in a vacuum. These events are rich with sociological issues –  inequality and poverty, racial profiling, the militarization of the police, protester and police interaction, social media (#Ferguson and hashtag activism) and the “criminalization of Black male youth”. Looking first at the disproportionate levels of poverty and subsequent exclusion from the economy of many Blacks in the US, Brookings, a Democratic leaning think tank, analyzed census tract data of changes in the poverty rates in Ferguson (and the surrounding area) between 2000 and 2008-2012.   They state: “But Ferguson has also been home to dramatic economic changes in recent years.\", \"url\": \"https://thesocietypages.org/toolbox/racism-police-ferguson/\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the first line\n",
    "\n",
    "with open (\"/data/rech/huiyuche/TREC_iKAT_2024/data/collections/ikat_23/cluweb22B_ikat.jsonl\", \"r\") as f:\n",
    "    line = f.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tsv file check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "['clueweb22-en0022-08-02126:19', 'Personal Requirements: ●', '18 years of age or older. ●', 'Excellent command of the English Language. ●', 'Computer literate. ●', 'Quiet working environment. ●', 'Willing and able to work from home. Technical Requirements: ●', 'Reliable internet connection at home, with download and upload speeds of at least 1 Mbps. ●', 'Laptop or Desktop computer. ●', 'Webcam. ●', 'Headset/Earphones. Benefits: ●', 'Work from Home.', 'https://www.learn4good.com/jobs/language/english/list/teaching/turkey/']\n"
     ]
    }
   ],
   "source": [
    "line = \"clueweb22-en0022-08-02126:19\tPersonal Requirements: ●\t18 years of age or older. ●\tExcellent command of the English Language. ●\tComputer literate. ●\tQuiet working environment. ●\tWilling and able to work from home. Technical Requirements: ●\tReliable internet connection at home, with download and upload speeds of at least 1 Mbps. ●\tLaptop or Desktop computer. ●\tWebcam. ●\tHeadset/Earphones. Benefits: ●\tWork from Home.\thttps://www.learn4good.com/jobs/language/english/list/teaching/turkey/\"\n",
    "\n",
    "splits= line.strip().split('\\t')\n",
    "\n",
    "print(len(splits))\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of tab in a line is 2\n"
     ]
    }
   ],
   "source": [
    "line = \"clueweb22-en0004-50-00485:0\tRACISM AND THE POLICE: The Shooting of Michael Brown in Ferguson - Sociology Toolbox RACISM AND THE POLICE: The Shooting of Michael Brown in Ferguson Todd Beer on August 18, 2014 *For a look at the patterns that have emerged in the police shootings of unarmed black citizens since the shooting of Michael Brown in Ferguson click here. . . . Systematic racism has been made evident again in the shooting of an unarmed young Black man, Michael Brown, by a police officer in Ferguson, Missouri.  Pulling stories directly from recent news headlines is one way to get students’ attention and demonstrate the abundant relevance of the sociological perspective.  The New York Times has a timeline of the events that serves as a useful starting point (from the mainstream media) to share the events with students that may have not kept up with the story. The community of Ferguson, Missouri (the site of the shooting) has responded with on-going mass protests. Ferguson cannot be understood in a vacuum. These events are rich with sociological issues –  inequality and poverty, racial profiling, the militarization of the police, protester and police interaction, social media (#Ferguson and hashtag activism) and the “criminalization of Black male youth”. Looking first at the disproportionate levels of poverty and subsequent exclusion from the economy of many Blacks in the US, Brookings, a Democratic leaning think tank, analyzed census tract data of changes in the poverty rates in Ferguson (and the surrounding area) between 2000 and 2008-2012.   They state: “But Ferguson has also been home to dramatic economic changes in recent years.\thttps://thesocietypages.org/toolbox/racism-police-ferguson/\"\n",
    "print(\"the number of tab in a line is\", sum([1 for c in line if c == \"\\t\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[{'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \"I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my knowledge based on my training data.\\n\\nI'm a large language model, which means I've been trained on a massive dataset of text from various sources, including books, articles, and websites. This training enables me to understand and generate human-like language, allowing me to engage in conversations, answer questions, and even create text based on a given prompt.\\n\\nI'm constantly learning and improving my responses based on the interactions I have with users like you. So, feel free to ask me anything, and I'll do my best to provide a helpful and accurate response!\"}]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[{'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model developed by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a physical presence. I exist solely to process and generate text.\"}]}, {'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate text, and assist with various tasks.\"}]}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[{'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate creative text, and assist with various tasks.\"}]}, {'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate text, and assist with various tasks.\"}]}], [{'generated_text': [{'role': 'user', 'content': 'what is the capital of Germany?'}, {'role': 'assistant', 'content': ' The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin is home to many famous landmarks, including the Brandenburg Gate, the Reichstag Building, and the Berlin Wall Memorial.'}]}, {'generated_text': [{'role': 'user', 'content': 'what is the capital of Germany?'}, {'role': 'assistant', 'content': \" The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin has been the capital of Germany since the country's reunification in 1990.\"}]}]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

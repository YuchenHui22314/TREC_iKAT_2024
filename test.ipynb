{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/rech/huiyuche/TREC_iKAT_2024')\n",
    "import importlib\n",
    "importlib.reload(sys.modules['llm'])\n",
    "from llm import LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rech/huiyuche/envs/trec_ikat/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the custom cache directory\n",
    "cache_dir = \"/data/rech/huiyuche/huggingface\"\n",
    "\n",
    "# llm = LM(\n",
    "#     model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     tokenizer_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     padding_side=\"left\",\n",
    "#     dtype=\"bf16\",\n",
    "#     device_map= \"auto\",\n",
    "#     use_flash_attention_2=False,\n",
    "#     access_token=None,\n",
    "#     cache_dir=cache_dir,\n",
    "#     accelerator = None\n",
    "# )\n",
    "llm = LM(\n",
    "    model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    tokenizer_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    padding_side=\"left\",\n",
    "    dtype=\"bf16\",\n",
    "    device_map= \"auto\",\n",
    "    use_flash_attention_2=False,\n",
    "    access_token=None,\n",
    "    cache_dir=cache_dir,\n",
    "    accelerator = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(llm.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "outputs, response = llm.hf_llm_generate(\n",
    "    messages,\n",
    "    temperature = 0.1,\n",
    "    top_p = 1,\n",
    "    max_new_tokens = 256,\n",
    "    do_sample = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 75])\n",
      "tensor([[    1,   733, 16289, 28793,  6526,   460,   368, 28804,   733, 28748,\n",
      "         16289, 28793,   315,   837,   264,  2475,  3842,  2229,  6202,   486,\n",
      "         25200,  1650, 16107, 28723,   315,   837,  5682,   298,  8270,  2930,\n",
      "         28733,  4091,  2245,  2818,   356,   272,  2787,   315,  5556, 28723,\n",
      "           315,   949, 28742, 28707,   506,   272,  5537,   298,   506,  9388,\n",
      "         28725, 13855, 28725,   442,   264, 13355, 28723,   315,  2588,   298,\n",
      "          1316,  4372,  4224, 28725,  8270,  9811,  3036, 28725,   304,  6031,\n",
      "           395,  4118,  9796, 28723,     2]], device='cuda:0')\n",
      "I am a large language model developed by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate creative content, and assist with various tasks.\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "print(outputs)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [{\"role\": \"user\", \"content\": \"Who are you?\"}]\n",
    "]\n",
    "outputs = llm.hf_llm_generate_via_pipline(\n",
    "    messages,\n",
    "    temperature = 0.1,\n",
    "    top_p = 1,\n",
    "    max_new_tokens = 256,\n",
    "    do_sample = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" I am a large language model developed by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate creative content, and assist with various tasks.\"]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    logging\n",
    ")\n",
    "\n",
    "\n",
    "# Specify the custom cache directory\n",
    "cache_dir = \"/data/rech/huiyuche/huggingface\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\", cache_dir=cache_dir\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", cache_dir = cache_dir, device_map=\"auto\")\n",
    "\n",
    "# with init_empty_weights():\n",
    "#     model = AutoModelForCausalLM()\n",
    "\n",
    "# model = load_checkpoint_and_dispatch(\n",
    "#     model,\n",
    "#     checkpoint=cache_dir + \"/models--meta-llama--Meta-Llama-3-8B-Instruct\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# print(model)\n",
    "print(model)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "print(accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[{'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \"I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my knowledge based on my training data.\\n\\nI'm a large language model, which means I've been trained on a massive dataset of text from various sources, including books, articles, and websites. This training enables me to understand and generate human-like language, allowing me to engage in conversations, answer questions, and even create text based on a given prompt.\\n\\nI'm constantly learning and improving my responses based on the interactions I have with users like you. So, feel free to ask me anything, and I'll do my best to provide a helpful and accurate response!\"}]}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

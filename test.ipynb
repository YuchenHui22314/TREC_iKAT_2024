{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test llm.py (Llama3 8b, mistral 7b v0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/rech/huiyuche/TREC_iKAT_2024/src/')\n",
    "from llm import LM\n",
    "import importlib\n",
    "importlib.reload(sys.modules['llm'])\n",
    "from llm import LM\n",
    "import gc\n",
    "import torch\n",
    "from accelerate.utils import release_memory\n",
    "\n",
    "def flush():\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rech/huiyuche/envs/trec_ikat/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:13<00:00, 44.37s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the custom cache directory\n",
    "cache_dir = \"/data/rech/huiyuche/huggingface\"\n",
    "\n",
    "# empty the memory\n",
    "flush()\n",
    "# or\n",
    "# release_memory()\n",
    "\n",
    "# llm = LM(\n",
    "#     model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     tokenizer_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     padding_side=\"left\",\n",
    "#     dtype=\"bf16\",\n",
    "#     device_map= \"auto\",\n",
    "#     use_flash_attention_2=False,\n",
    "#     access_token=None,\n",
    "#     cache_dir=cache_dir,\n",
    "#     accelerator = None\n",
    "# )\n",
    "llm = LM(\n",
    "    model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    tokenizer_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    padding_side=\"left\",\n",
    "    dtype=\"bf16\",\n",
    "    device_map= \"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    #use_flash_attention_2=False, (deprecated)\n",
    "    access_token=None,\n",
    "    cache_dir=cache_dir,\n",
    "    accelerator = None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.205310344696045"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bytes_to_giga_bytes(bytes):\n",
    "    return bytes / 1024 / 1024 / 1024\n",
    "\n",
    "bytes_to_giga_bytes(torch.cuda.max_memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(llm.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"what is the capital of France?\"},\n",
    "]\n",
    "response = llm.hf_llm_generate(\n",
    "    messages,\n",
    "    temperature = 1,\n",
    "    top_p = 0.9,\n",
    "    max_new_tokens = 256,\n",
    "    do_sample = True,\n",
    "    num_beams = 1,\n",
    "    num_return_sequences = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The capital city of France is Paris. Paris is the most populous city in France and is known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe. It is also home to numerous world-renowned universities, museums, galleries, and theaters, making it a global cultural hub. Paris is located in the north-central part of France and is surrounded by the Seine River.', 'The capital city of France is Paris. Paris is one of the most famous and visited cities in the world, known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe. Paris is also the political, cultural, and economic center of France.']\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [{\"role\": \"user\", \"content\": \"Who are you?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"what is the capital of Germany?\"}],\n",
    "]\n",
    "outputs = llm.hf_llm_generate_via_pipline(\n",
    "    messages,\n",
    "    temperature = 0.1,\n",
    "    top_p = 1,\n",
    "    max_new_tokens = 256,\n",
    "    do_sample = True,\n",
    "    num_beams = 1,\n",
    "    num_return_sequences = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate creative text, and assist with various tasks.\"}]}, {'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate text, and assist with various tasks.\"}]}], [{'generated_text': [{'role': 'user', 'content': 'what is the capital of Germany?'}, {'role': 'assistant', 'content': ' The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin is home to many famous landmarks, including the Brandenburg Gate, the Reichstag Building, and the Berlin Wall Memorial.'}]}, {'generated_text': [{'role': 'user', 'content': 'what is the capital of Germany?'}, {'role': 'assistant', 'content': \" The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin has been the capital of Germany since the country's reunification in 1990.\"}]}]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate creative text, and assist with various tasks.\",\n",
       "  \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate text, and assist with various tasks.\"],\n",
       " [' The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin is home to many famous landmarks, including the Brandenburg Gate, the Reichstag Building, and the Berlin Wall Memorial.',\n",
       "  \" The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin has been the capital of Germany since the country's reunification in 1990.\"]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[alternative[\"generated_text\"][-1][\"content\"] for alternative in output] for output in outputs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    logging\n",
    ")\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "# Specify the custom cache directory\n",
    "cache_dir = \"/data/rech/huiyuche/huggingface\"\n",
    "\n",
    "\n",
    "\n",
    "# with init_empty_weights():\n",
    "#     model = AutoModelForCausalLM()\n",
    "\n",
    "# model = load_checkpoint_and_dispatch(\n",
    "#     model,\n",
    "#     checkpoint=cache_dir + \"/models--meta-llama--Meta-Llama-3-8B-Instruct\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# print(model)\n",
    "\n",
    "\n",
    "# accelerator = Accelerator()\n",
    "# print(accelerator.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mistral 7b v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\", cache_dir=cache_dir\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\", cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama3 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\", cache_dir=cache_dir\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", cache_dir = cache_dir, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rankllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [06:06<00:00, 183.17s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:04<00:00, 62.43s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.1578)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def get_model(peft_model_name):\n",
    "    config = PeftConfig.from_pretrained(peft_model_name)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, num_labels=1)\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_name)\n",
    "    model = model.merge_and_unload()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "model = get_model('castorini/rankllama-v1-7b-lora-passage')\n",
    "\n",
    "# Define a query-passage pair\n",
    "query = \"What is llama?\"\n",
    "title = \"Llama\"\n",
    "passage = \"The llama is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the pre-Columbian era.\"\n",
    "\n",
    "# Tokenize the query-passage pair\n",
    "inputs = tokenizer(f'query: {query}', f'document: {title} {passage}', return_tensors='pt')\n",
    "\n",
    "# Run the model forward\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    score = logits[0][0]\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [04:52<00:00, 146.37s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:04<00:00, 62.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def get_model(peft_model_name):\n",
    "    config = PeftConfig.from_pretrained(peft_model_name)\n",
    "    base_model = AutoModel.from_pretrained(config.base_model_name_or_path, \n",
    "                                           cache_dir=cache_dir, \n",
    "                                           device_map=\"auto\")\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_name)\n",
    "    model = model.merge_and_unload()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "model = get_model('castorini/repllama-v1-7b-lora-passage')\n",
    "print(model)\n",
    "# Define query and passage inputs\n",
    "# query = \"What is llama?\"\n",
    "# title = \"Llama\"\n",
    "# passage = \"The llama is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the pre-Columbian era.\"\n",
    "# query_input = tokenizer(f'query: {query}</s>', return_tensors='pt')\n",
    "# passage_input = tokenizer(f'passage: {title} {passage}</s>', return_tensors='pt')\n",
    "\n",
    "# # Run the model forward to compute embeddings and query-passage similarity score\n",
    "# with torch.no_grad():\n",
    "#     # compute query embedding\n",
    "#     query_outputs = model(**query_input)\n",
    "#     query_embedding = query_outputs.last_hidden_state[0][-1]\n",
    "#     query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=0)\n",
    "\n",
    "#     # compute passage embedding\n",
    "#     passage_outputs = model(**passage_input)\n",
    "#     passage_embeddings = passage_outputs.last_hidden_state[0][-1]\n",
    "#     passage_embeddings = torch.nn.functional.normalize(passage_embeddings, p=2, dim=0)\n",
    "\n",
    "#     # compute similarity score\n",
    "#     score = torch.dot(query_embedding, passage_embeddings)\n",
    "#     print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clueweb22-en0000-00-00251:12': 2, 'clueweb22-en0000-06-17856:4': 3, 'clueweb22-en0000-78-08540:4': 3, 'clueweb22-en0000-84-18471:16': 2, 'clueweb22-en0001-53-12566:3': 1, 'clueweb22-en0002-40-16532:0': 1}\n"
     ]
    }
   ],
   "source": [
    "import pytrec_eval\n",
    "qrel_file_path = \"/data/rech/huiyuche/TREC_iKAT_2024/data/qrels/toy_qrel.txt\"\n",
    "with open(qrel_file_path, 'r') as f_qrel:\n",
    "    qrel_ndcg = pytrec_eval.parse_qrel(f_qrel)\n",
    "print(qrel_ndcg[\"9-1-3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clueweb22-en0000-00-00251:12': 1, 'clueweb22-en0000-06-17856:4': 1, 'clueweb22-en0000-78-08540:4': 1, 'clueweb22-en0000-84-18471:16': 1, 'clueweb22-en0001-53-12566:3': 1, 'clueweb22-en0002-40-16532:0': 1}\n"
     ]
    }
   ],
   "source": [
    "qrel = {\n",
    "        qid: {docid: 1 if rel_score > 0 else 0 for docid, rel_score in qrel_ndcg[qid].items()} for qid in qrel_ndcg.keys()\n",
    "    }\n",
    "print(qrel[\"9-1-3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9-1-3': {'map': 1.0, 'ndcg_cut_5': 1.0}}\n",
      "{'9-1-3': {'map': 1.0, 'ndcg_cut_5': 0.9299817480463239}}\n"
     ]
    }
   ],
   "source": [
    "# read ranking list\n",
    "ranking_list_path = \"/data/rech/huiyuche/TREC_iKAT_2024/results/test/toy_ranking_list.txt\"\n",
    "with open(ranking_list_path, 'r') as f_run:\n",
    "    run = pytrec_eval.parse_run(f_run)\n",
    "\n",
    "# pytrec_eval eval\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrel, {\"map\", \"ndcg_cut.5\"})\n",
    "res = evaluator.evaluate(run)\n",
    "print(res)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrel_ndcg, {\"map\", \"ndcg_cut.5\"})\n",
    "res = evaluator.evaluate(run)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 1000, 'b': 2}, {'a': 100, 'b': 5}, {'a': 4, 'b': 6}]\n"
     ]
    }
   ],
   "source": [
    "dict_a = {'a': 1, 'b': 2}\n",
    "dict_b = {'a': 3, 'b': 5}\n",
    "dict_c = {'a': 4, 'b': 6}\n",
    "list_abc = [dict_a, dict_b, dict_c]\n",
    "sorted_abc = sorted(list_abc[0:2], key=lambda x: x['a'], reverse=True)\n",
    "sorted_abc[0]['a'] = 100\n",
    "sorted_abc[1]['a'] = 1000\n",
    "print(list_abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'9-1-3': {'map': 0.4666666666666666, 'ndcg_cut_5': 0.4776237035032179}}\n",
    "{'9-1-3': {'map': 0.4666666666666666, 'ndcg_cut_5': 0.4776237035032179}}\n",
    "\n",
    "'9-1-3': {'map': 1.0, 'ndcg_cut_5': 1.0}}\n",
    "{'9-1-3': {'map': 1.0, 'ndcg_cut_5': 0.9299817480463239}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jsonl file check, average passage length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_passsage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# if num_passage > 1000000:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[1;32m     21\u001b[0m average_len \u001b[38;5;241m=\u001b[39m total_passage_len \u001b[38;5;241m/\u001b[39m num_passage\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe average length for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnum_passsage\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m passages is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, average_len)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_passsage' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "collection_path = \"/part/01/Tmp/yuchen/ikat_official_collectioin/ikat_2023_passages_07.jsonl\"\n",
    "# collection_path = \"/data/rech/huiyuche/TREC_iKAT_2024/data/collections/ikat_23/cluweb22B_ikat_v2.jsonl\" \n",
    "\n",
    "num_passage = 0\n",
    "total_passage_len = 0\n",
    "with open(collection_path, \"r\") as f:\n",
    "    # count the total number of lines in the file\n",
    "    while True:\n",
    "        try:\n",
    "            jsonl_dict = json.loads(f.readline())\n",
    "            num_passage += 1\n",
    "            total_passage_len += len(jsonl_dict[\"contents\"].split())\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        # if num_passage > 1000000:\n",
    "        #     break\n",
    "\n",
    "\n",
    "average_len = total_passage_len / num_passage\n",
    "print(f\"the average length for {num_passage} passages is: \", average_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average length for 7333325 passages is:  201.22463630072306\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"the average length for {num_passage} passages is: \", average_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_path = \"/data/rech/huiyuche/TREC_iKAT_2024/data/collections/ikat_23/cluweb22B_ikat_v2.jsonl\" \n",
    "with open(collection_path, \"r\") as f:\n",
    "    # count the total number of lines in the file\n",
    "    line = f.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "with open(\"/data/rech/huiyuche/cast23_collection.tsv\", \"r\") as f:\n",
    "    # count the total number of lines in the file\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        counter += 1\n",
    "        line = f.readline()\n",
    "\n",
    "print(\"the total number of lines in the file is: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"clueweb22-en0004-50-00485:0\", \"contents\": \"RACISM AND THE POLICE: The Shooting of Michael Brown in Ferguson - Sociology Toolbox RACISM AND THE POLICE: The Shooting of Michael Brown in Ferguson Todd Beer on August 18, 2014 *For a look at the patterns that have emerged in the police shootings of unarmed black citizens since the shooting of Michael Brown in Ferguson click here. . . . Systematic racism has been made evident again in the shooting of an unarmed young Black man, Michael Brown, by a police officer in Ferguson, Missouri.  Pulling stories directly from recent news headlines is one way to get students’ attention and demonstrate the abundant relevance of the sociological perspective.  The New York Times has a timeline of the events that serves as a useful starting point (from the mainstream media) to share the events with students that may have not kept up with the story. The community of Ferguson, Missouri (the site of the shooting) has responded with on-going mass protests. Ferguson cannot be understood in a vacuum. These events are rich with sociological issues –  inequality and poverty, racial profiling, the militarization of the police, protester and police interaction, social media (#Ferguson and hashtag activism) and the “criminalization of Black male youth”. Looking first at the disproportionate levels of poverty and subsequent exclusion from the economy of many Blacks in the US, Brookings, a Democratic leaning think tank, analyzed census tract data of changes in the poverty rates in Ferguson (and the surrounding area) between 2000 and 2008-2012.   They state: “But Ferguson has also been home to dramatic economic changes in recent years.\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the first line\n",
    "\n",
    "with open (\"/data/rech/huiyuche/TREC_iKAT_2024/data/collections/ikat_23/cluweb22B_ikat_v2.jsonl\", \"r\") as f:\n",
    "    line = f.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"id\": \"clueweb22-en0004-50-00485:0\", \"contents\": \"RACISM AND THE POLICE: The Shooting of Michael Brown in Ferguson - Sociology Toolbox RACISM AND THE POLICE: The Shooting of Michael Brown in Ferguson Todd Beer on August 18, 2014 *For a look at the patterns that have emerged in the police shootings of unarmed black citizens since the shooting of Michael Brown in Ferguson click here. . . . Systematic racism has been made evident again in the shooting of an unarmed young Black man, Michael Brown, by a police officer in Ferguson, Missouri.  Pulling stories directly from recent news headlines is one way to get students’ attention and demonstrate the abundant relevance of the sociological perspective.  The New York Times has a timeline of the events that serves as a useful starting point (from the mainstream media) to share the events with students that may have not kept up with the story. The community of Ferguson, Missouri (the site of the shooting) has responded with on-going mass protests. Ferguson cannot be understood in a vacuum. These events are rich with sociological issues –  inequality and poverty, racial profiling, the militarization of the police, protester and police interaction, social media (#Ferguson and hashtag activism) and the “criminalization of Black male youth”. Looking first at the disproportionate levels of poverty and subsequent exclusion from the economy of many Blacks in the US, Brookings, a Democratic leaning think tank, analyzed census tract data of changes in the poverty rates in Ferguson (and the surrounding area) between 2000 and 2008-2012.   They state: “But Ferguson has also been home to dramatic economic changes in recent years.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tsv file check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "['clueweb22-en0022-08-02126:19', 'Personal Requirements: ●', '18 years of age or older. ●', 'Excellent command of the English Language. ●', 'Computer literate. ●', 'Quiet working environment. ●', 'Willing and able to work from home. Technical Requirements: ●', 'Reliable internet connection at home, with download and upload speeds of at least 1 Mbps. ●', 'Laptop or Desktop computer. ●', 'Webcam. ●', 'Headset/Earphones. Benefits: ●', 'Work from Home.', 'https://www.learn4good.com/jobs/language/english/list/teaching/turkey/']\n"
     ]
    }
   ],
   "source": [
    "line = \"clueweb22-en0022-08-02126:19\tPersonal Requirements: ●\t18 years of age or older. ●\tExcellent command of the English Language. ●\tComputer literate. ●\tQuiet working environment. ●\tWilling and able to work from home. Technical Requirements: ●\tReliable internet connection at home, with download and upload speeds of at least 1 Mbps. ●\tLaptop or Desktop computer. ●\tWebcam. ●\tHeadset/Earphones. Benefits: ●\tWork from Home.\thttps://www.learn4good.com/jobs/language/english/list/teaching/turkey/\"\n",
    "\n",
    "splits= line.strip().split('\\t')\n",
    "\n",
    "print(len(splits))\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of tab in a line is 2\n"
     ]
    }
   ],
   "source": [
    "line = \"clueweb22-en0004-50-00485:0\tRACISM AND THE POLICE: The Shooting of Michael Brown in Ferguson - Sociology Toolbox RACISM AND THE POLICE: The Shooting of Michael Brown in Ferguson Todd Beer on August 18, 2014 *For a look at the patterns that have emerged in the police shootings of unarmed black citizens since the shooting of Michael Brown in Ferguson click here. . . . Systematic racism has been made evident again in the shooting of an unarmed young Black man, Michael Brown, by a police officer in Ferguson, Missouri.  Pulling stories directly from recent news headlines is one way to get students’ attention and demonstrate the abundant relevance of the sociological perspective.  The New York Times has a timeline of the events that serves as a useful starting point (from the mainstream media) to share the events with students that may have not kept up with the story. The community of Ferguson, Missouri (the site of the shooting) has responded with on-going mass protests. Ferguson cannot be understood in a vacuum. These events are rich with sociological issues –  inequality and poverty, racial profiling, the militarization of the police, protester and police interaction, social media (#Ferguson and hashtag activism) and the “criminalization of Black male youth”. Looking first at the disproportionate levels of poverty and subsequent exclusion from the economy of many Blacks in the US, Brookings, a Democratic leaning think tank, analyzed census tract data of changes in the poverty rates in Ferguson (and the surrounding area) between 2000 and 2008-2012.   They state: “But Ferguson has also been home to dramatic economic changes in recent years.\thttps://thesocietypages.org/toolbox/racism-police-ferguson/\"\n",
    "print(\"the number of tab in a line is\", sum([1 for c in line if c == \"\\t\"]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test general python operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yuchen\n",
      "abc\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "b = 3\n",
    "name = 5\n",
    "def b():\n",
    "    a()\n",
    "\n",
    "def a():\n",
    "    print(name)\n",
    "    print(\"abc\")\n",
    "\n",
    "\n",
    "name = \"yuchen\"\n",
    "b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[{'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \"I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my knowledge based on my training data.\\n\\nI'm a large language model, which means I've been trained on a massive dataset of text from various sources, including books, articles, and websites. This training enables me to understand and generate human-like language, allowing me to engage in conversations, answer questions, and even create text based on a given prompt.\\n\\nI'm constantly learning and improving my responses based on the interactions I have with users like you. So, feel free to ask me anything, and I'll do my best to provide a helpful and accurate response!\"}]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[{'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model developed by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a physical presence. I exist solely to process and generate text.\"}]}, {'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate text, and assist with various tasks.\"}]}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[{'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate creative text, and assist with various tasks.\"}]}, {'generated_text': [{'role': 'user', 'content': 'Who are you?'}, {'role': 'assistant', 'content': \" I am a large language model trained by Mistral AI. I am designed to generate human-like text based on the input I receive. I don't have the ability to have feelings, emotions, or a personality. I exist to help answer questions, generate text, and assist with various tasks.\"}]}], [{'generated_text': [{'role': 'user', 'content': 'what is the capital of Germany?'}, {'role': 'assistant', 'content': ' The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin is home to many famous landmarks, including the Brandenburg Gate, the Reichstag Building, and the Berlin Wall Memorial.'}]}, {'generated_text': [{'role': 'user', 'content': 'what is the capital of Germany?'}, {'role': 'assistant', 'content': \" The capital city of Germany is Berlin. Berlin is one of the 16 federal states of Germany and is located in the northeastern part of the country. It is the most populous city in Germany and is known for its rich history, diverse culture, and vibrant arts scene. Berlin has been the capital of Germany since the country's reunification in 1990.\"}]}]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test topics.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/rech/huiyuche/TREC_iKAT_2024/src/')\n",
    "from topics import (\n",
    "    Turn, \n",
    "    load_turns_from_ikat_topic_files, \n",
    "    save_turns_to_json, \n",
    "    load_turns_from_json,\n",
    "    Result,\n",
    "    Reformulation,\n",
    "    get_context_by_qid\n",
    ")\n",
    "from dataclasses import asdict\n",
    "import importlib\n",
    "importlib.reload(sys.modules['topics'])\n",
    "from topics import Turn, load_turns_from_ikat_topic_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_turns = load_turns_from_ikat_topic_files(ikat_topic_file = \"/data/rech/huiyuche/TREC_iKAT_2024/data/topics/2023_ikat_test_topics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test get_context_by_qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = get_context_by_qid(\"9-1-3\", list_of_turns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test if dataclasses.asdict works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_of_turns_dict = [asdict(turn) for turn in list_of_turns]\n",
    "print(lists_of_turns_dict)\n",
    "# save_to_json\n",
    "save_turns_to_json(list_of_turns, \"/data/rech/huiyuche/TREC_iKAT_2024/test/ikat_2023_test.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test if load_turns_from_json works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_turns = load_turns_from_json(\"/data/rech/huiyuche/TREC_iKAT_2024/test/2023_ikat_test_topics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test add_reformulation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn ID: 9-1-1\n",
      "Title: Finding a diet\n",
      "Current Utterance: Can you help me find a diet for myself?\n",
      "Oracle Utterance: Can you help me find a diet for myself considering that I'm vegetarian, allergic to soybeans, lactose intolerant, can't exercise too much, and should drink water regularly?\n",
      "Number of reformulations: 0\n",
      "Number of results: 0\n",
      "\n",
      "Turn ID: 9-1-1\n",
      "Title: Finding a diet\n",
      "Current Utterance: Can you help me find a diet for myself?\n",
      "Oracle Utterance: Can you help me find a diet for myself considering that I'm vegetarian, allergic to soybeans, lactose intolerant, can't exercise too much, and should drink water regularly?\n",
      "Number of reformulations: 1\n",
      "Number of results: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turn = list_of_turns[0]\n",
    "print(turn)\n",
    "turn.add_reformulation(reformulated_query=\"emm\",reformulation_name=\"test_name\")\n",
    "reformulation = turn.find_reformulation(\"test_name\")\n",
    "turn.add_result(\n",
    "    collection = \"clueweb12B\",\n",
    "    retrieval_model = \"BM25\",\n",
    "    reranker = \"rankllama\",\n",
    "    retrieval_query_type = \"a\",\n",
    "    reranking_query_type = \"b\",\n",
    "    generation_query_type = \"c\", \n",
    "    metrics_dict = {\n",
    "        \"map\": 0.5,\n",
    "        \"ndcg_cut_10\":0.5\n",
    "    }\n",
    ")\n",
    "print(turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'turn_id': '9-1-1', 'conversation_id': '9-1', 'title': 'Finding a diet', 'current_utterance': 'Can you help me find a diet for myself?', 'current_response': 'Sure, these diets fit your condition and preference: 1) Vegan Mediterranean combines the principles of the Mediterranean diet with veganism and emphasizes whole grains, vegetables, beans, nuts, seeds, herbs, and spices. It excludes animal foods while incorporating plant-based alternatives, offering a sustainable and compassionate approach. (2) Vegan Keto: is a high-fat, low-carbohydrate, and moderate-protein diet which is also exclusively plant-based. It suits individuals following a vegan lifestyle who wish to adopt a ketogenic eating plan. (3) Eco-Atkins is a vegetarian variation of the Atkins diet which focuses on whole grains, vegetables, tofu, legumes, nuts, and seeds while minimizing animal products. By reducing carbohydrate intake and incorporating plant-based protein sources, it promotes sustainability and healthier eating. (4) The Ornish Diet (with limited exercise) is designed to improve overall health, support weight loss, and reverse cardiovascular disease progression, the Ornish diet is low in fat. It emphasizes exercise, stress management, and healthy relationships alongside a low-fat eating plan. It is worth noting that vegan diets, in general, have been linked to a lower risk of chronic health conditions, including heart disease. These diet options provide alternatives for individuals with specific dietary preferences and goals.', 'oracle_utterance': \"Can you help me find a diet for myself considering that I'm vegetarian, allergic to soybeans, lactose intolerant, can't exercise too much, and should drink water regularly?\", 'response_provenance': ['clueweb22-en0035-25-01897:1', 'clueweb22-en0004-30-08099:2', 'clueweb22-en0038-84-16253:4', 'clueweb22-en0020-69-12751:1', 'clueweb22-en0007-46-12888:5'], 'context_utterances': [], 'ptkb': {'1': \"I don't like the new spin-off; because I keep comparing the two and it has lower quality.\", '2': 'Because of my kidney problem, I have to drink water frequently to stay hydrated.', '3': \"I'm going to change my phone.\", '4': \"I can't exercise too much because of the heart problem that I have.\", '5': \"I'm vegetarian.\", '6': \"I'm lactose intolerant.\", '7': \"I'm allergic to soybeans.\", '8': 'I just finished watching the Game of Thrones.', '9': \"I didn't like how the series ended, especially the war scenes.\", '10': \"I'm an Android user.\"}, 'ptkb_provenance': [5, 4, 2], 'reformulations': [{'reformulation_name': 'test_name', 'reformulated_query': 'emm'}], 'results': [{'collection': 'clueweb12B', 'retrieval_model': 'BM25', 'reranker': 'rankllama', 'retrieval_query_type': 'a', 'reranking_query_type': 'b', 'generation_query_type': 'c', 'metrics': {'map': 0.5, 'ndcg_cut_10': 0.5}}]}\n"
     ]
    }
   ],
   "source": [
    "turn_dict = asdict(turn)\n",
    "print(turn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### test from_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Reformulation(reformulation_name='test_name', reformulated_query='emm')]\n"
     ]
    }
   ],
   "source": [
    "new_turn = Turn()\n",
    "new_turn.from_dict(turn_dict)\n",
    "print(new_turn.reformulations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change turn object format (or Result, Reformulation object format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/rech/huiyuche/TREC_iKAT_2024/src/')\n",
    "import importlib\n",
    "import topics\n",
    "importlib.reload(sys.modules['topics'])\n",
    "from topics import Turn, load_turns_from_ikat_topic_files, save_turns_to_json, load_turns_from_json,Result,Reformulation\n",
    "from dataclasses import asdict\n",
    "import json\n",
    "\n",
    "# current version\n",
    "ikat_topic_file = \"/data/rech/huiyuche/TREC_iKAT_2024/data/topics/ikat_2023_test.json\"\n",
    "\n",
    "\n",
    "# modified version\n",
    "#where_to_save = \"/data/rech/huiyuche/TREC_iKAT_2024/test/ikat_2023_test.json\"\n",
    "where_to_save = \"/data/rech/huiyuche/TREC_iKAT_2024/data/topics/ikat_2023_test.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"turn_id\": \"9-1-3\",\n",
      "    \"conversation_id\": \"9-1\",\n",
      "    \"title\": \"Finding a diet\",\n",
      "    \"current_utterance\": \"What about the DASH diet? I heard it is a healthy diet.\",\n",
      "    \"current_response\": \"DASH diet is also a healthy diet that aims to reduce sodium intake while increasing the consumption of foods that are rich in nutrients. However, this diet includes meat and/or chicken. Since you are vegetarian this option is not suitable for you.\",\n",
      "    \"oracle_utterance\": \"What about the DASH diet? I heard it is a healthy diet.\",\n",
      "    \"response_provenance\": [\n",
      "        \"clueweb22-en0020-69-12751:1\",\n",
      "        \"clueweb22-en0028-21-06213:1\"\n",
      "    ],\n",
      "    \"context_utterances\": [\n",
      "        \"Can you help me find a diet for myself?\",\n",
      "        \"Ok, good. Can you tell me what diet is the fastest way to lose some weight?\"\n",
      "    ],\n",
      "    \"ptkb\": {\n",
      "        \"1\": \"I don't like the new spin-off; because I keep comparing the two and it has lower quality.\",\n",
      "        \"2\": \"Because of my kidney problem, I have to drink water frequently to stay hydrated.\",\n",
      "        \"3\": \"I'm going to change my phone.\",\n",
      "        \"4\": \"I can't exercise too much because of the heart problem that I have.\",\n",
      "        \"5\": \"I'm vegetarian.\",\n",
      "        \"6\": \"I'm lactose intolerant.\",\n",
      "        \"7\": \"I'm allergic to soybeans.\",\n",
      "        \"8\": \"I just finished watching the Game of Thrones.\",\n",
      "        \"9\": \"I didn't like how the series ended, especially the war scenes.\",\n",
      "        \"10\": \"I'm an Android user.\"\n",
      "    },\n",
      "    \"ptkb_provenance\": [\n",
      "        5\n",
      "    ],\n",
      "    \"reformulations\": [\n",
      "        {\n",
      "            \"reformulation_name\": \"oracle_utterance\",\n",
      "            \"reformulated_query\": \"What about the DASH diet? I heard it is a healthy diet.\",\n",
      "            \"ptkb_provenance\": []\n",
      "        }\n",
      "    ],\n",
      "    \"results\": [\n",
      "        {\n",
      "            \"collection\": \"ClueWeb_ikat\",\n",
      "            \"retrieval_model\": \"BM25\",\n",
      "            \"reranker\": \"none\",\n",
      "            \"generation_model\": \"none\",\n",
      "            \"retrieval_query_type\": \"oracle_utterance\",\n",
      "            \"reranking_query_type\": \"oracle_utterance\",\n",
      "            \"generation_query_type\": \"oracle_utterance\",\n",
      "            \"metrics\": {\n",
      "                \"map\": 0.14422770837920568,\n",
      "                \"recip_rank\": 1.0,\n",
      "                \"P_1\": 1.0,\n",
      "                \"P_3\": 1.0,\n",
      "                \"P_5\": 1.0,\n",
      "                \"P_10\": 1.0,\n",
      "                \"recall_5\": 0.04310344827586207,\n",
      "                \"ndcg_cut_1\": 0.5,\n",
      "                \"ndcg_cut_3\": 0.5432992037642228,\n",
      "                \"ndcg_cut_5\": 0.5312934851347721,\n",
      "                \"ndcg_cut_10\": 0.5293856728621718,\n",
      "                \"num_rel\": 116\n",
      "            },\n",
      "            \"response\": \"\"\n",
      "        },\n",
      "        {\n",
      "            \"collection\": \"ClueWeb_ikat\",\n",
      "            \"retrieval_model\": \"BM25\",\n",
      "            \"reranker\": \"rankllama\",\n",
      "            \"generation_model\": \"none\",\n",
      "            \"retrieval_query_type\": \"oracle_utterance\",\n",
      "            \"reranking_query_type\": \"oracle_utterance\",\n",
      "            \"generation_query_type\": \"oracle_utterance\",\n",
      "            \"metrics\": {\n",
      "                \"map\": 0.12116795185364664,\n",
      "                \"recip_rank\": 1.0,\n",
      "                \"P_1\": 1.0,\n",
      "                \"P_3\": 0.6666666666666666,\n",
      "                \"P_5\": 0.4,\n",
      "                \"P_10\": 0.5,\n",
      "                \"recall_5\": 0.017241379310344827,\n",
      "                \"ndcg_cut_1\": 1.0,\n",
      "                \"ndcg_cut_3\": 0.5865984075284455,\n",
      "                \"ndcg_cut_5\": 0.4239502565920201,\n",
      "                \"ndcg_cut_10\": 0.44271902701228,\n",
      "                \"num_rel\": 116\n",
      "            },\n",
      "            \"response\": \"\"\n",
      "        },\n",
      "        {\n",
      "            \"collection\": \"ClueWeb_ikat\",\n",
      "            \"retrieval_model\": \"BM25\",\n",
      "            \"reranker\": \"rankgpt\",\n",
      "            \"generation_model\": \"none\",\n",
      "            \"retrieval_query_type\": \"oracle_utterance\",\n",
      "            \"reranking_query_type\": \"oracle_utterance\",\n",
      "            \"generation_query_type\": \"oracle_utterance\",\n",
      "            \"metrics\": {\n",
      "                \"map\": 0.1341890295898336,\n",
      "                \"recip_rank\": 1.0,\n",
      "                \"P_1\": 1.0,\n",
      "                \"P_3\": 0.6666666666666666,\n",
      "                \"P_5\": 0.8,\n",
      "                \"P_10\": 0.9,\n",
      "                \"recall_5\": 0.034482758620689655,\n",
      "                \"ndcg_cut_1\": 0.5,\n",
      "                \"ndcg_cut_3\": 0.35195904451706733,\n",
      "                \"ndcg_cut_5\": 0.42209231892823046,\n",
      "                \"ndcg_cut_10\": 0.4915069380675004,\n",
      "                \"num_rel\": 116\n",
      "            },\n",
      "            \"response\": \"\"\n",
      "        },\n",
      "        {\n",
      "            \"collection\": \"ClueWeb_ikat_official\",\n",
      "            \"retrieval_model\": \"BM25\",\n",
      "            \"reranker\": \"none\",\n",
      "            \"generation_model\": \"none\",\n",
      "            \"retrieval_query_type\": \"oracle_utterance\",\n",
      "            \"reranking_query_type\": \"oracle_utterance\",\n",
      "            \"generation_query_type\": \"oracle_utterance\",\n",
      "            \"metrics\": {\n",
      "                \"map\": 0.21365648820982636,\n",
      "                \"recip_rank\": 1.0,\n",
      "                \"P_1\": 1.0,\n",
      "                \"P_3\": 1.0,\n",
      "                \"P_5\": 1.0,\n",
      "                \"P_10\": 1.0,\n",
      "                \"recall_5\": 0.04310344827586207,\n",
      "                \"recall_100\": 0.25862068965517243,\n",
      "                \"recall_1000\": 0.5862068965517241,\n",
      "                \"ndcg_cut_1\": 0.5,\n",
      "                \"ndcg_cut_3\": 0.5432992037642228,\n",
      "                \"ndcg_cut_5\": 0.5312934851347721,\n",
      "                \"ndcg_cut_10\": 0.5293856728621718,\n",
      "                \"num_rel\": 116\n",
      "            },\n",
      "            \"response\": \"\"\n",
      "        },\n",
      "        {\n",
      "            \"collection\": \"ClueWeb_ikat\",\n",
      "            \"retrieval_model\": \"BM25\",\n",
      "            \"reranker\": \"monot5_base\",\n",
      "            \"generation_model\": \"none\",\n",
      "            \"retrieval_query_type\": \"oracle_utterance\",\n",
      "            \"reranking_query_type\": \"oracle_utterance\",\n",
      "            \"generation_query_type\": \"oracle_utterance\",\n",
      "            \"metrics\": {\n",
      "                \"map\": 0.08620689655172414,\n",
      "                \"recip_rank\": 1.0,\n",
      "                \"P_1\": 1.0,\n",
      "                \"P_3\": 1.0,\n",
      "                \"P_5\": 1.0,\n",
      "                \"P_10\": 1.0,\n",
      "                \"recall_5\": 0.04310344827586207,\n",
      "                \"ndcg_cut_1\": 1.0,\n",
      "                \"ndcg_cut_3\": 0.6606188852699119,\n",
      "                \"ndcg_cut_5\": 0.6488848058312616,\n",
      "                \"ndcg_cut_10\": 0.5900964549354183,\n",
      "                \"num_rel\": 116\n",
      "            },\n",
      "            \"response\": \"\"\n",
      "        },\n",
      "        {\n",
      "            \"collection\": \"ClueWeb_ikat\",\n",
      "            \"retrieval_model\": \"BM25\",\n",
      "            \"reranker\": \"monot5_base_10k\",\n",
      "            \"generation_model\": \"none\",\n",
      "            \"retrieval_query_type\": \"oracle_utterance\",\n",
      "            \"reranking_query_type\": \"oracle_utterance\",\n",
      "            \"generation_query_type\": \"oracle_utterance\",\n",
      "            \"metrics\": {\n",
      "                \"map\": 0.18733642853707397,\n",
      "                \"recip_rank\": 1.0,\n",
      "                \"P_1\": 1.0,\n",
      "                \"P_3\": 1.0,\n",
      "                \"P_5\": 0.8,\n",
      "                \"P_10\": 0.6,\n",
      "                \"recall_5\": 0.034482758620689655,\n",
      "                \"ndcg_cut_1\": 1.0,\n",
      "                \"ndcg_cut_3\": 0.7932992037642228,\n",
      "                \"ndcg_cut_5\": 0.6389409533746568,\n",
      "                \"ndcg_cut_10\": 0.4948808844596415,\n",
      "                \"num_rel\": 116\n",
      "            },\n",
      "            \"response\": \"\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "list_of_turns = load_turns_from_json(ikat_topic_file)\n",
    "turn_dict_list = save_turns_to_json(list_of_turns, where_to_save)\n",
    "print(json.dumps(turn_dict_list[2], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test IKAT format output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'turn_id': '10-2_13', 'responses': [{'rank': 1, 'text': 'pseudo response 1', 'ptkb_provenance': [], 'passage_provenance': [{'id': 'clueweb22-en0013-24-12486:0', 'score': 16.136999130249023, 'used': True}, {'id': 'clueweb22-en0044-24-12435:0', 'score': 16.068199157714844, 'used': True}, {'id': 'clueweb22-en0012-73-01681:8', 'score': 15.756500244140625, 'used': True}, {'id': 'clueweb22-en0041-03-09116:0', 'score': 15.668299674987793, 'used': False}, {'id': 'clueweb22-en0025-44-01168:0', 'score': 15.497599601745605, 'used': False}]}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "ikat_output_path = \"/data/rech/huiyuche/TREC_iKAT_2024/results/ClueWeb_ikat/ikat_23_test/ikat_format_output/none.json\"\n",
    "with open(ikat_output_path, \"r\") as f:\n",
    "    ikat_output = json.load(f)\n",
    "print(ikat_output[\"turns\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "sys.path.append('/data/rech/huiyuche/TREC_iKAT_2024/src/')\n",
    "import rerank\n",
    "from rank_gpt import run_retriever, sliding_windows\n",
    "importlib.reload(sys.modules['rerank'])\n",
    "\n",
    "from rerank import (\n",
    "    load_rankllama, \n",
    "    rerank_rankllama,\n",
    "    hits_2_rankgpt_list\n",
    "    )\n",
    "\n",
    "cache_dir = \"/data/rech/huiyuche/huggingface\"\n",
    "\n",
    "\n",
    "query = \"what is the capital of China?\"\n",
    "\n",
    "passages = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Beijing. The modern day capital of China is Beijing (literally 'Northern Capital'), which first served as China's capital city in 1261, when the Mongol ruler Kublai Khan established his seat of power in the area centered around what is today Beihai Park.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Llama The llama is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the pre-Columbian era.\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_rankllama(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(3.4062, dtype=torch.bfloat16), tensor(4.4688, dtype=torch.bfloat16), tensor(-3.9062, dtype=torch.bfloat16), tensor(-9.8125, dtype=torch.bfloat16)]\n"
     ]
    }
   ],
   "source": [
    "scores = rerank_rankllama(query, passages, tokenizer, model)\n",
    "print(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.modeling_llama.LlamaForSequenceClassification'>\n",
      "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rankGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"source ~/.bashrc\")\n",
    "openai_key = os.getenv('openai_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data structure of hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "LuceneSearcher.list_prebuilt_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rech/huiyuche/envs/trec_ikat/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Jul 09, 2024 5:51:28 A.M. org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "retrieval_query_list = [\n",
    "    \"what is the capital of France?\",\n",
    "    \"what is the capital of Germany?\",\n",
    "    \"what is the capital of Italy?\"\n",
    "]\n",
    "qid_list_string = [\n",
    "    \"id1\",\n",
    "    \"id2\",\n",
    "    \"id3\"\n",
    "]\n",
    "\n",
    "query_dict = {\n",
    "    qid: query for qid, query in zip(qid_list_string, retrieval_query_list)\n",
    "}\n",
    "\n",
    "searcher = LuceneSearcher(\"/part/01/Tmp/yuchen/indexes/clueweb22b_ikat23_fengran_sparse_index_2/\")\n",
    "searcher.set_bm25(0.9, 0.4)\n",
    "hits = searcher.batch_search(retrieval_query_list, qid_list_string, k = 10, threads = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "liste = np.array([1,3,2,4,5], dtype=np.float32)\n",
    "index = np.argsort(-liste)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "hits[\"id1\"][0].rank = 0\n",
    "print(hits[\"id1\"][0].rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"query\": \"what is the capital of France?\",\n",
      "        \"hits\": [\n",
      "            {\n",
      "                \"content\": \"What is the Capital of France? | Mappr Capital Cities What is the Capital of France? What is the Capital of France? Paris is the capital city of France and the center of France. It is built on the Seine River, in the middle of the Paris Basin. Where is Paris? Known for its monuments, artistic and cultural life all over the world, Paris is also one of the major economic and political centers along with being an important city in world history and is one of the transit points of international transport. Paris, a fashion and luxury world capital, is also known as \\u201cCity of Lights\\u201d. When did Paris Become the Capital? The most important archaeological finds are the remains of the oldest permanent human settlement in the Paris region, which was discovered in 1991 in the 12th region.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0017-32-04653:0\",\n",
      "                \"rank\": 0,\n",
      "                \"score\": 9.22029972076416\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"Brexit & Second Homes in France: What you need to know FRANCE Brexit & Second Homes in France: What you need to know The UK\\u2019s departure from the European Union will have consequences for Brits with second homes in France, below we set out what these changes are and how we think they will impact the market Written By: Kate Everett-Allen, Knight Frank 17 Dec 2020 4 minutes to read Categories:Property SectorResidential Sales TopicBrexit World RegionsEurope France We spoke with Caroline Cohen of The French law Practice who answered the following questions for us: If I am looking to buy in France will my purchase costs increase from 1 January 2021? No, there are no additional taxes for non-residents looking to purchase in France after 1 January 2021. Purchase costs will remain at around 7% for an existing property and 2% for a new-build home. What changes if I want to sell my second home in France? There are several factors to consider: Capital Gains Tax In France, there are two payments due on a capital gain, a capital gains tax (CGT) and a social levy. The standard capital gains tax on the sale of a property will remain at 19% and is only payable on a second home, if your French property is your primary residence no CGT is payable. The standard social levy charge for EU residents with a second home in France is currently 7.5% but this increases to 17.2% for British homeowners from 1 January 2021 as they will be no longer be EU residents. This means total costs will equate to 36.2%, up from 26.5%. It\\u2019s worth bearing in mind that a capital gain is largely a positive for a homeowner, it means a property asset, in the case of most French purchases, one bought for a lifestyle benefit, has also increased in value generating a return on investment. A taper relief also exists for both CGT and the social levy.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0016-24-18543:0\",\n",
      "                \"rank\": 1,\n",
      "                \"score\": 8.990500450134277\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"London i^the capital of Great Britain. What is the capital o f Germany? Berlin is the capital o f Germany. What is the capital o f France? Paris is the capital o f France. What is the capital o f Poland? Warsaw is the capital o f Poland. What is the capital of Germany? Berlin is the capital of Germany. ffrom\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0021-14-14889:63\",\n",
      "                \"rank\": 2,\n",
      "                \"score\": 8.907099723815918\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"[ 13] For example, the fact described by the true statement \\\"Paris is the capital city of France\\\" implies that there is such a place as Paris, there is such a place as France, there are such things as capital cities, as well as that France has a government, that the government of France has the power to define its capital city, and that the French government has chosen Paris to be the capital, that there is such a thing as a place or a government, and so on. The verifiable accuracy of all of these assertions, if facts themselves, may coincide to create the fact, that Paris is the capital of France. Difficulties arise, however, in attempting to identify the constituent parts of negative, modal, disjunctive, or moral facts. [ 17] Fact\\u2013value distinction Main article: Fact\\u2013value distinction Moral philosophers since David Hume have debated whether values are objective, and thus factual. In A Treatise of Human Nature Hume pointed out there is no obvious way for a series of statements about what ought to be the case to be derived from a series of statements of what is the case. Those who insist there is a logical gulf between facts and values, such that it is fallacious to attempt to derive values from facts, include G. E. Moore, who called attempting to do so the naturalistic fallacy . Factual\\u2013counterfactual distinction Main article: Counterfactual conditional Factuality \\u2014what has occurred\\u2014 can also be contrasted with counterfactuality: what might have occurred, but did not. A counterfactual conditional or subjunctive conditional is a conditional (or \\\"if-then\\\") statement indicating what would be the case if events had been other than they were.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0037-88-06710:10\",\n",
      "                \"rank\": 3,\n",
      "                \"score\": 8.903499603271484\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"Who attacked both Britain and northern Gaul? - Answers \\ud83e\\udd1d \\ud83d\\udcbb \\ud83d\\udcb0 \\ud83d\\udcaf Resources \\ud83d\\udcd3 \\ud83c\\udfc6 \\ud83d\\udd00 \\ud83e\\udd1d Who attacked both Britain and northern Gaul? Wiki User \\u2219 2013-04-12 03:37:36 Study now Best Answer Copy angles/saxons Wiki User \\u2219 2013-04-12 03:37:36 This answer is: Study guides United Kingdom 24 cards What is the capital of the United Kingdom What is the capital of Denmark What is the capital of Scotland What are narrow deep valleys called 4.5 \\u2606\\u2605\\u2606\\u2605\\u2606\\u2605\\u2606\\u2605\\u2606\\u2605 6 Reviews Study now More answers Anonymous \\u2219 Lvl 1 \\u2219 2020-04-06 19:37:34 Copy 1#. Who attacked both Britain and Northern gaul? This answer is: Who attacked both Britain and northern France? Germany What happened to deserters from France and Britain? they wree attacked by both France and Britain and they were devastated What happened to deserters from Britain and France? they wree attacked by both France and Britain and they were devastated Who attacked both Britain and France? Adyan and friends from Germany Why did America shortly stop trade with France and brion? Both France and Britain attacked America.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0001-30-00978:0\",\n",
      "                \"rank\": 4,\n",
      "                \"score\": 8.807000160217285\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"What is the name of the hats that the people in Paris France wear? - Answers What is the name of the hats that the people in Paris France wear? Wiki User \\u2219 2008-05-21 04:16:44 Study now Best Answer Copy barret Wiki User \\u2219 2008-05-21 04:16:44 Study guides \\ud83d\\udcd3 What country has a capital that name is Paris? France, Paris, France What is the name of the city that has the Eiffel tower? paris france What is France capital name? Paris What is a city in Paris? Paris is the name of a city, located in France. What is title I can use for a story about Paris France? Just use the name \\\"Paris\\\" or \\\"France :S What are name of cuisines of Paris? name of cuisines of France What is the meaning of the name Paris?\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0015-61-14458:0\",\n",
      "                \"rank\": 5,\n",
      "                \"score\": 8.795999526977539\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"Wiki User \\u2219 2014-08-26 16:03:59 Study guides United Kingdom 24 cards What is the capital of the United Kingdom What is the capital of Denmark What is the capital of Scotland What are narrow deep valleys called \\u27a1\\ufe0f See all cards 4.5 \\u2606\\u2605\\u2606\\u2605\\u2606\\u2605\\u2606\\u2605\\u2606\\u2605 6 Reviews How did the weak leadership contribute to the revolutionary mood in France? Cause the had swag for days How did Louis weak leadership contribute to the growing crisis in France? Heueueheh How did Louis xvis weak leadership contribute to the growing crisis in France? Heueueheh How did Louis XVI weak leadership contribute to growing crisis in France? Heueueheh How did Louis XVI weak leadership contribute to the growing crisis in France? Heueueheh How did Louis XIV's weak leadership contribute to the growing crisis in France? I don't know, I have problem too What did etruscans contribute ti the worlds knowledge? They went to south of Syria to trade and contribute but the Romans were to weak to contribute back to the etruscans. Even the indians were to weak Did the consulate bring weak and unorganized leadership in France? That would have been impossible since it brought Napoleon Bonaparte into the government as the First Consul.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0021-60-09500:1\",\n",
      "                \"rank\": 6,\n",
      "                \"score\": 8.719300270080566\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"14] Correspondence and the slingshot argument Pascal Engel's version of the correspondence theory of truth explains that what makes a sentence true is that it corresponds to a fact. [ 15] This theory presupposes the existence of an objective world. The Slingshot argument claims to show that all true statements stand for the same thing, the truth value true. If this argument holds, and facts are taken to be what true statements stand for, then one arrives at the counter-intuitive conclusion that there is only one fact: the truth.[16] Compound facts Any non-trivial true statement about reality is necessarily an abstraction composed of a complex of objects and properties or relations .\\\"Facts possess internal structure, being complexes of objects and properties or relations\\\" [ 13] For example, the fact described by the true statement \\\"Paris is the capital city of France\\\" implies that there is such a place as Paris, there is such a place as France, there are such things as capital cities, as well as that France has a government, that the government of France has the power to define its capital city, and that the French government has chosen Paris to be the capital, that there is such a thing as a place or a government, and so on. The verifiable accuracy of all of these assertions, if facts themselves, may coincide to create the fact, that Paris is the capital of France. Difficulties arise, however, in attempting to identify the constituent parts of negative, modal, disjunctive, or moral facts. [\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0037-88-06710:9\",\n",
      "                \"rank\": 7,\n",
      "                \"score\": 8.717900276184082\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"Dijon city guide - essential visitor information in English Dijon - city and visitor guide Discover the historic capital city of Burgundy About-France.com - the connoisseur's guide to France Explore France \\u25ba Essential pages Travel in France Where to go What to see and do \\u25ba You are here : France \\u203a Heritage cities \\u203a Dijon Dijon - the capital of Burgundy On this page Location and access Tourist attractions In the area Local accommodation Parking : the blue dots show parking lots near the tram route. Traffic access to the historic centre is limited, but there are numerous car parks and plenty of street parking too. Just an hour and a half from Paris by high speed train, Dijon, the capital of the modernday region of Bourgogne - Franche- Comt\\u00e9, is a city with a very proud past. In the Middle Ages, the Dukes of Burgundy were second only in power to the Kings of France: and indeed for much of the Middle Ages, the Dukedom of Burgundy was to all intents and purposes an independent state, separate from and at times at war with the Kingdom of France. In the 14th and 15th centuries, the Duchy of Burgundy was a major European power, whose territory stretched at its peak from the North Sea (Flanders, Artois and Picardy) to the border with Switzerland, and Dijon was its capital. Since the regional reforms of 2016, Dijon has become the smallest regional capital in France apart from Orleans , and no longer occupies the position of importance that it enjoyed in medieval and Renaissance times . With a population of just over 150,000, it is far smaller than the big provincial capitals of France like Lyon or Toulouse or Strasbourg. In 2015, its historic centre was listed as a UNESCO world heritage site. Located at the northern end of the main Burgundy vineyard area, Dijon is nowadays best known as one of the wine capitals of France, and also for its famous mustard, la Moutarde de Dijon, which is known worldwide. Don't expect however to find a city surrounded by mustard fields; the yellow fields that can be seen round Dijon in the spring time are fields of colza, or oil-seed rape, not of mustard.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0015-31-19006:0\",\n",
      "                \"rank\": 8,\n",
      "                \"score\": 8.682100296020508\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"[ greit bntn] | G erm any:] Poland [ paubndj 77; C\\\\ I -j . Berlin is the capital o f Germany. W hatj^ the capital o f Great Britain? London i^the capital of Great Britain. What is the capital o f Germany? Berlin is the capital o f Germany. What is the capital o f France? Paris is the capital o f France.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0021-14-14889:62\",\n",
      "                \"rank\": 9,\n",
      "                \"score\": 8.652999877929688\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"Paris, the capital of France, was originally a small tribal settlement in 3rd century BC. It flourished under the Romans, becoming a prosperous city. In the Middle Ages, it became the capital of a major European kingdom and a center of learning. Paris played a central role in the French Revolution and was transformed into a modern city in the 19th century by Napoleon III. Today, it's a leading global city known for art, fashion, and culture\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"pseudo-id\",\n",
      "                \"rank\": 10,\n",
      "                \"score\": 0.1\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "rank_gpt_list, new_hits_dict = hits_2_rankgpt_list(searcher, query_dict, hits)\n",
    "for i in range(3):\n",
    "    rank_gpt_list[i][\"hits\"].append(\n",
    "        {\n",
    "            \"content\" : \"Paris, the capital of France, was originally a small tribal settlement in 3rd century BC. It flourished under the Romans, becoming a prosperous city. In the Middle Ages, it became the capital of a major European kingdom and a center of learning. Paris played a central role in the French Revolution and was transformed into a modern city in the 19th century by Napoleon III. Today, it's a leading global city known for art, fashion, and culture\",\n",
    "            \"qid\":\"id1\",\n",
    "            \"docid\": \"pseudo-id\",\n",
    "            \"rank\": 10,\n",
    "            \"score\":0.1\n",
    "        }\n",
    "    )\n",
    "print(json.dumps(rank_gpt_list[0:1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "new_rank_gpt_list = []\n",
    "rank_end = 11\n",
    "for item in tqdm(rank_gpt_list):\n",
    "    new_item = sliding_windows(item, rank_start=0, rank_end=rank_end, window_size=5, step=2, model_name='gpt-3.5-turbo', api_key=openai_key)\n",
    "    new_rank_gpt_list.append(new_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"query\": \"what is the capital of France?\",\n",
      "        \"hits\": [\n",
      "            {\n",
      "                \"content\": \"What is the Capital of France? | Mappr Capital Cities What is the Capital of France? What is the Capital of France? Paris is the capital city of France and the center of France. It is built on the Seine River, in the middle of the Paris Basin. Where is Paris? Known for its monuments, artistic and cultural life all over the world, Paris is also one of the major economic and political centers along with being an important city in world history and is one of the transit points of international transport. Paris, a fashion and luxury world capital, is also known as \\u201cCity of Lights\\u201d. When did Paris Become the Capital? The most important archaeological finds are the remains of the oldest permanent human settlement in the Paris region, which was discovered in 1991 in the 12th region.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0017-32-04653:0\",\n",
      "                \"rank\": 0,\n",
      "                \"score\": 9.22029972076416\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"Paris, the capital of France, was originally a small tribal settlement in 3rd century BC. It flourished under the Romans, becoming a prosperous city. In the Middle Ages, it became the capital of a major European kingdom and a center of learning. Paris played a central role in the French Revolution and was transformed into a modern city in the 19th century by Napoleon III. Today, it's a leading global city known for art, fashion, and culture\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"pseudo-id\",\n",
      "                \"rank\": 1,\n",
      "                \"score\": 8.990500450134277\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"[ greit bntn] | G erm any:] Poland [ paubndj 77; C\\\\ I -j . Berlin is the capital o f Germany. W hatj^ the capital o f Great Britain? London i^the capital of Great Britain. What is the capital o f Germany? Berlin is the capital o f Germany. What is the capital o f France? Paris is the capital o f France.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0021-14-14889:62\",\n",
      "                \"rank\": 2,\n",
      "                \"score\": 8.907099723815918\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"London i^the capital of Great Britain. What is the capital o f Germany? Berlin is the capital o f Germany. What is the capital o f France? Paris is the capital o f France. What is the capital o f Poland? Warsaw is the capital o f Poland. What is the capital of Germany? Berlin is the capital of Germany. ffrom\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0021-14-14889:63\",\n",
      "                \"rank\": 3,\n",
      "                \"score\": 8.903499603271484\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"Brexit & Second Homes in France: What you need to know FRANCE Brexit & Second Homes in France: What you need to know The UK\\u2019s departure from the European Union will have consequences for Brits with second homes in France, below we set out what these changes are and how we think they will impact the market Written By: Kate Everett-Allen, Knight Frank 17 Dec 2020 4 minutes to read Categories:Property SectorResidential Sales TopicBrexit World RegionsEurope France We spoke with Caroline Cohen of The French law Practice who answered the following questions for us: If I am looking to buy in France will my purchase costs increase from 1 January 2021? No, there are no additional taxes for non-residents looking to purchase in France after 1 January 2021. Purchase costs will remain at around 7% for an existing property and 2% for a new-build home. What changes if I want to sell my second home in France? There are several factors to consider: Capital Gains Tax In France, there are two payments due on a capital gain, a capital gains tax (CGT) and a social levy. The standard capital gains tax on the sale of a property will remain at 19% and is only payable on a second home, if your French property is your primary residence no CGT is payable. The standard social levy charge for EU residents with a second home in France is currently 7.5% but this increases to 17.2% for British homeowners from 1 January 2021 as they will be no longer be EU residents. This means total costs will equate to 36.2%, up from 26.5%. It\\u2019s worth bearing in mind that a capital gain is largely a positive for a homeowner, it means a property asset, in the case of most French purchases, one bought for a lifestyle benefit, has also increased in value generating a return on investment. A taper relief also exists for both CGT and the social levy.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0016-24-18543:0\",\n",
      "                \"rank\": 4,\n",
      "                \"score\": 8.807000160217285\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"Dijon city guide - essential visitor information in English Dijon - city and visitor guide Discover the historic capital city of Burgundy About-France.com - the connoisseur's guide to France Explore France \\u25ba Essential pages Travel in France Where to go What to see and do \\u25ba You are here : France \\u203a Heritage cities \\u203a Dijon Dijon - the capital of Burgundy On this page Location and access Tourist attractions In the area Local accommodation Parking : the blue dots show parking lots near the tram route. Traffic access to the historic centre is limited, but there are numerous car parks and plenty of street parking too. Just an hour and a half from Paris by high speed train, Dijon, the capital of the modernday region of Bourgogne - Franche- Comt\\u00e9, is a city with a very proud past. In the Middle Ages, the Dukes of Burgundy were second only in power to the Kings of France: and indeed for much of the Middle Ages, the Dukedom of Burgundy was to all intents and purposes an independent state, separate from and at times at war with the Kingdom of France. In the 14th and 15th centuries, the Duchy of Burgundy was a major European power, whose territory stretched at its peak from the North Sea (Flanders, Artois and Picardy) to the border with Switzerland, and Dijon was its capital. Since the regional reforms of 2016, Dijon has become the smallest regional capital in France apart from Orleans , and no longer occupies the position of importance that it enjoyed in medieval and Renaissance times . With a population of just over 150,000, it is far smaller than the big provincial capitals of France like Lyon or Toulouse or Strasbourg. In 2015, its historic centre was listed as a UNESCO world heritage site. Located at the northern end of the main Burgundy vineyard area, Dijon is nowadays best known as one of the wine capitals of France, and also for its famous mustard, la Moutarde de Dijon, which is known worldwide. Don't expect however to find a city surrounded by mustard fields; the yellow fields that can be seen round Dijon in the spring time are fields of colza, or oil-seed rape, not of mustard.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0015-31-19006:0\",\n",
      "                \"rank\": 5,\n",
      "                \"score\": 8.795999526977539\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"[ 13] For example, the fact described by the true statement \\\"Paris is the capital city of France\\\" implies that there is such a place as Paris, there is such a place as France, there are such things as capital cities, as well as that France has a government, that the government of France has the power to define its capital city, and that the French government has chosen Paris to be the capital, that there is such a thing as a place or a government, and so on. The verifiable accuracy of all of these assertions, if facts themselves, may coincide to create the fact, that Paris is the capital of France. Difficulties arise, however, in attempting to identify the constituent parts of negative, modal, disjunctive, or moral facts. [ 17] Fact\\u2013value distinction Main article: Fact\\u2013value distinction Moral philosophers since David Hume have debated whether values are objective, and thus factual. In A Treatise of Human Nature Hume pointed out there is no obvious way for a series of statements about what ought to be the case to be derived from a series of statements of what is the case. Those who insist there is a logical gulf between facts and values, such that it is fallacious to attempt to derive values from facts, include G. E. Moore, who called attempting to do so the naturalistic fallacy . Factual\\u2013counterfactual distinction Main article: Counterfactual conditional Factuality \\u2014what has occurred\\u2014 can also be contrasted with counterfactuality: what might have occurred, but did not. A counterfactual conditional or subjunctive conditional is a conditional (or \\\"if-then\\\") statement indicating what would be the case if events had been other than they were.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0037-88-06710:10\",\n",
      "                \"rank\": 6,\n",
      "                \"score\": 8.719300270080566\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"What is the name of the hats that the people in Paris France wear? - Answers What is the name of the hats that the people in Paris France wear? Wiki User \\u2219 2008-05-21 04:16:44 Study now Best Answer Copy barret Wiki User \\u2219 2008-05-21 04:16:44 Study guides \\ud83d\\udcd3 What country has a capital that name is Paris? France, Paris, France What is the name of the city that has the Eiffel tower? paris france What is France capital name? Paris What is a city in Paris? Paris is the name of a city, located in France. What is title I can use for a story about Paris France? Just use the name \\\"Paris\\\" or \\\"France :S What are name of cuisines of Paris? name of cuisines of France What is the meaning of the name Paris?\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0015-61-14458:0\",\n",
      "                \"rank\": 7,\n",
      "                \"score\": 8.717900276184082\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"Who attacked both Britain and northern Gaul? - Answers \\ud83e\\udd1d \\ud83d\\udcbb \\ud83d\\udcb0 \\ud83d\\udcaf Resources \\ud83d\\udcd3 \\ud83c\\udfc6 \\ud83d\\udd00 \\ud83e\\udd1d Who attacked both Britain and northern Gaul? Wiki User \\u2219 2013-04-12 03:37:36 Study now Best Answer Copy angles/saxons Wiki User \\u2219 2013-04-12 03:37:36 This answer is: Study guides United Kingdom 24 cards What is the capital of the United Kingdom What is the capital of Denmark What is the capital of Scotland What are narrow deep valleys called 4.5 \\u2606\\u2605\\u2606\\u2605\\u2606\\u2605\\u2606\\u2605\\u2606\\u2605 6 Reviews Study now More answers Anonymous \\u2219 Lvl 1 \\u2219 2020-04-06 19:37:34 Copy 1#. Who attacked both Britain and Northern gaul? This answer is: Who attacked both Britain and northern France? Germany What happened to deserters from France and Britain? they wree attacked by both France and Britain and they were devastated What happened to deserters from Britain and France? they wree attacked by both France and Britain and they were devastated Who attacked both Britain and France? Adyan and friends from Germany Why did America shortly stop trade with France and brion? Both France and Britain attacked America.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0001-30-00978:0\",\n",
      "                \"rank\": 8,\n",
      "                \"score\": 8.682100296020508\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"14] Correspondence and the slingshot argument Pascal Engel's version of the correspondence theory of truth explains that what makes a sentence true is that it corresponds to a fact. [ 15] This theory presupposes the existence of an objective world. The Slingshot argument claims to show that all true statements stand for the same thing, the truth value true. If this argument holds, and facts are taken to be what true statements stand for, then one arrives at the counter-intuitive conclusion that there is only one fact: the truth.[16] Compound facts Any non-trivial true statement about reality is necessarily an abstraction composed of a complex of objects and properties or relations .\\\"Facts possess internal structure, being complexes of objects and properties or relations\\\" [ 13] For example, the fact described by the true statement \\\"Paris is the capital city of France\\\" implies that there is such a place as Paris, there is such a place as France, there are such things as capital cities, as well as that France has a government, that the government of France has the power to define its capital city, and that the French government has chosen Paris to be the capital, that there is such a thing as a place or a government, and so on. The verifiable accuracy of all of these assertions, if facts themselves, may coincide to create the fact, that Paris is the capital of France. Difficulties arise, however, in attempting to identify the constituent parts of negative, modal, disjunctive, or moral facts. [\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0037-88-06710:9\",\n",
      "                \"rank\": 9,\n",
      "                \"score\": 8.652999877929688\n",
      "            },\n",
      "            {\n",
      "                \"content\": \"Wiki User \\u2219 2014-08-26 16:03:59 Study guides United Kingdom 24 cards What is the capital of the United Kingdom What is the capital of Denmark What is the capital of Scotland What are narrow deep valleys called \\u27a1\\ufe0f See all cards 4.5 \\u2606\\u2605\\u2606\\u2605\\u2606\\u2605\\u2606\\u2605\\u2606\\u2605 6 Reviews How did the weak leadership contribute to the revolutionary mood in France? Cause the had swag for days How did Louis weak leadership contribute to the growing crisis in France? Heueueheh How did Louis xvis weak leadership contribute to the growing crisis in France? Heueueheh How did Louis XVI weak leadership contribute to growing crisis in France? Heueueheh How did Louis XVI weak leadership contribute to the growing crisis in France? Heueueheh How did Louis XIV's weak leadership contribute to the growing crisis in France? I don't know, I have problem too What did etruscans contribute ti the worlds knowledge? They went to south of Syria to trade and contribute but the Romans were to weak to contribute back to the etruscans. Even the indians were to weak Did the consulate bring weak and unorganized leadership in France? That would have been impossible since it brought Napoleon Bonaparte into the government as the First Consul.\",\n",
      "                \"qid\": \"id1\",\n",
      "                \"docid\": \"clueweb22-en0021-60-09500:1\",\n",
      "                \"rank\": 10,\n",
      "                \"score\": 0.1\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(new_rank_gpt_list[0:1], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### monoT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 05:10:47.229103: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-10 05:10:49.215882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "cache_dir = \"/data/rech/huiyuche/huggingface\"\n",
    "sys.path.append('/data/rech/huiyuche/TREC_iKAT_2024/src/')\n",
    "import importlib\n",
    "import rerank\n",
    "import llm\n",
    "importlib.reload(sys.modules['llm'])\n",
    "importlib.reload(sys.modules['rerank'])\n",
    "from rerank import (\n",
    "    load_t5_DP,\n",
    "    rerank_t5_DP\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model, decoder_stard_id, targeted_ids = load_t5_DP(\n",
    "    cache_dir = cache_dir,\n",
    "    model_name = \"castorini/monot5-base-msmarco\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125, 19, 8, 1784, 13, 1410, 58, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"what is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "retrieval_query_list = [\n",
    "    \"what is the capital of France?\",\n",
    "    \"what is the capital of Germany?\",\n",
    "    \"what is the capital of Italy?\"\n",
    "]\n",
    "qid_list_string = [\n",
    "    \"id1\",\n",
    "    \"id2\",\n",
    "    \"id3\"\n",
    "]\n",
    "\n",
    "query_dict = {\n",
    "    qid: query for qid, query in zip(qid_list_string, retrieval_query_list)\n",
    "}\n",
    "\n",
    "searcher = LuceneSearcher(\"/part/01/Tmp/yuchen/indexes/clueweb22b_ikat23_fengran_sparse_index_2/\")\n",
    "#searcher = LuceneSearcher(\"/data/rech/huiyuche/TREC_iKAT_2024/data/indexes/clueweb22b_ikat23_fengran_sparse_index_2\")\n",
    "searcher.set_bm25(0.9, 0.4)\n",
    "hits = searcher.batch_search(retrieval_query_list, qid_list_string, k = 1000, threads = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking:   0%|          | 0/3 [00:00<?, ?it/s]/data/rech/huiyuche/envs/trec_ikat/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Reranking:  33%|███▎      | 1/3 [00:16<00:32, 16.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9456437230110168, 0.996783971786499, 0.9666732549667358, 6.670828042842913e-06, 0.0817103460431099, 0.7397226095199585, 0.00876297801733017, 0.0719098299741745, 2.736906481004553e-06, 4.171901673544198e-05, 0.03734677657485008, 0.9959157109260559, 7.1600134106120095e-06, 0.0015542390756309032, 0.9418118596076965, 7.144066785258474e-06, 0.00045250411494635046, 0.00036305320099927485, 2.427258550596889e-05, 2.3689308363827877e-05, 0.9712644219398499, 0.24110576510429382, 0.0002644523046910763, 0.5568421483039856, 0.00040787336183711886, 0.9873769879341125, 0.03674014285206795, 0.8245005011558533, 0.00019249253091402352, 0.8373112678527832, 0.5780085921287537, 7.149000884965062e-05, 0.9149550199508667, 0.00017964046855922788, 0.9679471850395203, 0.9127674102783203, 0.00016537107876501977, 0.0011061328696087003, 1.9488272300804965e-05, 5.129402325110277e-06, 0.9169191122055054, 0.05827391520142555, 1.4319890624392428e-06, 0.001757705700583756, 8.745069862925448e-06, 0.0010489207925274968, 3.5366301744943485e-05, 6.532152838190086e-06, 4.790351522387937e-05, 0.6138872504234314, 0.0001232071517733857, 7.505595021939371e-06, 0.0007147350115701556, 0.0006743739359080791, 0.8066482543945312, 0.9184075593948364, 1.0099451174028218e-05, 0.07541801035404205, 3.299747959317756e-06, 6.844995368737727e-05, 4.490935680223629e-05, 3.3092524063249584e-06, 0.0005152165540494025, 5.137794141774066e-05, 0.00012852053623646498, 0.0007394577260129154, 3.33322022925131e-05, 0.8545381426811218, 6.515223958558636e-06, 0.004646585322916508, 1.554210393805988e-05, 0.0004629059403669089, 0.7783973813056946, 1.3692495031136787e-06, 0.0292636901140213, 1.4042961993254721e-05, 0.38074594736099243, 4.458813691599062e-06, 0.40141087770462036, 0.00011668021761579439, 0.13233478367328644, 2.55204645327467e-06, 0.00010193372145295143, 0.18372327089309692, 4.0163493395084515e-05, 0.028063781559467316, 0.0004872975405305624, 0.0020295290742069483, 0.8122103214263916, 2.8985228709643707e-05, 0.9729106426239014, 1.0033413673227187e-05, 9.043727914104238e-06, 0.9085102081298828, 0.9395524263381958, 4.037961844005622e-06, 5.34822502231691e-05, 3.2069667668110924e-06, 0.0008841773960739374, 5.266810148896184e-06, 0.15301120281219482, 8.766218525124714e-05, 2.121221950801555e-05, 2.8378556180541636e-06, 5.339566632756032e-05, 0.0002940376289188862, 0.2977895140647888, 1.69255454238737e-05, 1.6059185554695432e-06, 0.9923264980316162, 0.7416223287582397, 0.26026105880737305, 0.296744167804718, 5.008808329876047e-06, 6.762351404177025e-05, 0.005059835035353899, 1.1709366845025215e-06, 4.8072374738694634e-06, 0.004437947180122137, 3.116536163361161e-06, 7.974461186677217e-05, 0.00010986660345224664, 0.9684048891067505, 0.48417356610298157, 0.960161030292511, 1.0421491424494889e-05, 2.0918903373967623e-06, 0.07707192748785019, 0.0060783918015658855, 0.03286603093147278, 4.155074566369876e-05, 0.5383610725402832, 6.01574811298633e-06, 0.7214609384536743, 6.679052603431046e-06, 2.732659959292505e-05, 0.00137059495318681, 0.4435143768787384, 3.026654121640604e-05, 8.175556104106363e-06, 0.00011914065544260666, 1.0504578312975354e-05, 0.6746372580528259, 1.3126306839694735e-05, 0.0002775968750938773, 0.47848233580589294, 1.1964162695221603e-05, 0.552680492401123, 0.852961540222168, 5.996456093271263e-05, 1.3230090189608745e-05, 0.004512957762926817, 0.003234714735299349, 0.00015375828661490232, 0.00043240387458354235, 0.06437257677316666, 9.995480650104582e-05, 0.01149133499711752, 1.596980291651562e-05, 4.8743740990175866e-06, 4.147606887272559e-06, 8.165415783878416e-05, 0.009450125508010387, 0.015080459415912628, 0.5862626433372498, 0.05247573181986809, 0.143134206533432, 1.6141437981787021e-06, 4.408817403600551e-06, 1.926523145812098e-05, 0.09130354970693588, 4.317873390391469e-06, 1.088126464310335e-05, 1.1303254723316059e-05, 3.6755202017957345e-05, 0.2915849983692169, 1.9262233763583936e-06, 0.8124520182609558, 0.03668728098273277, 3.2527536859561224e-06, 0.00020054855849593878, 1.4555133020621724e-05, 4.047357833769638e-06, 0.8030484318733215, 3.9543288039567415e-06, 0.00014850815932732075, 3.8268139178398997e-05, 0.010829400271177292, 2.519169356673956e-05, 0.1192154660820961, 0.0013164213160052896, 1.3246462913230062e-05, 0.0010917831677943468, 2.9165878459025407e-06, 0.527004599571228, 0.005460426677018404, 0.02110324427485466, 1.5199053450487554e-05, 5.149188837094698e-06, 0.00016637459339108318, 7.62090439820895e-06, 2.433342524454929e-05, 0.0002396866329945624, 0.000780807517003268, 5.816656994284131e-06, 0.09666406363248825, 0.13015590608119965, 1.8492519302526489e-06, 0.0109923230484128, 0.08451817184686661, 2.2283276848611422e-05, 0.4895690977573395, 5.246487035037717e-06, 3.908669441443635e-06, 0.15236929059028625, 0.00012293491454329342, 0.00018359524256084114, 9.710660378914326e-05, 0.00010540665243752301, 0.00022298964904621243, 2.281563638462103e-06, 6.0057609516661614e-05, 0.9093768000602722, 0.15693670511245728, 0.005169369280338287, 5.430576493381523e-06, 0.0012005125172436237, 5.618765135295689e-05, 0.260154664516449, 1.757620702846907e-05, 0.0005290685803629458, 2.333135626031435e-06, 2.5384847504028585e-06, 6.283103630266851e-06, 0.5878669023513794, 0.0051345424726605415, 0.005941323004662991, 0.015011489391326904, 0.45535072684288025, 3.7805302781634964e-06, 2.168535957025597e-06, 0.12019163370132446, 3.076456778217107e-05, 4.3298032323946245e-06, 2.310053605469875e-05, 0.15881596505641937, 0.018358051776885986, 1.813554081309121e-05, 7.354264198511373e-06, 0.5110127925872803, 0.09724307805299759, 4.947009074385278e-06, 6.902070254000137e-06, 1.3186467185732909e-05, 1.9364035324542783e-05, 0.03200472146272659, 3.333982931508217e-06, 0.002203388838097453, 4.3872390961041674e-05, 6.8568665483326185e-06, 0.0002356016921112314, 5.770555799244903e-05, 4.9943414524022955e-06, 0.2401028275489807, 8.895388418750372e-06, 0.8982254266738892, 8.805558536550961e-06, 0.00054694595746696, 0.0017618435667827725, 0.17616979777812958, 0.0003313310444355011, 0.8289234042167664, 5.863474143552594e-06, 7.476804603356868e-05, 3.0192020403774222e-06, 5.03965611642343e-06, 7.316938717849553e-05, 0.9779382944107056, 1.59699993673712e-05, 2.7851438062498346e-06, 0.0006101493490859866, 0.05410289391875267, 0.0431380532681942, 0.00697746966034174, 0.05341142788529396, 0.22890427708625793, 0.5842360854148865, 0.018774230033159256, 3.1866929930401966e-06, 2.248206510557793e-05, 0.016880668699741364, 0.48787057399749756, 0.0007156258798204362, 3.094123712799046e-06, 3.389752237126231e-06, 0.5627654194831848, 0.0010783434845507145, 9.337935807707254e-06, 1.986990810110001e-06, 0.002160318661481142, 0.1041455939412117, 4.082743998878868e-06, 0.0059468927793204784, 2.4024704998737434e-06, 0.37584060430526733, 3.608225233620033e-05, 0.8935143947601318, 5.230785609455779e-05, 0.04970801994204521, 2.5347085284010973e-06, 2.2673781131743453e-05, 0.009156569838523865, 0.0020693375263363123, 0.06262554973363876, 7.327237835852429e-05, 3.1252448025043122e-06, 9.33859519136604e-06, 0.23718085885047913, 0.008252906613051891, 6.769313040422276e-05, 6.0219914303161204e-05, 0.00016363982285838574, 1.796531432773918e-05, 9.47803414419468e-07, 3.4822246561816428e-06, 0.02669488824903965, 0.0004805701901204884, 0.284047394990921, 2.5000063033076003e-06, 8.418801735388115e-05, 5.7519366237102076e-05, 0.9298338890075684, 2.668836941666086e-06, 0.00022664625430479646, 0.5369710326194763, 2.2528179215441924e-06, 0.003270287998020649, 0.00033650113618932664, 0.0012723833788186312, 1.33861055928719e-06, 0.7729898691177368, 0.0003375698288436979, 0.001476423698477447, 0.2890385389328003, 6.254092568269698e-06, 1.8097767906510853e-06, 0.3698892593383789, 0.7522967457771301, 2.719448502830346e-06, 2.610140427350416e-06, 2.4520551960449666e-05, 0.0004282472073100507, 1.9406259525567293e-05, 0.001914655789732933, 0.061919402331113815, 0.0025022609625011683, 0.00012521864846348763, 2.5378892587468727e-06, 0.20914621651172638, 0.5694841146469116, 0.5223614573478699, 4.610601899912581e-05, 0.0008525642333552241, 8.997782060760073e-06, 0.0006526545039378107, 5.527571920538321e-05, 1.095171319320798e-05, 9.051278902916238e-05, 6.165816103020916e-06, 1.0329321412427817e-05, 0.7801693081855774, 0.00019908667309209704, 2.802037215587916e-06, 3.891967935487628e-06, 3.5793043480225606e-06, 0.5865657925605774, 0.017190076410770416, 0.05360322818160057, 0.0016832782421261072, 7.212987111415714e-05, 0.9153780937194824, 1.7158678247142234e-06, 0.49476107954978943, 3.162664870615117e-05, 6.641304207732901e-05, 0.7976759672164917, 1.4704253317177063e-06, 1.6354385934391757e-06, 0.37461766600608826, 0.026122886687517166, 0.7781155705451965, 5.221419996814802e-06, 1.6275454299830017e-06, 4.011424152849941e-06, 1.037355832522735e-05, 1.026407062454382e-05, 0.0012404922163113952, 3.720978611454484e-06, 0.7331746220588684, 1.12192094547936e-06, 0.012662505730986595, 1.268680534849409e-06, 2.0663819668698125e-06, 1.6556006130485912e-06, 2.293439820277854e-06, 0.9016118049621582, 3.557914851626265e-06, 0.00021554702834691852, 0.00032330083195120096, 0.0041930051520466805, 0.10360749065876007, 6.3573552324669436e-06, 0.10134778916835785, 0.12096883356571198, 0.00032716640271246433, 7.049909618217498e-05, 3.3459944006608566e-06, 5.135482206242159e-05, 3.311004547867924e-06, 0.9148073792457581, 0.880746066570282, 1.983100264624227e-05, 0.0006903677713125944, 5.52117944607744e-06, 4.649404672818491e-06, 0.36896753311157227, 0.0013989759609103203, 0.6409865617752075, 0.0010858842870220542, 0.00042801257222890854, 0.5924237966537476, 5.52647361473646e-06, 0.9031887054443359, 0.0009054883848875761, 0.0006766936276108027, 0.00012697400234173983, 0.00023997612879611552, 4.5625765778822824e-05, 0.1190214678645134, 0.7863043546676636, 3.470746605671593e-06, 1.1462797374406364e-05, 9.826917448663153e-06, 2.913682465077727e-06, 0.00018380086112301797, 0.0011622493620961905, 0.05681559443473816, 0.0024196323938667774, 0.0018204636871814728, 0.0990016981959343, 9.588213742972584e-07, 3.154809746774845e-06, 1.644211806706153e-05, 0.7545108795166016, 0.23203352093696594, 0.0026502651162445545, 4.958872887073085e-05, 0.007971866056323051, 1.6939425222517457e-06, 8.157729098456912e-06, 0.01961328648030758, 2.7562291506910697e-06, 1.6703284927643836e-05, 5.2726241847267374e-05, 0.011056367307901382, 0.4897770285606384, 0.00020305856014601886, 1.1908793567272369e-05, 4.846340289077489e-06, 0.7552121877670288, 6.283860420808196e-05, 0.0001525687548564747, 0.004580129403620958, 2.7532264539331663e-06, 1.6511115745743155e-06, 1.2322311704338063e-05, 2.850470264093019e-05, 3.722992551047355e-05, 3.066328872591839e-06, 5.006431456422433e-05, 7.649675535503775e-05, 2.6690056984080002e-05, 2.019118937823805e-06, 0.0007994103361852467, 8.372833690373227e-06, 8.391443770960905e-06, 0.00057718635071069, 0.19110630452632904, 1.2321054782660212e-05, 2.762261647148989e-05, 3.2906282285694033e-06, 6.875041435705498e-05, 0.010282321833074093, 5.6533346651121974e-05, 0.08237563818693161, 4.052997610415332e-05, 6.560189376614289e-06, 2.301829908901709e-06, 0.005528199952095747, 1.9506323951645754e-06, 2.610123146951082e-06, 0.0014059051172807813, 2.3681916445639217e-06, 3.904247023456264e-06, 8.379019709536806e-05, 0.6923174858093262, 3.771883712033741e-06, 0.05859210714697838, 2.5735398594406433e-05, 0.0007383289630524814, 0.0006215885514393449, 0.0007254566880874336, 0.00014276267029345036, 7.0321552811947186e-06, 1.854067477324861e-06, 0.0008010605233721435, 3.749787083506817e-06, 4.055282624904066e-05, 2.7799198505817913e-05, 7.842498234822415e-06, 2.9020684451097623e-06, 6.656363257206976e-05, 0.31096458435058594, 2.2996466668701032e-06, 8.219186565838754e-05, 2.4762330212979577e-06, 0.00022720838023815304, 1.5312782579712803e-06, 4.4127922592451796e-05, 3.723307145264698e-06, 3.2141774681804236e-06, 2.107495220116107e-06, 2.0936962755513377e-05, 1.0240361916658003e-05, 6.238203786779195e-05, 0.00014543898578267545, 3.7110662560735364e-06, 4.23371284341556e-06, 1.6147304222613457e-06, 0.00031996131292544305, 8.36533508845605e-05, 4.732676188723417e-06, 0.001074425526894629, 2.940325612144079e-05, 3.7131655972189037e-06, 0.0016608292935416102, 0.00010282671428285539, 0.0005823480314575136, 3.2378268315369496e-06, 1.2140729268139694e-05, 0.0007951224106363952, 5.326565769792069e-06, 2.4492449028912233e-06, 8.249820893979631e-06, 0.020628951489925385, 2.2632643776887562e-06, 1.3530947171602747e-06, 0.8998381495475769, 2.1072273739264347e-05, 3.80963660973066e-06, 0.0003493232070468366, 8.40506618260406e-06, 3.925752935174387e-06, 1.815652694858727e-06, 0.0031710455659776926, 0.003936998546123505, 2.3630366285942728e-06, 2.1613343506032834e-06, 8.830635124468245e-06, 4.069997521582991e-05, 5.220469120104099e-06, 3.2196689971897285e-06, 2.4870560082490556e-06, 0.006447378545999527, 0.0006164299556985497, 5.955294182058424e-05, 4.2386836867081e-05, 0.3738027513027191, 0.8334943652153015, 0.716462254524231, 4.332607659307541e-06, 7.975693006301299e-05, 0.003958425484597683, 0.00015579759201500565, 5.693202183465473e-06, 4.95903441333212e-06, 3.9779042708687484e-06, 1.9646699001896195e-05, 2.938924126283382e-06, 2.3750157197355293e-05, 1.3922476682637352e-06, 4.829587851418182e-06, 0.00024802685948088765, 1.9367687400517752e-06, 0.0001714189857011661, 3.9049509723554365e-06, 3.148818723275326e-05, 4.568687927530846e-06, 8.762449579080567e-05, 2.353390527787269e-06, 0.0001688860938884318, 1.3613353985419963e-05, 0.07665164023637772, 0.8080877661705017, 0.008206083439290524, 0.7479724287986755, 2.847030555130914e-05, 1.3424496501102112e-06, 0.0005213904078118503, 4.4906573748448864e-05, 2.4044367819442414e-05, 1.9688184238475515e-06, 1.697741595307889e-06, 6.089887392590754e-06, 2.100362280543777e-06, 4.468625775189139e-06, 6.938797741895542e-05, 0.1907099336385727, 2.5091690076806117e-06, 3.795815700868843e-06, 4.5967684854986146e-05, 2.3391351078316802e-06, 0.0005978163098916411, 0.0011022555408999324, 9.416468856215943e-06, 2.64581080955395e-06, 2.871463266274077e-06, 1.9806716409220826e-06, 4.296006227377802e-05, 0.0006686965934932232, 0.27130308747291565, 0.1484696865081787, 4.8543797674938105e-06, 6.817132998548914e-06, 6.581915840797592e-06, 2.0187010250083404e-06, 1.794330273696687e-06, 0.0002922645362559706, 0.0049680122174322605, 0.03999818488955498, 4.3657408241415396e-05, 0.0002265687071485445, 0.0007733256788924336, 0.004235831089317799, 8.88829049472406e-07, 4.5105378376320004e-05, 1.1664044905046467e-05, 0.005436660721898079, 9.257026249542832e-06, 1.0949154784611892e-05, 4.3771660784841515e-06, 2.014700612562592e-06, 0.0007295950781553984, 1.8065442418446764e-05, 9.986475561163388e-06, 1.861322402874066e-06, 6.292584203038132e-06, 1.1084421203122474e-06, 0.10543964803218842, 2.219526777480496e-06, 7.272613856912358e-06, 0.00022976507898420095, 4.931059811497107e-05, 4.0009283111430705e-05, 3.6396879295352846e-05, 6.535742159030633e-06, 0.00061674730386585, 4.382174211059464e-06, 0.0001723332970868796, 0.00011895913485204801, 1.4360931345436256e-05, 2.2613398868998047e-06, 4.887719569524052e-06, 6.16170436842367e-05, 6.498372385976836e-05, 0.00016588225844316185, 0.07517701387405396, 0.002209912985563278, 2.4504188331775367e-05, 6.621813099627616e-06, 3.331081416035886e-06, 2.8358533654682105e-06, 0.777946412563324, 0.33507877588272095, 0.0003386993776075542, 0.0027021332643926144, 6.795446552132489e-06, 3.2483640097780153e-06, 7.081935473252088e-05, 0.023382455110549927, 9.978687558032107e-06, 4.590428488882026e-06, 0.9554732441902161, 1.8073660612571985e-05, 0.00019632693147286773, 0.0003109263489022851, 3.382728209544439e-06, 0.0008480690885335207, 2.44715283770347e-06, 2.4201244741561823e-06, 3.280556120444089e-05, 2.4430864868918434e-06, 3.0472718208329752e-05, 2.2371737031789962e-06, 1.0510200809221715e-05, 2.5424460545764305e-06, 8.93811920832377e-06, 3.0220596727303928e-06, 1.0259032023895998e-05, 5.064467040938325e-05, 0.0017707186052575707, 2.5785632260522107e-06, 8.3678442024393e-06, 1.9668255845317617e-05, 3.838941847789101e-05, 4.082892246515257e-06, 1.7965820688914391e-06, 4.156089562457055e-05, 1.7610955183045007e-06, 1.699421954981517e-05, 1.1771411664085463e-05, 7.79465917730704e-05, 8.24288417788921e-06, 6.817283428972587e-05, 0.7081093788146973, 0.5752283334732056, 6.950812348804902e-06, 0.01848309300839901, 3.370060994711821e-06, 2.3889497242635116e-06, 2.539067281759344e-05, 8.452516340184957e-06, 0.0020361403003335, 4.084075953869615e-06, 3.585132617445197e-06, 6.726277933921665e-05, 9.686402336228639e-06, 0.004979254677891731, 4.386606178741204e-06, 1.3844051863998175e-05, 1.9333738237037323e-05, 7.380323495453922e-06, 0.885578453540802, 0.5720977187156677, 4.4437183532863855e-06, 3.626113311838708e-06, 1.983851461773156e-06, 6.262166607484687e-06, 0.0001664801238803193, 0.0034013271797448397, 4.6949598981882446e-06, 2.2488304239232093e-05, 3.443146124482155e-05, 0.006341640371829271, 3.3697267554089194e-06, 5.429002158052754e-06, 7.245594133564737e-06, 7.354054832831025e-05, 0.018038153648376465, 2.600085963422316e-06, 0.4422224462032318, 1.2391638847475406e-05, 0.00012239661009516567, 0.00013093835150357336, 7.166071736719459e-05, 2.210886350439978e-06, 6.615199708903674e-06, 3.1423367090610554e-06, 2.110416789946612e-05, 2.8581118840520503e-06, 0.0013660052791237831, 3.813367584371008e-05, 3.7061186048958916e-06, 1.2927249599670176e-06, 2.706751956793596e-06, 1.7564191239216598e-06, 5.874931503058178e-06, 2.5017450752784498e-05, 4.680145138991065e-06, 0.00011990529310423881, 0.9311376810073853, 1.1563226507860236e-05, 2.8479464617703343e-06, 0.00025513701257295907, 4.371285467641428e-05, 2.8143242616351927e-06, 2.1860817014385248e-06, 2.5021792680490762e-05, 1.4320504533316125e-06, 7.80456539359875e-05, 0.0001422974601155147, 1.1475824067019857e-05, 0.0014462287072092295, 5.935872650297824e-06, 2.773656206045416e-06, 0.002560208085924387, 3.337291218485916e-06, 0.8252475261688232, 3.071332321269438e-05, 0.0006001055589877069, 0.0009492993704043329, 2.6468960641068406e-06, 0.0011679059825837612, 2.8466774892876856e-05, 6.6563643486006185e-06, 0.01642497070133686, 1.833008923313173e-06, 7.295250543393195e-05, 5.093345407658489e-06, 2.294802470714785e-05, 6.3094275901676156e-06, 6.0627044149441645e-06, 3.831866706605069e-06, 0.008181705139577389, 2.662321321622585e-06, 2.5668363377917558e-05, 0.0005444357520900667, 3.188553364452673e-06, 0.0029280860908329487, 0.00011630864173639566, 0.0002421067765681073, 0.00015735402121208608, 2.341690787943662e-06, 1.3395450878306292e-05, 1.232306476595113e-05, 1.915316943268408e-06, 6.476580892922357e-06, 0.0001629189937375486, 2.6505992991587846e-06, 5.255099495116156e-06, 0.009059654548764229, 3.1120753192226402e-06, 3.12273004965391e-06, 0.9263060092926025, 0.006039369851350784, 3.1466279324376956e-06, 0.00011875694326590747, 0.0010297833941876888, 0.00012803297431673855, 4.901779448118759e-06, 0.00012803297431673855, 3.188285518263001e-06, 2.367401293668081e-06, 2.6381592306279344e-06, 3.889648723998107e-05, 2.5269102934544208e-06, 0.00021287902200128883, 3.95232746086549e-05, 0.19231295585632324, 0.3455067574977875, 0.16444526612758636, 0.8882797956466675, 0.00031134934397414327, 3.0126273031783057e-06, 9.86582108453149e-06, 1.6876891777428682e-06, 4.022188477392774e-06, 3.2547739010624355e-06, 0.022877182811498642, 2.7487338229548186e-05, 1.0608462616801262e-05, 0.0022742804139852524, 1.570886865920329e-06, 3.512094053803594e-06, 0.000319625367410481, 1.2697977354036993e-06, 0.010804490186274052, 1.948992576217279e-05, 5.616388989437837e-06, 0.03553185984492302, 2.545073357396177e-06, 0.00010654878860805184, 3.418880805838853e-05, 1.2860001561421086e-06, 4.758367140311748e-05, 2.1864525479031727e-06, 0.00795808993279934, 0.00013580784434452653, 1.9357162273081485e-06, 4.5810498704668134e-05, 0.0006176908500492573, 1.808322281249275e-06, 0.00017951616609934717, 0.00035902485251426697, 2.4868470518413233e-06, 2.2226229702937417e-05, 4.134941264055669e-06, 2.578730118329986e-06, 0.04381624236702919, 3.708929398271721e-06, 2.9328315577004105e-05, 2.262526095364592e-06, 3.414764933040715e-06, 6.809848855482414e-06, 1.525688048786833e-06, 0.0003288121079094708, 3.155792728648521e-05, 0.003621534211561084, 3.2600362374068936e-06, 1.4705932699143887e-05, 5.042636075813789e-06, 0.0004242938302922994, 6.923457112861797e-05, 9.33702103793621e-05, 4.3638365241349675e-06, 0.0015651159919798374, 1.3740606163992197e-06, 0.3771493434906006, 1.6987764865916688e-06, 5.4038035159464926e-05, 5.68990708416095e-06, 7.940180694276933e-06, 0.2550536096096039, 1.827333471737802e-05, 0.0001513559400336817, 0.002456569578498602, 2.740695526881609e-05, 7.998550427146256e-05, 8.00190719019156e-06, 1.955208517756546e-06, 2.3284967483050423e-06, 1.735577461658977e-05, 0.0009042907622642815, 0.0001756999990902841, 1.1584827007027343e-05, 0.7307467460632324, 0.00021996973373461515, 2.9178450859035365e-05, 3.719255983014591e-05, 0.00025379922590218484, 1.1654826266749296e-05, 2.820415102178231e-05, 2.6021300527645508e-06, 2.9362490749917924e-06, 2.804253426802461e-06, 0.0015187712851911783, 1.4310047845356166e-05, 1.8644029751158087e-06, 1.45648673424148e-05, 0.00014820134674664587, 2.8868762456113473e-05, 5.611447249975754e-06, 1.1582540537347086e-05, 1.891571082524024e-05, 0.0018935413099825382, 1.554727896291297e-05, 0.22324620187282562, 3.512549255901831e-06, 0.6811045408248901, 3.498955493341782e-06, 2.6963070922647603e-06, 4.121213351027109e-06, 0.9140765070915222, 8.226840691349935e-06, 2.754405613814015e-06, 0.00014487786393146962, 3.102050868619699e-06, 3.1552431210002396e-06, 0.00015406485181301832, 2.7761093406297732e-06, 1.821730620577e-05, 1.4681492757517844e-05, 5.502272415469633e-06, 3.2278589969791938e-06, 6.448790827562334e-06, 4.701726084022084e-06, 6.626033155043842e-06, 2.5317507606814615e-05, 5.472223710967228e-05, 9.677833077148534e-06, 2.5877103325910866e-05, 2.0492886960710166e-06, 1.818628220462415e-06, 1.215183874592185e-05, 0.00026497579528950155, 0.715893566608429, 1.933264456965844e-06, 6.12700532656163e-05, 0.0012413124786689878, 2.816445203279727e-06, 0.02481006272137165, 0.001704229973256588, 4.999723387300037e-05, 1.7477971141488524e-06, 2.3357839381787926e-05, 1.9978003820142476e-06, 0.022858334705233574, 1.87503537745215e-05, 2.5579829525668174e-05, 2.2324178644339554e-05, 0.0005982232978567481, 5.1737202738877386e-05, 4.083269686816493e-06, 2.267853460580227e-06, 1.20088554922404e-06, 0.00013178838707972318, 3.172509605064988e-05, 1.4271640793594997e-06, 2.6119807898794534e-06, 6.542726623592898e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking:  67%|██████▋   | 2/3 [00:30<00:14, 14.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9953970313072205, 2.01152920453751e-06, 0.7174405455589294, 0.8589284420013428, 6.07621950621251e-06, 0.0001394230785081163, 1.5411380445584655e-05, 0.16442130506038666, 0.006078634411096573, 0.13731539249420166, 0.8877601623535156, 9.802843851502985e-05, 1.456777044950286e-05, 0.0008441982208751142, 0.00040752513450570405, 0.1008419543504715, 1.0655906407919247e-05, 0.002407826716080308, 0.0008673791890032589, 0.8346805572509766, 0.0061761499382555485, 0.4766225814819336, 0.00020182768639642745, 0.819810688495636, 0.3919393718242645, 0.06538358330726624, 0.00026937693473882973, 4.998672466172138e-06, 0.0003038840659428388, 6.597803803742863e-06, 0.01524920854717493, 0.00011770183482440189, 2.0283585399738513e-05, 2.350113027205225e-05, 0.024353962391614914, 1.7835785683928407e-06, 7.114362233551219e-06, 0.4937599301338196, 1.3999332622915972e-05, 0.010468983091413975, 1.4748639841855038e-05, 2.040050458163023e-05, 2.4147428121068515e-05, 2.1775629647891037e-05, 0.00037814624374732375, 0.00013539304200094193, 0.8553444147109985, 2.1106765416334383e-05, 0.0012444118037819862, 0.002643442479893565, 2.349998612771742e-05, 5.7489078244543634e-06, 0.001704434514977038, 0.01121425535529852, 0.0040554204024374485, 1.3606292668555398e-05, 1.3224425856606103e-05, 0.003415321232751012, 0.8373131155967712, 1.4372288205777295e-05, 1.2281437193450984e-05, 3.1584004318574443e-05, 1.5077680473041255e-06, 4.994090340915136e-05, 3.210036811651662e-05, 1.1185470611962955e-05, 0.09833581745624542, 2.7400656108511612e-05, 0.00021803034178446978, 6.354830838972703e-05, 0.00014434214972425252, 0.001586282392963767, 4.132414233026793e-06, 0.018411785364151, 0.004872797057032585, 5.0941180234076455e-06, 0.00424278574064374, 0.8532323837280273, 2.6311440706194844e-06, 0.004973096773028374, 0.2396276295185089, 3.841068519250257e-06, 0.003676380729302764, 0.0005723937647417188, 2.5650868337834254e-05, 0.014250505715608597, 0.0004980946541763842, 0.00018346942670177668, 8.79705396528152e-07, 0.00016688606410752982, 8.380714280065149e-05, 2.6378287657280453e-05, 0.14273986220359802, 2.3342618078459054e-06, 0.00019241419795434922, 0.00013517709157895297, 0.03893851861357689, 0.0004822662740480155, 0.37490278482437134, 6.617823146370938e-06, 8.153428098012228e-06, 0.0004378654411993921, 0.00015925457410048693, 4.360304956207983e-06, 7.74086970523058e-07, 0.00016175249766092747, 0.4734606146812439, 3.6627909139497206e-05, 0.0015408239560201764, 8.563738811062649e-05, 0.11821818351745605, 1.6294419765472412e-05, 1.5631203496013768e-06, 1.6373830931115663e-06, 5.592497018369613e-06, 0.005838512443006039, 3.5096929877909133e-06, 0.0001500394137110561, 2.334122655156534e-05, 2.4999228116939776e-05, 0.0004433743888512254, 0.0007218155660666525, 2.4247126475529512e-06, 0.003287479979917407, 0.846879243850708, 1.7377485619363142e-06, 0.00010747247142717242, 0.004230820573866367, 0.0008039535023272038, 3.590398591768462e-06, 2.2468821043730713e-05, 0.1302378624677658, 1.502401346442639e-06, 8.316713501699269e-05, 1.8161850675824098e-05, 1.707085539237596e-05, 1.0951398508041166e-06, 1.2103780591132818e-06, 0.9367306232452393, 0.0005656119901686907, 1.9911483832402155e-05, 0.9690978527069092, 0.09337440878152847, 5.074035016150447e-06, 3.308054510853253e-05, 0.0007266454049386084, 0.010639763437211514, 5.676309592672624e-05, 0.0004196994996163994, 5.206368768995162e-06, 0.007587259169667959, 0.010562662035226822, 0.41693609952926636, 2.978801376229967e-06, 0.9537620544433594, 8.579828318033833e-06, 1.0399955499451607e-05, 2.202382165705785e-05, 0.0004950058064423501, 0.00015829155745450407, 2.3359566512226593e-06, 0.01356060616672039, 0.05868690460920334, 0.0003957358712796122, 8.386141416849568e-05, 0.0005889351014047861, 0.0006332476623356342, 2.9053942398604704e-06, 9.594785296940245e-06, 0.0031704457942396402, 0.00023354253789875656, 4.12339932154282e-06, 0.0005228068330325186, 0.9529803991317749, 0.0006491117528639734, 1.015528596326476e-05, 0.6548983454704285, 0.13913816213607788, 8.139979399857111e-06, 0.000513367005623877, 1.2774925153280492e-06, 3.5283992474433035e-06, 8.214829904318321e-06, 2.29018296522554e-05, 2.2187543891050154e-06, 1.5282918184311711e-06, 1.4020012713444885e-05, 6.738771389791509e-06, 0.00011527890455909073, 0.00011017968063242733, 1.3172855688026175e-05, 0.0010631592012941837, 2.2784239263273776e-05, 1.1938400348299183e-06, 1.701214841887122e-06, 7.876806921558455e-05, 0.00012287723075132817, 0.00032137465314008296, 1.031867850542767e-05, 0.0017679695738479495, 3.635992470663041e-06, 1.511142272647703e-06, 3.1306349228543695e-06, 7.639383693458512e-05, 5.3812484111404046e-05, 4.272567093721591e-05, 1.0959704468405107e-06, 0.829278826713562, 1.5657672065572115e-06, 0.0017179757123813033, 0.05438615754246712, 0.00029859060305170715, 3.7895542845944874e-06, 0.015851594507694244, 0.0002583542373031378, 0.5770145058631897, 0.0008800085633993149, 0.00020358472829684615, 0.0014741942286491394, 5.7556233514333144e-05, 6.2800727391731925e-06, 0.00020998777472414076, 2.6257396257278742e-06, 0.0003300479147583246, 4.068716589245014e-05, 1.1853512660309207e-05, 0.00047326975618489087, 0.0026086061261594296, 4.447179890121333e-05, 7.923364319140092e-05, 2.554164666435099e-06, 2.652475586728542e-06, 0.015015813522040844, 1.3958213003206765e-06, 6.7664104790310375e-06, 1.875684802143951e-06, 9.740062523633242e-06, 0.8135697841644287, 2.2647285732091404e-05, 0.0003283375408500433, 0.0025034775026142597, 0.022501101717352867, 0.3115917146205902, 3.2255420592264272e-06, 4.1908060666173697e-05, 1.3862189916835632e-05, 3.3765612897695974e-05, 1.779397098289337e-05, 0.0001376518775941804, 1.3959570424049161e-06, 0.0003144487563986331, 8.284646355605219e-06, 0.2129327356815338, 7.3639839683892205e-06, 9.836499884841032e-06, 1.5900930520729162e-05, 1.0684169637897867e-06, 8.378752681892365e-06, 2.063873125734972e-06, 0.0008264033240266144, 1.8608945993037196e-06, 1.0936766557279043e-05, 0.001661042682826519, 1.6007733165679383e-06, 3.170985291944817e-05, 1.3407109236140968e-06, 1.9609542505349964e-05, 9.216028411174193e-05, 0.003312627086415887, 0.5715811848640442, 0.2088630646467209, 0.00022745723254047334, 1.7849495634436607e-05, 0.0006787287420593202, 0.015664134174585342, 1.578832780069206e-05, 0.28380054235458374, 0.0033496972173452377, 1.3929529814049602e-06, 0.23231081664562225, 0.019484085962176323, 4.4140510908619035e-06, 4.180069026915589e-06, 2.34697608902934e-06, 1.987635187106207e-06, 2.5844768970273435e-05, 0.0003915902052540332, 6.682202365482226e-05, 0.00045132244122214615, 0.14840422570705414, 8.59566280269064e-05, 0.0020986103918403387, 4.561161404126324e-06, 0.3425801694393158, 0.0004449269617907703, 0.03067455254495144, 1.3647336345457006e-06, 0.4325835704803467, 0.0009054693509824574, 3.856002877000719e-06, 5.3397643569041975e-06, 1.3340263649297412e-06, 1.2358701724224375e-06, 0.0001510360452812165, 0.0001346573408227414, 2.7645173759083264e-05, 5.032139961258508e-05, 1.4097585108174826e-06, 3.024472789547872e-06, 0.8486472368240356, 1.4759377336304169e-05, 3.156768070766702e-05, 4.204661763651529e-06, 0.0003001000441145152, 0.0006480004522018135, 0.04799051210284233, 6.417734402930364e-05, 0.18409642577171326, 1.2787040759576485e-06, 2.718512223509606e-05, 1.4885791642882396e-05, 0.00018147847731597722, 0.29909008741378784, 2.7512762699188897e-06, 8.904520655050874e-06, 5.6167318689404055e-06, 1.1090448424511123e-05, 1.944089490280021e-06, 5.93079676036723e-06, 0.0005302039789967239, 9.451767255086452e-05, 1.9350313777977135e-06, 2.89312083623372e-06, 0.0010420592734590173, 2.4597668470960343e-06, 1.3978302604300552e-06, 3.561543417163193e-05, 1.5101639974091086e-06, 0.0001225353335030377, 8.18278749648016e-06, 2.790717189782299e-05, 4.475860987440683e-05, 1.5526327388215577e-06, 0.6626258492469788, 0.3611733913421631, 0.00039411141187883914, 4.788586920767557e-06, 3.9129048673203215e-05, 2.197020876337774e-06, 1.3655095472131507e-06, 1.6221218857026543e-06, 6.498376478702994e-06, 0.00024524086620658636, 2.185843868574011e-06, 3.346339326526504e-06, 0.0002842701505869627, 0.00044131831964477897, 0.0013724092859774828, 0.00016799711738713086, 4.03036347051966e-06, 0.0008691645343787968, 1.2600286936503835e-05, 0.3671541213989258, 4.700878889707383e-06, 7.314626145671355e-06, 3.519399569995585e-06, 4.803186584467767e-06, 2.3971426799107576e-06, 1.2747622349706944e-05, 0.08215436339378357, 0.00015397336392197758, 1.823831780711771e-06, 1.1507926274134661e-06, 0.02590256556868553, 2.3282500478671864e-06, 0.007044896949082613, 1.0858158020710107e-05, 5.718082684325054e-05, 2.509195383026963e-06, 2.9833954613422975e-06, 1.619695649424102e-05, 7.983932846400421e-06, 5.268242148304125e-06, 3.650864164228551e-05, 1.893280386866536e-05, 0.0285304244607687, 2.059828375422512e-06, 9.13202438823646e-06, 4.6078366722213104e-06, 0.29763665795326233, 3.280731471022591e-05, 8.33170852274634e-05, 0.9913026094436646, 4.117009666515514e-05, 8.041917681111954e-06, 1.4501484656648245e-05, 1.6476510609209072e-06, 4.62645803054329e-05, 8.417267963523045e-05, 1.1076358532591257e-06, 1.5682195453337044e-06, 1.8486402041162364e-05, 2.2976579202804714e-05, 3.2350858418794814e-06, 1.8832575960914255e-06, 0.0006117208977229893, 0.00031302470597438514, 2.975490588141838e-06, 0.0007639552932232618, 0.00012907553173135966, 7.580074452562258e-05, 5.178410901862662e-06, 2.1385192667366937e-05, 1.672399434937688e-06, 1.822205013013445e-05, 9.579274546922534e-07, 3.23763219967077e-06, 3.94962899008533e-06, 3.1376290280604735e-05, 2.9258060294523602e-06, 0.000258212152402848, 1.0177672265854198e-05, 1.2541775049612625e-06, 5.66934113521711e-06, 5.568256256083259e-06, 9.511405551165808e-06, 0.052981916815042496, 4.788130354427267e-06, 0.0002398473588982597, 2.3040811356622726e-06, 1.2601960861502448e-06, 0.001127080642618239, 4.968539997207699e-06, 1.3308660982147558e-06, 3.0739327030460117e-06, 2.3067148049449315e-06, 1.5174187865341082e-05, 2.1406915493571432e-06, 3.3167691526614362e-06, 6.293100796028739e-06, 2.572815901658032e-05, 0.5532324314117432, 1.888507540570572e-06, 8.668082955409773e-06, 0.02301851101219654, 2.526847765693674e-06, 1.9833123587886803e-06, 1.619283102627378e-05, 1.8423096435071784e-06, 2.7829189548356226e-06, 2.1981923055136576e-06, 0.0009413895895704627, 2.5141607693512924e-06, 3.0128426260489505e-06, 2.6945465378958033e-06, 3.825043222605018e-06, 0.011608047410845757, 0.0044807507656514645, 5.747988689108752e-05, 4.0860513763618656e-06, 2.1844475668331143e-06, 3.660426727947197e-06, 1.2989525203010999e-05, 1.5084961887623649e-05, 2.873147877835436e-06, 0.0002678408636711538, 0.13844627141952515, 7.515827019233257e-05, 1.2207835879962659e-06, 5.616785074380459e-06, 1.2462079212127719e-05, 0.00010230971383862197, 1.774007614585571e-06, 5.939864422543906e-06, 0.48994728922843933, 7.312136403925251e-06, 0.10674827545881271, 1.9315069948788732e-05, 1.998398147406988e-05, 2.9921459372417303e-06, 1.2802312085113954e-05, 2.067091736535076e-05, 1.9371111193322577e-05, 2.313747063453775e-05, 2.70140867542068e-06, 1.6665415387251414e-05, 1.1060018550779205e-05, 1.835479270084761e-05, 1.0981520063069183e-05, 1.5041349797684234e-05, 1.7381331645083264e-06, 2.9221640943433158e-05, 0.0008426398853771389, 4.4668449845630676e-06, 2.6378791517345235e-05, 0.0418916754424572, 3.94890594179742e-06, 1.8111667259290698e-06, 2.3982609036465874e-06, 1.1772395964726456e-06, 1.9059112901231856e-06, 0.001118289539590478, 4.043553417432122e-06, 3.688048764161067e-06, 1.7483958799857646e-05, 1.7977774405153468e-05, 1.255023448720749e-06, 1.0586808230073075e-06, 3.526905629769317e-06, 2.103689894283889e-06, 1.5645969142497052e-06, 1.2873683772340883e-06, 0.0017716948641464114, 3.263064718339592e-05, 0.00019934061856474727, 0.0004353864642325789, 5.399821020546369e-05, 1.114954784497968e-06, 7.555314368801191e-05, 0.00012538675218820572, 9.314996532339137e-06, 8.479320968035609e-06, 1.7422239579900634e-06, 5.907726063014707e-06, 7.2616080615262035e-06, 1.6718360711820424e-05, 8.798824069344846e-07, 3.7201283703325316e-05, 0.02221876010298729, 3.760967729249387e-06, 1.202384737553075e-05, 1.0532225132919848e-05, 5.161081389815081e-06, 0.013300600461661816, 4.007757070212392e-06, 0.00043983454816043377, 3.299165882708621e-06, 0.0001376777363475412, 5.9760594012914225e-05, 1.7220744439327973e-06, 0.03410586714744568, 2.685762410692405e-06, 3.7254951621434884e-06, 1.1313491086184513e-06, 1.2764111261276412e-06, 1.1680419447657187e-05, 1.4935311583030852e-06, 0.027572866529226303, 2.218138661191915e-06, 2.2793497919337824e-05, 0.0009388878243044019, 1.7050268752427655e-06, 4.3760728658526205e-06, 8.066599548328668e-05, 0.0006638870690949261, 0.0004376968427095562, 1.4350485798786394e-05, 3.347797610331327e-05, 1.4100788575888146e-05, 4.190778327028966e-06, 2.503494442862575e-06, 2.383917490078602e-06, 1.9013889414054574e-06, 0.4896147847175598, 1.4067249139770865e-05, 7.54573602534947e-06, 6.983209459576756e-06, 0.00021188009122852236, 3.0539172257704195e-06, 5.380659786169417e-06, 1.9149269064655527e-05, 1.2275069138922845e-06, 4.022689972771332e-05, 0.0007820436730980873, 6.4423045841977e-05, 3.507058136165142e-05, 0.002275468548759818, 0.006332785356789827, 1.8338500922254752e-06, 5.067946403869428e-06, 4.82793484479771e-06, 7.191534223238705e-06, 0.00036231332342140377, 6.317513179965317e-05, 7.69920734455809e-06, 3.366440751051414e-06, 0.00010660124098649248, 2.0248795408406295e-05, 1.8849051457436872e-06, 2.173266238969518e-06, 7.023328635114012e-06, 9.57284191827057e-06, 5.166138635104289e-06, 1.1431650818849448e-05, 9.451611731492449e-06, 7.93573690316407e-06, 7.188681593106594e-06, 2.44256921178021e-06, 4.330641331762308e-06, 5.1820152293657884e-05, 0.00012030006473651156, 4.535660082183313e-06, 2.7073560886492487e-06, 2.6959092792822048e-05, 1.1927035302505828e-05, 1.4566227946488652e-05, 0.07012510299682617, 2.859316737158224e-06, 4.077887933817692e-05, 0.00022665575670544058, 0.03406710922718048, 4.396682925289497e-06, 0.0002613913675304502, 1.261662782781059e-05, 2.1226596800261177e-06, 6.44612664473243e-05, 4.460778654902242e-06, 7.873390131862834e-05, 4.861278284806758e-06, 9.168466590381286e-07, 1.116588236982352e-06, 1.3244227830000455e-06, 0.0005832111928611994, 1.6782048987806775e-05, 0.00015256977349054068, 1.1120071121695219e-06, 1.2701561900030356e-06, 1.7124153828262934e-06, 1.7864038454717956e-05, 1.8266695406055078e-05, 1.7524203030916397e-06, 1.2971497653779807e-06, 1.2664876294365968e-06, 1.0696087429096224e-06, 9.79010678747727e-07, 1.2505563518061535e-06, 0.000603370601311326, 2.568064473962295e-06, 1.8647300521479337e-06, 1.67426132975379e-05, 2.125323362633935e-06, 8.296270266328065e-07, 2.832091240634327e-06, 1.5041967344586737e-05, 9.37292895741848e-07, 9.50987305259332e-06, 4.4771391003450844e-06, 2.5569514036760665e-05, 0.0001039195922203362, 0.01729375682771206, 3.058810034417547e-05, 0.0001434082951163873, 4.164769507042365e-06, 0.002002349589020014, 0.0006211189902387559, 3.3122971217380837e-05, 5.263932962407125e-06, 3.279403244960122e-06, 2.7038651751354337e-05, 5.629966835840605e-06, 6.902228506078245e-06, 6.332062639557989e-06, 2.6219061055599013e-06, 3.357389687153045e-06, 5.847815373272169e-06, 7.902470315457322e-06, 1.7883852478917106e-06, 9.413359657628462e-05, 0.7945629358291626, 2.5172653295157943e-06, 1.2942879266120144e-06, 3.244060962970252e-06, 0.14944109320640564, 6.295539947132056e-07, 1.4804776355958893e-06, 7.116590859368443e-05, 0.0020225641783326864, 0.000584914640057832, 1.2441304306776146e-06, 1.1917086339963134e-05, 6.085347558837384e-05, 3.382082923053531e-06, 1.7874081095214933e-06, 0.00045818102080374956, 5.347067144612083e-06, 8.105957931547891e-06, 2.6957321097142994e-05, 1.973096459551016e-06, 6.619167834287509e-06, 1.7521161908007343e-06, 1.7840072814578889e-06, 0.0003325269208289683, 1.5030838767415844e-05, 3.6901913063047687e-06, 8.549133781343699e-06, 2.0958577806595713e-05, 6.140085588413058e-06, 8.693688869243488e-06, 0.04374676197767258, 1.5799878383404575e-05, 1.9638664525700733e-06, 3.847991320071742e-05, 0.00020645240147132427, 6.808973557781428e-05, 1.9352621620782884e-06, 2.62065123024513e-06, 1.4616857697546948e-06, 2.0189377210044768e-06, 1.896443222904054e-06, 4.440890552359633e-05, 0.0016741076251491904, 7.98795372247696e-06, 9.252730524167418e-05, 2.544704557294608e-06, 0.0014823251403868198, 2.443240191496443e-06, 1.5074493603606243e-05, 3.1496478186454624e-05, 3.1496478186454624e-05, 1.7274645642828546e-06, 7.579965767945396e-06, 1.059295391314663e-05, 6.095541266404325e-06, 2.0273289464967092e-06, 3.6187341265758732e-06, 4.113674094696762e-06, 3.9794329495634884e-05, 5.166991286387201e-06, 1.4570355233445298e-05, 2.5908780116878916e-06, 1.501221049693413e-06, 0.0002860391396097839, 2.541308958825539e-06, 4.161248398304451e-06, 0.009772906079888344, 0.0034846197813749313, 0.013127812184393406, 1.612880987522658e-05, 1.7824903579821694e-06, 1.4974947362134117e-06, 6.337679224088788e-05, 0.0002966862521134317, 0.014307444915175438, 0.0004303256864659488, 2.3498271275457228e-06, 0.0004234836669638753, 3.3220176192116924e-06, 5.383434472605586e-05, 3.7158190480113262e-06, 4.1057496673602145e-06, 2.6622631139616715e-06, 1.7452439351473004e-05, 1.6556508853682317e-05, 1.2742352737404872e-06, 1.6073481674538925e-06, 1.0948641602226417e-06, 0.0018009997438639402, 9.437806625101075e-07, 1.9228721157560358e-06, 1.9922811134165386e-06, 0.0005791282746940851, 2.9912330319348257e-06, 3.68255095963832e-05, 7.291802830877714e-06, 4.450771484698635e-06, 3.6288565752329305e-06, 2.0714094262075378e-06, 2.6435211566422367e-06, 2.7704769308911636e-05, 2.8952847515029134e-06, 2.651483373483643e-05, 0.0030139018781483173, 1.0581788046692964e-05, 1.9202223029424204e-06, 1.5425318906636676e-06, 5.860315923200687e-06, 1.0571680832072161e-05, 0.026296231895685196, 2.853859996321262e-06, 0.0002896127989515662, 4.5698861868004315e-06, 5.5807922763051465e-06, 2.7535033950698562e-05, 2.016729922615923e-05, 1.4820267097093165e-05, 1.8500300939194858e-05, 2.1025587102485588e-06, 4.438786709215492e-05, 5.03332057633088e-06, 9.757550287758932e-06, 2.586326445452869e-05, 0.0005626557394862175, 0.0002763920638244599, 1.6626845535938628e-06, 2.0470267827477073e-06, 1.330163172497123e-06, 2.1467682017828338e-05, 8.632627213955857e-06, 2.0680085981439333e-06, 8.900437933334615e-06, 0.01251230388879776, 1.4685153928439831e-06, 3.967417342209956e-06, 0.0008848179131746292, 8.544137017452158e-06, 0.00012572894047480077, 1.2507025530794635e-05, 2.7565183700062335e-06, 3.854363512800774e-06, 1.0775037480925675e-05, 0.001387274358421564, 2.958031018351903e-06, 0.00010470351116964594, 3.495911732898094e-05, 2.1829919205629267e-06, 3.370454942341894e-05, 1.026289646688383e-05, 2.0705069800897036e-06, 0.0001630764309084043, 1.990516693695099e-06, 5.9173394220124464e-06, 0.0015861025312915444, 7.823322084732354e-05, 9.774455065780785e-06, 7.448614724125946e-06, 6.27177132628276e-06, 0.0022329024504870176, 2.1805699361721054e-05, 8.1441803558846e-06, 1.3718125728701125e-06, 1.934435431394377e-06, 6.115378710092045e-06, 0.00016569731815252453, 0.0026639276184141636, 7.27643691789126e-06, 1.386097301292466e-05, 9.06796230992768e-06, 0.007329238578677177, 0.0016534217866137624, 3.3338176308461698e-06, 2.2028491457604105e-06, 2.9081886623316677e-06, 3.0673174933326663e-06, 5.658894679072546e-06, 4.014646947325673e-06, 4.985827672498999e-06, 1.6917945686145686e-05, 6.916139682289213e-05, 1.667478500166908e-06, 3.5370237583265407e-06, 2.2577928575628903e-06, 9.096159919863567e-05, 6.440308061428368e-05, 5.274329851090442e-06, 6.803481028327951e-06, 4.611446001945296e-06, 1.1085732012361404e-06, 0.057573024183511734, 1.355093672827934e-06, 1.8355542579229223e-06, 2.3380646325676935e-06, 1.0878781040446484e-06, 2.3804279862815747e-06, 3.7929132759018103e-06, 9.965820026991423e-06, 2.0868549199803965e-06, 2.2992014692135854e-06, 0.000597797567024827, 7.865900261094794e-05, 1.69477959843789e-06, 1.1162356713612098e-05, 0.0001259881682926789, 7.386272045550868e-05, 1.628952190912969e-06, 3.1548731840302935e-06, 4.7781468310859054e-05, 2.564579290265101e-06, 0.00030820787651464343, 1.7994223526329733e-05, 2.7926740585826337e-05, 0.003916527610272169, 3.1537902032141574e-06, 0.00021702544472645968, 0.00020016344205942005, 3.101749143752386e-06, 3.4621938539203256e-05, 3.5850368931278354e-06, 2.4140620098478394e-06, 1.1751957345040864e-06, 9.902053079713369e-07, 7.804716915416066e-06, 1.384911570312397e-06, 3.6283825011196313e-06, 1.3356966519495472e-06, 1.2279541579118813e-06, 1.3298460999067174e-06, 1.3288799891597591e-06, 0.00022736591927241534, 1.2103226936233114e-06, 2.423576734145172e-05, 3.748922244994901e-05, 1.1624213584582321e-05, 9.678155947767664e-06, 2.5949086648324737e-06, 2.932135657829349e-06, 4.311737939133309e-05, 7.595958322781371e-06, 3.912331521860324e-06, 0.0688207671046257, 0.0030635769944638014, 2.365999762332649e-06, 1.9663417333504185e-05, 3.7759759834443685e-06, 5.2204504754627123e-05, 5.06067317473935e-06, 0.0002055810036836192, 2.255204208267969e-06, 3.666956672532251e-06, 1.6133465123857604e-06, 1.3353603662835667e-06, 2.0332654457888566e-06, 1.172644374491938e-06, 1.373061195408809e-06, 1.7268206420339993e-06, 1.1607656915657572e-06, 1.8524236793382443e-06, 2.004715270231827e-06, 0.12152262032032013, 1.5669204458390595e-06, 1.6016620065784082e-06, 1.1989140148216393e-05, 5.865773346158676e-05, 3.3231774523301283e-06, 6.475317059084773e-05, 9.023667371366173e-05, 0.0002497042005416006, 7.595022907480597e-05, 3.2337444281438366e-05, 1.0278147783537861e-05, 0.0002661625330802053, 2.7990754460915923e-06, 3.1562453841615934e-06, 1.719766828500724e-06, 1.3570206647273153e-06, 1.300325607189734e-06, 4.051968971907627e-06, 3.1787647003511665e-06, 5.005369985155994e-06, 3.7184281609370373e-06, 7.100595212250482e-06, 0.00015673362941015512, 2.1310983356670476e-05, 8.819528011372313e-05, 2.1132450456207152e-06, 1.6541281411264208e-06, 0.96342933177948, 2.748548922681948e-06, 2.4867927095328923e-06, 5.3576177379e-06, 4.720860943052685e-06, 2.924065302067902e-05, 2.04483762900054e-06, 7.873373760958202e-06, 2.640059619807289e-06, 4.236596396367531e-06, 4.472014552447945e-06, 0.0002503306604921818, 6.385846518242033e-06, 2.0202360246912576e-06, 7.480897329514846e-05, 2.8504518923000433e-06, 2.8798663151974324e-06, 1.4974503983467002e-06, 1.3491877552951337e-06, 1.565809043313493e-06, 1.4994610637586447e-06, 0.014421587809920311, 0.0011499680113047361, 1.3509014706869493e-06, 5.5781365517759696e-05, 2.6687478111853125e-06, 4.383879513625288e-06, 6.120011221355526e-06, 1.5444013570231618e-06, 2.117476651619654e-05, 2.267525087518152e-05, 1.3031675734964665e-05, 1.9021144908037968e-06, 0.00015898465062491596, 2.017124870690168e-06, 4.01040279029985e-06, 0.00015279791841749102, 1.0811018000822514e-06, 4.104942945559742e-06, 1.2745093044941314e-05, 3.7862382669118233e-06, 2.6669498765841126e-05, 1.7367792679578997e-06, 4.0998179429152515e-06, 2.244110191895743e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 3/3 [00:44<00:00, 14.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.017968302592635155, 0.9974547028541565, 0.01277640275657177, 0.0001642050192458555, 0.9243150353431702, 0.5052817463874817, 0.9671682119369507, 0.00034803044400177896, 5.477433660416864e-05, 5.4697349696652964e-05, 0.694668173789978, 2.2093896404840052e-05, 6.178530838951701e-06, 9.633455920265988e-05, 2.332322401343845e-05, 1.3590860362455714e-05, 0.000193543586647138, 0.7538589835166931, 0.6507065892219543, 0.8190565705299377, 0.5761234164237976, 0.4250348210334778, 5.779129787697457e-05, 1.1080785043304786e-05, 0.0002873626654036343, 0.0001381249603582546, 0.3830854892730713, 0.0001821334008127451, 0.0031503415666520596, 0.000164004901307635, 0.9741204380989075, 3.930743332603015e-06, 0.016553279012441635, 2.3085483462637058e-06, 7.739575812593102e-05, 0.9647181034088135, 0.9194992184638977, 0.9082461595535278, 0.0010069620329886675, 4.751959568238817e-05, 0.0005041667609475553, 0.0006544327479787171, 0.03802913427352905, 9.870432222669479e-06, 5.651775609294418e-06, 0.000620631966739893, 0.00016398659499827772, 6.300162203842774e-05, 4.402104241307825e-05, 0.004802128300070763, 2.3981096092029475e-05, 0.002425375860184431, 0.00027536728885024786, 0.0016930520068854094, 3.5936449421569705e-05, 0.00012866621545981616, 9.486476301390212e-06, 6.64593608235009e-06, 2.075787506328197e-06, 2.2272484784480184e-05, 0.0005226787179708481, 0.00113032222725451, 3.960344838560559e-05, 0.37404635548591614, 9.216766920872033e-05, 0.3283844590187073, 5.108500045025721e-05, 0.03956097736954689, 0.9158101677894592, 0.7109760642051697, 1.2029845493088942e-05, 0.0001458773185731843, 0.011149733327329159, 0.00011343019286869094, 8.542466275685001e-06, 0.0012597853783518076, 0.039265226572752, 0.46821701526641846, 0.00016113642777781934, 2.528377626731526e-05, 7.338208524743095e-05, 0.00011633914982667193, 0.9677737951278687, 0.0006026837509125471, 1.3102181583235506e-05, 3.545822619344108e-05, 0.030851155519485474, 0.000846219714730978, 6.049293460819172e-06, 1.751022136886604e-05, 3.1580818813381484e-06, 8.056455953919794e-06, 0.9229889512062073, 2.5514049411867745e-05, 2.7110485461889766e-05, 1.7371638023178093e-05, 0.0011132481740787625, 0.9159615635871887, 0.0001748488430166617, 0.004683284554630518, 3.4469507227186114e-05, 0.7370898723602295, 4.969866949977586e-06, 3.4522068744990975e-05, 1.6080349496405688e-06, 0.0767781138420105, 2.9996801913512172e-06, 0.00021715772163588554, 0.00017094204667955637, 0.9190670251846313, 4.712297140940791e-06, 0.00944916158914566, 0.0008726681116968393, 1.1480936336738523e-05, 3.3628235541982576e-05, 6.121026672190055e-06, 0.08908058702945709, 2.1114906303409953e-06, 0.8009399175643921, 0.06718739122152328, 0.0014322529314085841, 0.0009475337574258447, 0.3935757279396057, 0.4838421642780304, 3.0225233786040917e-05, 0.037195682525634766, 0.0013147485442459583, 9.815761586651206e-05, 0.0003426048788242042, 0.00012217166658956558, 2.811472768371459e-06, 1.8901453131547896e-06, 0.00280794408172369, 5.768992650700966e-06, 1.5939493096084334e-05, 1.6378360214730492e-06, 8.836449705995619e-05, 0.47484126687049866, 0.14838697016239166, 0.025801897048950195, 3.4890554161393084e-06, 0.0011531250784173608, 0.0002021992113441229, 0.4073335528373718, 0.009098824113607407, 0.00022347239428199828, 0.0005315474118106067, 9.520514868199825e-05, 0.0007762552704662085, 0.007180158048868179, 0.0018822307465597987, 0.9167612195014954, 0.001266011269763112, 0.008209954015910625, 9.471769772062544e-06, 4.524108135228744e-06, 3.528851084411144e-05, 6.7264695644553285e-06, 0.8936030864715576, 0.008369306102395058, 0.0005092016072012484, 0.0004456561873666942, 0.002099924487993121, 4.480740244616754e-06, 0.19538486003875732, 1.7194375686813146e-05, 4.131486275582574e-05, 0.0636940449476242, 5.990415047563147e-06, 5.985036295896862e-06, 3.958864454034483e-06, 2.1770272269350244e-06, 2.6338597308495082e-05, 0.39860033988952637, 2.3722812329651788e-05, 0.00021106087660882622, 7.38191301934421e-05, 0.0026584728620946407, 0.9077037572860718, 0.02478383295238018, 0.064280666410923, 0.15161219239234924, 0.0001945782860275358, 7.436761279677739e-06, 2.0606594262062572e-05, 2.422483521513641e-05, 0.008054181933403015, 0.012997297570109367, 0.15758444368839264, 0.000163001794135198, 3.986612136941403e-05, 0.35762354731559753, 0.2852528989315033, 2.0729406969621778e-05, 3.385322997928597e-06, 1.0421362276247237e-05, 1.4537686183757614e-06, 0.0010258224792778492, 8.803971468296368e-06, 5.393750143412035e-06, 0.00010623246635077521, 0.00012737285578623414, 8.245597200584598e-06, 0.00021467475744429976, 8.9750192273641e-06, 1.4972739336371887e-05, 0.00036434305366128683, 0.0029036838095635176, 3.3182466268044664e-06, 2.0371840037114453e-06, 4.408136192068923e-06, 8.249338861787692e-05, 0.0005321105709299445, 2.6083611373906024e-05, 0.00015244151290971786, 0.12764114141464233, 2.056919038295746e-05, 2.1857019874005346e-06, 0.8360896706581116, 1.097425410989672e-05, 8.580090252507944e-06, 0.011578802950680256, 1.3440915608953219e-05, 0.002943422645330429, 5.518115813174518e-06, 0.32838624715805054, 0.010375860147178173, 5.8344936405774206e-05, 0.5147144198417664, 0.0002355094184167683, 0.00014346367970574647, 0.0030031120404601097, 9.999702342611272e-06, 2.4108290745061822e-05, 0.0018843694124370813, 0.0015460985014215112, 4.448155596037395e-05, 0.4367316961288452, 0.005126676522195339, 0.00017837126506492496, 6.492616194009315e-06, 1.4213670510798693e-05, 4.0947645175037906e-05, 0.0024217444006353617, 0.00012384841102175415, 4.480526513361838e-06, 0.0672667995095253, 2.648652662173845e-05, 0.04229428991675377, 0.00024068920174613595, 0.03433678671717644, 1.1918950804101769e-05, 5.389182842918672e-05, 0.0005011913017369807, 0.0001476848847232759, 0.009216390550136566, 1.2793012501788326e-05, 0.0001856079907156527, 0.17284449934959412, 6.716541247442365e-05, 3.420122766328859e-06, 0.8504210114479065, 2.159554605896119e-05, 0.004201148636639118, 0.8722510933876038, 0.00385222933255136, 8.948285540100187e-06, 0.060968946665525436, 0.8648847341537476, 0.8605524897575378, 2.0250261513865553e-05, 1.780081447577686e-06, 1.2004335076198913e-05, 2.4772964025032707e-05, 0.001151352422311902, 2.6790651190822246e-06, 1.5061101294122636e-05, 9.679615322966129e-05, 0.06922630220651627, 0.5642687678337097, 0.9853119254112244, 5.105137915961677e-06, 4.951605660608038e-06, 1.9646345208457205e-06, 1.4218579053704161e-05, 0.0002404149272479117, 0.0006002560257911682, 0.00015687986160628498, 0.4659578800201416, 0.00014589888451155275, 8.382877240364905e-06, 0.00010734996612882242, 3.900994670402724e-06, 2.6762898414744996e-05, 0.9001049995422363, 0.00027974360273219645, 0.8814635276794434, 0.48704254627227783, 3.4878263249993324e-05, 1.6746400888223434e-06, 4.73541695100721e-06, 2.6769457690534182e-05, 0.020682118833065033, 1.4510371784126619e-06, 3.1260226478480035e-06, 1.006883212539833e-05, 1.4302079307526583e-06, 2.906649797296268e-06, 0.006467469036579132, 9.457283340452705e-06, 7.962751988088712e-05, 7.382780495390762e-06, 9.908760148391593e-06, 7.397192803182406e-06, 0.00010225392179563642, 3.3035894375643693e-06, 0.3162204623222351, 0.0003329897590447217, 0.002830154960975051, 4.9384110752725974e-05, 0.0004644773143809289, 0.003712674370035529, 0.1549871265888214, 5.0648530304897577e-05, 0.00011935409565921873, 0.0011004111729562283, 0.4671263098716736, 0.0014393054880201817, 0.0012699244543910027, 0.015844333916902542, 2.6925819838652387e-05, 4.143386468058452e-05, 0.00011873462790390477, 2.805647227432928e-06, 1.5903220628388226e-05, 9.529543604003266e-05, 1.6119614883791655e-05, 1.3865059372619726e-05, 1.6395661077694967e-05, 1.1207403986190911e-05, 0.016438651829957962, 0.37037035822868347, 7.148782606236637e-05, 3.6028561680723215e-06, 1.4870121276544523e-06, 9.203996160067618e-05, 7.615075446665287e-05, 4.1750258787942585e-06, 0.004220230504870415, 2.2250766051001847e-05, 3.688586366479285e-05, 2.77750823443057e-05, 4.7988182814151514e-06, 4.7925618673616555e-06, 0.000533947313670069, 0.00029852287843823433, 1.0078909326693974e-05, 1.963607792276889e-05, 1.8129801901523024e-05, 7.949177961563691e-05, 0.002734946319833398, 2.259072607557755e-05, 0.000336374097969383, 0.009088093414902687, 5.7285400544060394e-05, 0.00032437831396237016, 0.06826184689998627, 9.448511264054105e-06, 0.000588775728829205, 0.00023867319396231323, 2.5819858819886576e-06, 1.2185642844997346e-05, 0.011629452928900719, 0.00027222753851674497, 6.838940862508025e-06, 0.04383240267634392, 4.051861833431758e-05, 4.3993754843540955e-06, 0.0007862459751777351, 0.0014634979888796806, 6.3825768847891595e-06, 0.35310059785842896, 8.35909231682308e-05, 0.00013441898045130074, 0.0002567370538599789, 0.17247183620929718, 0.0011592077789828181, 4.4162912672618404e-05, 0.00019929140398744494, 0.0013960293726995587, 0.06502047926187515, 0.00212717242538929, 8.991785762191284e-06, 1.4857100723020267e-05, 2.568107083789073e-05, 1.469676226406591e-05, 0.00013193525956012309, 0.0004415838629938662, 7.025672857707832e-06, 0.0001851551205618307, 2.0762387066497467e-05, 0.00030417076777666807, 2.6057628929265775e-05, 0.015394991263747215, 0.383633017539978, 1.197368237626506e-05, 4.8349287681048736e-05, 0.0008076152298599482, 9.661483090894762e-06, 7.114714662748156e-06, 0.4640991985797882, 0.0763411819934845, 3.0233652069000527e-05, 3.5567204577091616e-06, 0.00010528862185310572, 2.724115074670408e-05, 1.2630460332729854e-05, 0.00026904503465630114, 4.314432771934662e-06, 2.993296948261559e-05, 0.00036707986146211624, 1.7783486327971332e-05, 3.5406694223638624e-05, 7.539674697909504e-05, 0.00028841610765084624, 1.3255651538202073e-05, 0.00020790251437574625, 0.00015264282410498708, 0.3716847002506256, 2.4696373657207005e-05, 1.3141901945346035e-05, 1.209046968142502e-05, 1.8681494111660868e-05, 2.125450009771157e-05, 2.1302937966538593e-05, 5.599504220299423e-06, 0.03707212582230568, 2.0866978047706652e-06, 1.0654331163095776e-05, 0.0001809814857551828, 3.172957440256141e-05, 2.987204652526998e-06, 3.4702632092376007e-06, 7.023408670647768e-06, 2.5441087927902117e-05, 0.001367565942928195, 0.0007088619749993086, 0.00048184674233198166, 0.0014274095883592963, 7.167452713474631e-05, 0.3570972979068756, 0.545608639717102, 6.774770736228675e-05, 9.772131761565106e-07, 4.5504602894652635e-06, 2.0655802472901996e-06, 3.0273413358372636e-06, 0.0003154925361741334, 1.8300363080925308e-05, 8.327793693752028e-06, 8.492876986565534e-06, 8.162343874573708e-06, 4.18941599491518e-05, 2.8101618227083236e-05, 0.00037606724072247744, 1.9241560949012637e-06, 0.05284648388624191, 0.8203374743461609, 0.049436941742897034, 1.2033587154292036e-05, 1.5593190255458467e-05, 1.6776175471022725e-05, 3.3720240026013926e-05, 0.024204585701227188, 7.732168887741864e-05, 0.00020185114408377558, 6.463657337008044e-05, 0.005316606722772121, 3.223358589821146e-06, 0.0005202030879445374, 0.11764618009328842, 1.7049542293534614e-05, 5.941023482591845e-05, 0.0008664110791869462, 0.14354626834392548, 2.291109376528766e-05, 0.21133047342300415, 0.00023459371004719287, 2.0367748220451176e-05, 3.3913867810042575e-05, 3.0403841719817137e-06, 1.1193047612323426e-05, 0.0001115266713895835, 1.3355642067836015e-06, 1.5539777450612746e-05, 0.0009091902174986899, 0.17859965562820435, 4.454055306268856e-05, 3.044393906748155e-06, 0.00013541874068323523, 0.0018724503461271524, 0.00016085748211480677, 0.00012055441766278818, 0.001336537068709731, 0.02615950256586075, 0.11622390896081924, 0.014813223853707314, 2.5675168217276223e-05, 0.000125216378364712, 1.4637078493251465e-05, 0.0002117631520377472, 0.0095681082457304, 0.7209465503692627, 0.005023251753300428, 9.223329470842145e-06, 1.402180532750208e-05, 8.650161908008158e-05, 3.913331511284923e-06, 0.1232432946562767, 0.0003566681989468634, 1.928120582306292e-05, 1.9482915831758874e-06, 0.00047398803872056305, 4.151641405769624e-05, 1.4222118807083461e-05, 3.7208828871371225e-06, 0.006676585413515568, 0.001331079751253128, 9.134245374298189e-06, 3.881655720761046e-05, 2.6874411560129374e-05, 0.005675111897289753, 4.2113948438782245e-05, 3.7946970223856624e-06, 5.604179023066536e-06, 1.6104142559925094e-05, 8.413295290665701e-06, 0.00023134934599511325, 5.977781256660819e-05, 4.185778016108088e-05, 1.7858450519270264e-05, 0.0018148731905966997, 1.1961903510382399e-05, 7.2719617492111865e-06, 4.8413923650514334e-05, 2.4718781787669286e-05, 2.9317188818822615e-06, 5.465027243189979e-06, 3.901016498275567e-06, 2.189072984037921e-05, 0.004373004660010338, 3.0042268917895854e-05, 3.366767487023026e-05, 0.0002519992704037577, 0.08977284282445908, 1.2814117553716642e-06, 3.465900226728991e-05, 2.553947979322402e-06, 3.686120908241719e-05, 4.211006398691097e-06, 0.00011032906331820413, 9.799328836379573e-05, 2.8702874260488898e-05, 1.5788898963364772e-05, 3.530011326802196e-06, 9.628195584809873e-06, 0.006536947563290596, 0.5603564977645874, 0.0006446015904657543, 3.7212271308817435e-06, 0.5214142799377441, 0.0008480472606606781, 6.771166226826608e-05, 4.196601366857067e-06, 9.179349035548512e-06, 2.260110932184034e-06, 9.822944775805809e-06, 0.00037493574200198054, 1.6923415387282148e-05, 2.5721044949023053e-05, 1.1236631507927086e-05, 5.5846776376711205e-05, 0.0002427576546324417, 0.00036814104532822967, 0.08245109021663666, 0.0032881954684853554, 6.530564860440791e-06, 4.822196206077933e-05, 0.016662994399666786, 0.21087457239627838, 6.070537892810535e-06, 2.6027951207652222e-06, 9.153876453638077e-05, 3.119182292721234e-05, 6.951157047296874e-06, 0.0004285138857085258, 0.34935736656188965, 1.0512325388845056e-05, 1.2859786693297792e-05, 2.4053402739809826e-05, 2.024979266934679e-06, 2.2160047592478804e-05, 0.0006426863837987185, 1.0306799595127814e-05, 4.134085884288652e-06, 5.602265900961356e-06, 0.000619638420175761, 2.9164152692828793e-06, 6.440074503188953e-05, 3.5159243907401105e-06, 0.00021710760483983904, 0.0030445349402725697, 0.00010901005589403212, 1.6904446965781972e-05, 7.727194315521047e-05, 2.434977886878187e-06, 0.00020666488853748888, 3.4879725717473775e-05, 3.4890822462330107e-06, 3.1753195344208507e-06, 0.8629140853881836, 1.8182767234975472e-05, 1.2199934644741006e-05, 1.1458775588835124e-05, 0.0011348288971930742, 6.494894932984607e-06, 1.3777556659988477e-06, 0.8674745559692383, 0.003380388719961047, 0.00014230805390980095, 1.7359647017656243e-06, 8.310259545396548e-06, 2.290759584866464e-05, 0.001071393839083612, 3.6961964724469e-06, 0.0037976724561303854, 0.04347823187708855, 3.494838529150002e-05, 0.001054027583450079, 1.971576966752764e-05, 8.873427759681363e-06, 9.661033982411027e-05, 1.6594149201409891e-06, 0.0024773674085736275, 0.00013906312233302742, 0.05127934738993645, 0.00012789448373951018, 4.984907354810275e-05, 0.0007520620711147785, 0.0036659683100879192, 0.0009570513502694666, 6.538603429362411e-06, 0.00587963592261076, 0.0073221540078520775, 0.00674057612195611, 1.4568785445590038e-05, 4.4420048652682453e-05, 0.000217489039641805, 3.1916167699819198e-06, 8.451659959973767e-05, 0.00010348005162086338, 2.843452421075199e-06, 5.179922482057009e-06, 0.00030111544765532017, 2.805446638376452e-05, 3.8564589885936584e-06, 1.8694182699618977e-06, 4.193833319732221e-06, 3.4751712973957183e-06, 2.6346140202804236e-06, 6.373824362526648e-06, 2.3719142063782783e-06, 1.3730874570683227e-06, 0.0012136913137510419, 0.0001940848887898028, 3.162091888953e-05, 4.797230303665856e-06, 3.873579407809302e-05, 0.12075083702802658, 0.009043743833899498, 4.4289176003076136e-05, 0.0004969137953594327, 3.9443864807253703e-05, 5.820490969199454e-06, 1.8030934370472096e-05, 0.00034732508356682956, 4.714958413387649e-05, 0.00037788462941534817, 0.4353938400745392, 5.179372601560317e-05, 1.3426160876406357e-06, 0.0003820064594037831, 1.3979555433252244e-06, 5.115222666063346e-05, 1.1398339665902313e-05, 0.020892096683382988, 0.00018383399583399296, 0.05928167700767517, 1.4451565220952034e-05, 0.00042577722342684865, 0.008426975458860397, 0.0006405363674275577, 0.6959490776062012, 0.0005968963378109038, 8.637484279461205e-05, 3.134342114208266e-05, 0.07152080535888672, 1.609893843124155e-05, 5.8346572586742695e-06, 4.620730578608345e-06, 0.0004347104695625603, 2.5924871351890033e-06, 4.052792064612731e-06, 0.00012066505587426946, 0.000437144743045792, 0.0003856405965052545, 5.7895551435649395e-05, 5.00759961141739e-06, 5.593334662989946e-06, 1.1427813660702668e-05, 0.0002212313120253384, 3.5186376408091746e-06, 2.535305611672811e-06, 1.6458716345368885e-05, 0.4639202952384949, 2.3702862108621048e-06, 2.2408914901461685e-06, 7.2965744948305655e-06, 6.001542351441458e-05, 0.0007934028399176896, 3.919015853171004e-06, 3.205275788786821e-06, 2.9385123525571544e-06, 1.7391478195349919e-06, 3.2087596082419623e-06, 0.25779443979263306, 1.1199186928934068e-06, 1.7968706742976792e-05, 9.81142875389196e-06, 1.1458775588835124e-05, 1.1458775588835124e-05, 1.2185642844997346e-05, 1.2185642844997346e-05, 1.1458775588835124e-05, 1.2185642844997346e-05, 1.1458775588835124e-05, 1.1458775588835124e-05, 1.1458775588835124e-05, 1.1458775588835124e-05, 1.1458775588835124e-05, 1.1458775588835124e-05, 0.698846697807312, 1.549077296658652e-06, 0.02743629179894924, 3.3116928079834906e-06, 3.4381780551484553e-06, 1.5803329006303102e-05, 0.029787663370370865, 0.002710957545787096, 5.012856490793638e-05, 0.33350870013237, 0.002236780943349004, 1.740742504807713e-06, 8.39961830934044e-06, 0.2205895632505417, 0.00042638403829187155, 7.989608093339484e-06, 4.9880654842127115e-05, 0.0006265389383770525, 3.1950460197549546e-06, 4.1378366404387634e-06, 6.850212230347097e-05, 0.0004158757219556719, 9.641188626119401e-06, 2.011668220802676e-05, 0.0007326796185225248, 0.009332536719739437, 1.5545218730039778e-06, 2.3111952032195404e-05, 2.1980392830300843e-06, 0.0002170380757888779, 5.154397058504401e-06, 1.2703107131528668e-05, 1.2703107131528668e-05, 9.543762644170783e-06, 1.8298149370821193e-05, 1.1614750292210374e-05, 8.307058124046307e-06, 1.088753379008267e-05, 1.0412310984975193e-05, 1.7965503502637148e-05, 0.0003059436858166009, 2.872905315598473e-05, 3.0928081287129316e-06, 4.580192126013571e-06, 0.005423032213002443, 8.18037642602576e-06, 1.7458414731663652e-06, 3.74170376744587e-05, 1.0663195098459255e-05, 2.8902342819492333e-05, 7.657009518879931e-06, 3.446953542152187e-06, 3.1625481824448798e-06, 3.42215207638219e-06, 9.047240018844604e-05, 8.603419701103121e-05, 0.00014379568165168166, 1.4929138160368893e-05, 6.259555084398016e-05, 1.1933168025279883e-05, 8.614511170890182e-05, 0.001178803388029337, 0.41976112127304077, 0.8885744214057922, 1.3915461750002578e-05, 0.021310003474354744, 8.655862870909914e-07, 3.965079667977989e-05, 6.732091424055398e-05, 0.00014478339289780706, 0.00012054856051690876, 5.563118611462414e-05, 4.935614924761467e-06, 0.016773775219917297, 0.0016759894788265228, 0.0001577998191351071, 2.3483362383558415e-05, 4.5500324631575495e-05, 0.0007862782222218812, 0.00010460599878570065, 8.260614777100272e-06, 5.662360308633652e-06, 2.9490681754396064e-06, 8.155177965818439e-06, 2.8016367650707252e-05, 1.9626098946901038e-05, 7.636494956386741e-06, 9.652212611399591e-05, 0.001670041703619063, 0.00020415097242221236, 0.2556244432926178, 1.6136425983859226e-05, 4.853536302107386e-05, 0.0016548518324270844, 5.9389640227891505e-06, 7.203206041594967e-05, 2.8813140033889795e-06, 1.2093802070012316e-05, 0.0018139973981305957, 3.800412059717928e-06, 2.1389264475146774e-06, 0.0007340473239310086, 5.338491519069066e-06, 2.203216172347311e-05, 7.925781392259523e-05, 3.085742946495884e-06, 2.5280817226303043e-06, 0.3771716356277466, 0.5101633071899414, 7.935048415674828e-06, 7.82178631197894e-06, 1.6031877748901024e-05, 0.00026309152599424124, 5.053010227129562e-06, 0.0012400998966768384, 0.0036000008694827557, 0.00012491943198256195, 3.745951971723116e-06, 0.0004266015312168747, 2.1221353563305456e-06, 2.181722447858192e-05, 8.025277566048317e-06, 9.758444321050774e-06, 0.029979191720485687, 7.18987485015532e-06, 5.869589585927315e-06, 0.00022921455092728138, 3.739452949957922e-05, 3.175067467964254e-05, 1.4213264876161702e-05, 2.113172013196163e-05, 0.00014412114978767931, 2.474228949722601e-06, 0.00045506880269385874, 3.957267836085521e-06, 0.0001256488758372143, 7.860611731302924e-06, 1.7799622582970187e-05, 0.6095618009567261, 1.60480758495396e-05, 8.366782822122332e-06, 9.908760148391593e-06, 7.130725862225518e-06, 6.780176136089722e-06, 7.635774636582937e-06, 9.908760148391593e-06, 9.908760148391593e-06, 8.31822762847878e-06, 9.908760148391593e-06, 9.908760148391593e-06, 9.908760148391593e-06, 6.463105364673538e-06, 7.862088750698604e-06, 9.908760148391593e-06, 9.908760148391593e-06, 9.908760148391593e-06, 8.29948749014875e-06, 0.008053279481828213, 2.243316203021095e-06, 5.949919068370946e-05, 0.8834535479545593, 1.4576469766325317e-05, 1.2034573046548758e-05, 2.8913389087392716e-06, 3.2904554245760664e-06, 0.0002368711429880932, 2.9855911634513177e-05, 5.66213884667377e-06, 3.974908850068459e-06, 3.240814112359658e-05, 4.592215191223659e-06, 0.811453104019165, 2.9058737709419802e-05, 2.0835143459407846e-06, 3.0332857932080515e-05, 3.0665569283883087e-06, 2.0539770048344508e-05, 1.197303117805859e-05, 4.7042285586940125e-06, 9.264444997825194e-06, 1.9220969988964498e-05, 3.840761928586289e-05, 3.101010588579811e-05, 1.08392250695033e-05, 5.078750746179139e-06, 0.004215972498059273, 0.002261445624753833, 5.097339908388676e-06, 0.0004550826852209866, 3.4134493489546003e-06, 5.955031610938022e-06, 2.4917376322264317e-06, 3.1099541956791654e-05, 3.469611328910105e-06, 4.338055441621691e-05, 0.002154043409973383, 8.081334271992091e-06, 0.0001348294026684016, 0.610251784324646, 0.001456187223084271, 0.0003769681788980961, 2.7768302970798686e-05, 0.08945351094007492, 0.0005019513773731887, 9.265451808460057e-06, 1.2164812687842641e-05, 1.5277773854904808e-06, 3.1021425002109027e-06, 1.7117507695729728e-06, 0.00029733526753261685, 0.002263346454128623, 1.844329631239816e-06, 0.16256554424762726, 2.5745432594703743e-06, 1.448923740099417e-05, 3.7556026654783636e-05, 0.00038223847514018416, 4.15393242292339e-06, 2.9571031063824194e-06, 0.3762885332107544, 2.3874349608377088e-06, 2.766294710454531e-05, 0.15820546448230743, 8.07395699666813e-05, 1.7970094631891698e-05, 0.00019346813496667892, 0.00012512794637586921, 1.6012103515095077e-05, 1.442940720153274e-05, 7.748437201371416e-05, 5.937695732427528e-06, 5.120936748426175e-06, 2.1529291188926436e-05, 3.5162095173291164e-06, 0.0006243661046028137, 0.7676481008529663, 7.917724360595457e-06, 0.003874588292092085, 2.6452307793078944e-06, 5.921031061006943e-06, 1.6911516240725177e-06, 3.1957713417796185e-06, 0.0014695700956508517, 0.0053372434340417385, 0.0016222054837271571, 0.0004589857708197087, 4.151205575908534e-05, 1.0659575309546199e-05, 5.4067490964371245e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    " from tqdm import tqdm\n",
    " import json\n",
    " for qid, hit in tqdm(hits.items(), total=len(hits), desc=\"Reranking\"):\n",
    "\n",
    "    reranking_query = query_dict[qid]\n",
    "    reranked_scores = rerank_t5_DP(\n",
    "        reranking_query,\n",
    "        [json.loads(searcher.doc(doc_object.docid).raw())[\"contents\"] for doc_object in hit],\n",
    "        tokenizer,\n",
    "        model,\n",
    "        decoder_stard_id,\n",
    "        targeted_ids,\n",
    "    )\n",
    "\n",
    "    print(reranked_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'multiprocessing.managers.ListProxy'>\n",
      "i =  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on device:  cuda:3\n",
      "Model on device:  cuda:1\n",
      "Model on device:  cuda:2\n",
      "Model on device:  cuda:0\n",
      "i =  1\n",
      "Model on device:  cuda:1\n",
      "Model on device:  cuda:3\n",
      "Model on device:  cuda:2\n",
      "Model on device:  cuda:0\n",
      "i =  2\n",
      "Model on device:  cuda:2\n",
      "Model on device:  cuda:1\n",
      "Model on device:  cuda:3\n",
      "Model on device:  cuda:0\n",
      "i =  3\n",
      "Model on device:  cuda:3\n",
      "Model on device:  cuda:2\n",
      "Model on device:  cuda:1\n",
      "Model on device:  cuda:0\n",
      "i =  4\n",
      "Model on device:  cuda:3\n",
      "Model on device:  cuda:2\n",
      "Model on device:  cuda:1\n",
      "Model on device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "sys.path.append('/data/rech/huiyuche/TREC_iKAT_2024/test/')\n",
    "import test_ddp\n",
    "importlib.reload(sys.modules['test_ddp'])\n",
    "from test_ddp import example, aha \n",
    "\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "# Run the aha function\n",
    "output = aha()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test monot5 ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "cache_dir = \"/data/rech/huiyuche/huggingface\"\n",
    "sys.path.append('/data/rech/huiyuche/TREC_iKAT_2024/src/')\n",
    "import importlib\n",
    "import rerank\n",
    "import llm\n",
    "importlib.reload(sys.modules['llm'])\n",
    "importlib.reload(sys.modules['rerank'])\n",
    "from rerank import (\n",
    "    load_t5_DDP,\n",
    "    rerank_t5_DDP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    PreTrainedTokenizer\n",
    "    )\n",
    "\n",
    "tokenizer, model, decoder_stard_id, targeted_ids = load_t5_DDP(\n",
    "    cache_dir = cache_dir,\n",
    "    model_name = \"castorini/monot5-base-msmarco\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rech/huiyuche/envs/trec_ikat/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Jul 08, 2024 8:53:59 A.M. org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "retrieval_query_list = [\n",
    "    \"what is the capital of France?\",\n",
    "    \"what is the capital of Germany?\",\n",
    "    \"what is the capital of Italy?\"\n",
    "]\n",
    "qid_list_string = [\n",
    "    \"id1\",\n",
    "    \"id2\",\n",
    "    \"id3\"\n",
    "]\n",
    "\n",
    "query_dict = {\n",
    "    qid: query for qid, query in zip(qid_list_string, retrieval_query_list)\n",
    "}\n",
    "\n",
    "searcher = LuceneSearcher(\"/part/01/Tmp/yuchen/indexes/clueweb22b_ikat23_fengran_sparse_index_2/\")\n",
    "#searcher = LuceneSearcher(\"/data/rech/huiyuche/TREC_iKAT_2024/data/indexes/clueweb22b_ikat23_fengran_sparse_index_2\")\n",
    "searcher.set_bm25(0.9, 0.4)\n",
    "hits = searcher.batch_search(retrieval_query_list, qid_list_string, k = 1000, threads = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clueweb22-en0017-32-04653:0', 'clueweb22-en0016-24-18543:0', 'clueweb22-en0021-14-14889:63', 'clueweb22-en0037-88-06710:10', 'clueweb22-en0001-30-00978:0']\n",
      "[9.22029972076416, 8.990500450134277, 8.907099723815918, 8.903499603271484, 8.807000160217285]\n"
     ]
    }
   ],
   "source": [
    "id1_first_5_ids_1000 = [doc_object.docid for doc_object in hits[\"id1\"][:5]]\n",
    "id1_first_5_scores_1000 = [doc_object.score for doc_object in hits[\"id1\"][:5]]\n",
    "print(id1_first_5_ids_1000)\n",
    "print(id1_first_5_scores_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hits = searcher.batch_search(retrieval_query_list, qid_list_string, k = 50, threads = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clueweb22-en0017-32-04653:0', 'clueweb22-en0016-24-18543:0', 'clueweb22-en0021-14-14889:63', 'clueweb22-en0037-88-06710:10', 'clueweb22-en0001-30-00978:0']\n",
      "[9.22029972076416, 8.990500450134277, 8.907099723815918, 8.903499603271484, 8.807000160217285]\n"
     ]
    }
   ],
   "source": [
    "id1_first_5_ids_50 = [doc_object.docid for doc_object in hits[\"id1\"][:5]]\n",
    "id1_first_5_scores_50 = [doc_object.score for doc_object in hits[\"id1\"][:5]]\n",
    "print(id1_first_5_ids_50)\n",
    "print(id1_first_5_scores_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world_size 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking:   0%|          | 0/3 [00:00<?, ?it/s]2024-07-04 02:27:27.199244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 02:27:29.468771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-04 02:27:38.162606: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 02:27:39.956140: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 02:27:47.561211: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 02:27:49.444943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 02:27:56.715687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 02:27:58.550896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking:  33%|███▎      | 1/3 [01:18<02:36, 78.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0.9456437230110168, 0.996783971786499, 0.9666732549667358, 6.670789844065439e-06, 0.08171119540929794, 0.7397224307060242, 0.008762986399233341, 0.07190988212823868, 2.7369012514100177e-06, 4.171929685981013e-05, 0.03734675794839859, 0.9959157109260559, 7.160047516663326e-06, 0.0015542611945420504, 0.941811740398407, 7.144080427679e-06, 0.0004525045515038073, 0.0003630525025073439, 2.427274921501521e-05, 2.3689328372711316e-05, 0.9712644219398499, 0.24110570549964905, 0.0002644528285600245, 0.5568408966064453, 0.00040787571924738586, 0.9873769879341125, 0.036740224808454514, 0.824500322341919, 0.00019249400065746158, 0.8373112678527832, 0.5780090093612671, 7.149000884965062e-05, 0.9149550199508667, 0.00017964081780519336, 0.9679471850395203, 0.9127671718597412, 0.0001653720100875944, 0.001106139156036079, 1.948814133356791e-05, 5.129407327331137e-06, 0.9169193506240845, 0.058274175971746445, 1.4319725778477732e-06, 0.001757692196406424, 8.745161721890327e-06, 0.0010489268461242318, 3.536660369718447e-05, 6.532165571115911e-06, 4.790314778801985e-05, 0.6138888001441956, 0.0001232078648172319, 7.505494522774825e-06, 0.0007147459546104074, 0.0006743752164766192, 0.8066490292549133, 0.9184074401855469, 1.0099402970809024e-05, 0.07541836053133011, 3.299741820228519e-06, 6.844962626928464e-05, 4.490940409596078e-05, 3.3092271678469842e-06, 0.0005152150988578796, 5.137798871146515e-05, 0.00012851980864070356, 0.0007394591229967773, 3.333245331305079e-05, 0.8545377254486084, 6.515273526019882e-06, 0.004646587651222944, 1.5542209439445287e-05, 0.0004629068716894835, 0.7783974409103394, 1.3692613265448017e-06, 0.029263760894536972, 1.4042814655113034e-05, 0.38074594736099243, 4.45879231847357e-06, 0.40141087770462036, 0.0001166803267551586, 0.13233457505702972, 2.5520487270114245e-06, 0.00010193284833803773, 0.18372267484664917, 4.016360981040634e-05, 0.028063634410500526, 0.000487298471853137, 0.0020295388530939817, 0.8122107982635498, 2.898536877182778e-05, 0.9729106426239014, 1.0033318176283501e-05, 9.043745194503572e-06, 0.9085103869438171, 0.9395525455474854, 4.03796593673178e-06, 5.3482199291465804e-05, 3.2069729059003294e-06, 0.0008841798990033567, 5.266805146675324e-06, 0.15301185846328735, 8.766193786868826e-05, 2.1212299543549307e-05, 2.837841975633637e-06, 5.339566632756032e-05, 0.0002940376289188862, 0.2977893352508545, 1.692535079200752e-05, 1.6059093468356878e-06, 0.9923264980316162, 0.7416220307350159, 0.26026079058647156, 0.2967442274093628, 5.008721927879378e-06, 6.76236450090073e-05, 0.005059921182692051, 1.1709256568792625e-06, 4.8072374738694634e-06, 0.004437941126525402, 3.1165152449830202e-06, 7.974453910719603e-05, 0.00010986670531565323, 0.9684047698974609, 0.4841724932193756, 0.9601609110832214, 1.0421510523883626e-05, 2.091876240228885e-06, 0.07707155495882034, 0.006078449543565512, 0.032865870743989944, 4.1550629248376936e-05, 0.5383612513542175, 6.015719009155873e-06, 0.721458911895752, 6.679033504042309e-06, 2.7326521376380697e-05, 0.0013705923920497298, 0.4435144364833832, 3.0266483008745126e-05, 8.175540642696433e-06, 0.00011914133210666478, 1.0504578312975354e-05, 0.6746375560760498, 1.3126495105098002e-05, 0.00027759288786910474, 0.47848284244537354, 1.196409448311897e-05, 0.5526809692382812, 0.8529613018035889, 5.996479376335628e-05, 1.3229951036919374e-05, 0.004513005260378122, 0.0032347331289201975, 0.00015375447401311249, 0.00043240346712991595, 0.06437303870916367, 9.995547588914633e-05, 0.011491340585052967, 1.5969771993695758e-05, 4.874337264482165e-06, 4.147606887272559e-06, 8.16540778032504e-05, 0.009450147859752178, 0.015080757439136505, 0.5862635374069214, 0.05247582867741585, 0.14313480257987976, 1.6141299283845e-06, 4.408838321978692e-06, 1.9265213268226944e-05, 0.09130251407623291, 4.31789430876961e-06, 1.0881347407121211e-05, 1.1303448445687536e-05, 3.6755274777533486e-05, 0.291583389043808, 1.9262324713054113e-06, 0.8124517202377319, 0.03668743744492531, 3.2527382245461922e-06, 0.00020055084314662963, 1.4555160305462778e-05, 4.047334641654743e-06, 0.8030486106872559, 3.9543324419355486e-06, 0.00014850830484647304, 3.826817555818707e-05, 0.010829358361661434, 2.5191477107000537e-05, 0.11921456456184387, 0.0013164111878722906, 1.3246362868812867e-05, 0.0010917738545686007, 2.9166017156967428e-06, 0.5270065069198608, 0.005460525397211313, 0.021102987229824066, 1.5199053450487554e-05, 5.1492033890099265e-06, 0.00016637427324894816, 7.620984433742706e-06, 2.4333519831998274e-05, 0.00023968708410393447, 0.0007808119989931583, 5.8167174756817985e-06, 0.09666285663843155, 0.1301565319299698, 1.8492519302526489e-06, 0.010992353782057762, 0.08451837301254272, 2.2283298676484264e-05, 0.4895678460597992, 5.2465165936155245e-06, 3.908676717401249e-06, 0.15237028896808624, 0.00012293549661990255, 0.00018359489331487566, 9.710734593681991e-05, 0.00010540554649196565, 0.0002229888050351292, 2.2815765987616032e-06, 6.005743853165768e-05, 0.909376859664917, 0.15693658590316772, 0.005169334821403027, 5.430633791547734e-06, 0.0012005195021629333, 5.6187433074228466e-05, 0.2601551115512848, 1.757633981469553e-05, 0.0005290666013024747, 2.333126758458093e-06, 2.5384774744452443e-06, 6.283139555307571e-06, 0.587867796421051, 0.005134460050612688, 0.005941334646195173, 0.01501127053052187, 0.4553495943546295, 3.7805195916007506e-06, 2.1685214051103685e-06, 0.12018986791372299, 3.076430220971815e-05, 4.3298032323946245e-06, 2.3100472390069626e-05, 0.15881647169589996, 0.01835809275507927, 1.813547351048328e-05, 7.3542992140573915e-06, 0.5110127329826355, 0.09724298119544983, 4.946985427523032e-06, 6.90214892529184e-06, 1.3186392607167363e-05, 1.9363722458365373e-05, 0.03200453892350197, 3.334011580591323e-06, 0.0022033657878637314, 4.387280569062568e-05, 6.856808340671705e-06, 0.0002356082113692537, 5.7705339713720605e-05, 4.994298251403961e-06, 0.24010354280471802, 8.895337487047072e-06, 0.8982253074645996, 8.805566722003277e-06, 0.0005469495663419366, 0.0017618670826777816, 0.17616966366767883, 0.00033133200486190617, 0.828922688961029, 5.8634518609324005e-06, 7.476740574929863e-05, 3.0191934001777554e-06, 5.039660663896939e-06, 7.31702966731973e-05, 0.9779384136199951, 1.5970015738275833e-05, 2.7851172035298077e-06, 0.0006101481849327683, 0.05410273000597954, 0.04313826933503151, 0.006977430544793606, 0.05341092497110367, 0.22890318930149078, 0.5842357873916626, 0.018774308264255524, 3.1867170946497936e-06, 2.2481959604192525e-05, 0.016880756244063377, 0.48787039518356323, 0.0007156122010201216, 3.0940973374526948e-06, 3.389752237126231e-06, 0.5627657175064087, 0.0010783466277644038, 9.337962183053605e-06, 1.9869794414262287e-06, 0.0021602921187877655, 0.10414577275514603, 4.0827594602887984e-06, 0.00594684761017561, 2.4024798221944366e-06, 0.375841349363327, 3.608214683481492e-05, 0.8935145139694214, 5.230845636106096e-05, 0.04970772936940193, 2.5347158043587115e-06, 2.267375930387061e-05, 0.009156656451523304, 0.002069327514618635, 0.06262591481208801, 7.327251660171896e-05, 3.125229795841733e-06, 9.338506060885265e-06, 0.23718178272247314, 0.008252843283116817, 6.76922281854786e-05, 6.022031084285118e-05, 0.00016363935719709843, 1.796546894183848e-05, 9.478043807575887e-07, 3.482221245576511e-06, 0.02669490873813629, 0.0004805687931366265, 0.28404685854911804, 2.5000110781547846e-06, 8.418761717621237e-05, 5.752002107328735e-05, 0.9298338890075684, 2.6688344405556563e-06, 0.00022664949938189238, 0.5369706749916077, 2.2528138288180344e-06, 0.003270318964496255, 0.0003364959848113358, 0.0012723676627501845, 1.3386079444899224e-06, 0.7729904055595398, 0.0003375698288436979, 0.001476437784731388, 0.2890383005142212, 6.25413986199419e-06, 1.8097646261594491e-06, 0.3698892593383789, 0.7522964477539062, 2.719435542530846e-06, 2.610150659165811e-06, 2.4520342776668258e-05, 0.0004282427253201604, 1.9406408682698384e-05, 0.001914694090373814, 0.0619187094271183, 0.002502232091501355, 0.00012521864846348763, 2.5379085855092853e-06, 0.20914480090141296, 0.569483757019043, 0.5223606824874878, 4.610575342667289e-05, 0.0008525601588189602, 8.99779024621239e-06, 0.0006526526412926614, 5.5276403145398945e-05, 1.0951649528578855e-05, 9.051286906469613e-05, 6.165916602185462e-06, 1.0329331416869536e-05, 0.7801688313484192, 0.00019908857939299196, 2.8019919682265026e-06, 3.891986580129014e-06, 3.579311169232824e-06, 0.5865647792816162, 0.01719002053141594, 0.053603652864694595, 0.0016832799883559346, 7.212966738734394e-05, 0.9153780937194824, 1.7158629361802014e-06, 0.49475976824760437, 3.1626470445189625e-05, 6.641317304456607e-05, 0.7976750135421753, 1.4704309023727546e-06, 1.6354354102077195e-06, 0.37461793422698975, 0.026122737675905228, 0.7781143188476562, 5.221384981268784e-06, 1.6275406551358174e-06, 4.011443252238678e-06, 1.0373478289693594e-05, 1.0264149750582874e-05, 0.001240491052158177, 3.7209608763077995e-06, 0.7331744432449341, 1.1219242423976539e-06, 0.012662440538406372, 1.2686926993410452e-06, 2.0663860595959704e-06, 1.655599021432863e-06, 2.2934682419872843e-06, 0.9016116857528687, 3.557914851626265e-06, 0.00021554743580054492, 0.0003232995804864913, 0.004192987456917763, 0.10360796004533768, 6.35740980214905e-06, 0.10134816914796829, 0.1209702268242836, 0.0003271670138929039, 7.049903069855645e-05, 3.3460455597378314e-06, 5.135550964041613e-05, 3.311026603114442e-06, 0.9148072600364685, 0.880746066570282, 1.9831135432468727e-05, 0.0006903605535626411, 5.521200364455581e-06, 4.64937829747214e-06, 0.36896610260009766, 0.0013989787548780441, 0.6409865617752075, 0.0010858956957235932, 0.00042801257222890854, 0.592424213886261, 5.526431323232828e-06, 0.9031885862350464, 0.0009054979309439659, 0.0006766871083527803, 0.00012697497732006013, 0.0002399765799054876, 4.5625201892107725e-05, 0.11902172863483429, 0.7863047122955322, 3.4707795748545323e-06, 1.1462830116215628e-05, 9.826832865655888e-06, 2.9136740522517357e-06, 0.0001838019088609144, 0.0011622548336163163, 0.056815795600414276, 0.002419607248157263, 0.0018204759107902646, 0.09900157153606415, 9.58826831265469e-07, 3.1548279366688803e-06, 1.6442132618976757e-05, 0.754509449005127, 0.23203276097774506, 0.002650247421115637, 4.958882345817983e-05, 0.007971989922225475, 1.693955482551246e-06, 8.157643605954945e-06, 0.01961340941488743, 2.756231879175175e-06, 1.670330311753787e-05, 5.272668931866065e-05, 0.011056304909288883, 0.4897778034210205, 0.0002030543255386874, 1.1908793567272369e-05, 4.846322099183453e-06, 0.7552120089530945, 6.283794209593907e-05, 0.000152567881741561, 0.004580153618007898, 2.7532264539331663e-06, 1.651121124268684e-06, 1.2322311704338063e-05, 2.850483542715665e-05, 3.722978362930007e-05, 3.066314320676611e-06, 5.006431456422433e-05, 7.649646431673318e-05, 2.669028617674485e-05, 2.0191228031762876e-06, 0.0007994164479896426, 8.372818228963297e-06, 8.39130825625034e-06, 0.0005771824507974088, 0.19110539555549622, 1.2321078429522458e-05, 2.762245640042238e-05, 3.29065983351029e-06, 6.875061080791056e-05, 0.010282292030751705, 5.653323751175776e-05, 0.08237511664628983, 4.052989970659837e-05, 6.5601648202573415e-06, 2.3018342290015426e-06, 0.00552819250151515, 1.9506510398059618e-06, 2.6101381536136614e-06, 0.0014058983651921153, 2.368180503253825e-06, 3.904236109519843e-06, 8.379043720196933e-05, 0.6923182010650635, 3.7718909879913554e-06, 0.058592211455106735, 2.5735669623827562e-05, 0.0007383275660686195, 0.000621593848336488, 0.0007254636147990823, 0.00014276293222792447, 7.032128451101016e-06, 1.854067477324861e-06, 0.0008010620367713273, 3.7497904941119486e-06, 4.0553135477239266e-05, 2.7799040253739804e-05, 7.842468221497256e-06, 2.9020905003562802e-06, 6.656407640548423e-05, 0.3109641671180725, 2.299653260706691e-06, 8.219123992603272e-05, 2.476225972714019e-06, 0.00022720577544532716, 1.5313001995309605e-06, 4.412863927427679e-05, 3.723317831827444e-06, 3.21419611282181e-06, 2.107487034663791e-06, 2.0936962755513377e-05, 1.0240460142085794e-05, 6.238221249077469e-05, 0.00014543786528520286, 3.7110878565727035e-06, 4.233744675730122e-06, 1.6147349697348545e-06, 0.00031995950848795474, 8.365311805391684e-05, 4.7327262109320145e-06, 0.0010744326282292604, 2.940345075330697e-05, 3.7131833323655883e-06, 0.0016608451260253787, 0.00010282611765433103, 0.0005823518731631339, 3.237830014768406e-06, 1.2140682883909903e-05, 0.0007951193838380277, 5.326596237864578e-06, 2.449226258249837e-06, 8.249914571933914e-06, 0.020628951489925385, 2.2632577838521684e-06, 1.353093352918222e-06, 0.8998377919197083, 2.107213185809087e-05, 3.809589316006168e-06, 0.00034932486596517265, 8.405042535741813e-06, 3.925760211132001e-06, 1.8156492842535954e-06, 0.00317106069996953, 0.003937046974897385, 2.363050043641124e-06, 2.1613343506032834e-06, 8.830626939015929e-06, 4.0700244426261634e-05, 5.220424100116361e-06, 3.219693326173001e-06, 2.4870605557225645e-06, 0.006447302643209696, 0.0006164334481582046, 5.955294182058424e-05, 4.2387080611661077e-05, 0.37380123138427734, 0.8334941864013672, 0.7164615392684937, 4.33263267041184e-06, 7.975716289365664e-05, 0.003958462737500668, 0.00015579775208607316, 5.693180355592631e-06, 4.959025318385102e-06, 3.97794610762503e-06, 1.964699913514778e-05, 2.9389577775873477e-06, 2.375042822677642e-05, 1.3922557400292135e-06, 4.829564659303287e-06, 0.00024802639381960034, 1.9367596451047575e-06, 0.00017141882563009858, 3.9049509723554365e-06, 3.148809992126189e-05, 4.568653366732178e-06, 8.762249490246177e-05, 2.353428953938419e-06, 0.00016888834943529218, 1.3613431292469613e-05, 0.07665161788463593, 0.8080880641937256, 0.008205948397517204, 0.7479721307754517, 2.847041469067335e-05, 1.3424483995549963e-06, 0.0005213944241404533, 4.4906661059940234e-05, 2.404439146630466e-05, 1.9688015981955687e-06, 1.697736706773867e-06, 6.08987011219142e-06, 2.1003763777116546e-06, 4.468617134989472e-06, 6.938805017853156e-05, 0.19071099162101746, 2.509166506570182e-06, 3.795830025410396e-06, 4.5967422920512035e-05, 2.3391173726849956e-06, 0.0005978134577162564, 0.0011022567050531507, 9.416432476427872e-06, 2.6457807962287916e-06, 2.8714578093058662e-06, 1.980681190616451e-06, 4.295977487345226e-05, 0.0006686921115033329, 0.2713049650192261, 0.1484692543745041, 4.854388862440828e-06, 6.81710025673965e-06, 6.581840807484696e-06, 2.0186876099614892e-06, 1.794311515368463e-06, 0.00029226651531644166, 0.004967979155480862, 0.03999817743897438, 4.3657448259182274e-05, 0.0002265687071485445, 0.0007733248639851809, 0.004235796630382538, 8.888205229595769e-07, 4.5105465687811375e-05, 1.166413403552724e-05, 0.005436763633042574, 9.256981684302446e-06, 1.0949216630251613e-05, 4.377178811409976e-06, 2.0146890165051445e-06, 0.0007295964751392603, 1.8065509721054696e-05, 9.986446457332931e-06, 1.8613276324686012e-06, 6.292704256338766e-06, 1.1084399602623307e-06, 0.10544012486934662, 2.2195542896952247e-06, 7.272572474903427e-06, 0.00022976528271101415, 4.93104598717764e-05, 4.0009508666116744e-05, 3.639698479673825e-05, 6.535717147926334e-06, 0.0006167507963255048, 4.3821487452078145e-06, 0.00017233460675925016, 0.00011895766510860994, 1.4360931345436256e-05, 2.2613398868998047e-06, 4.887719569524052e-06, 6.16170436842367e-05, 6.498372385976836e-05, 0.00016588225844316185, 0.07517701387405396, 0.002209912985563278, 2.4504188331775367e-05, 6.621813099627616e-06, 3.331081416035886e-06, 2.8358533654682105e-06, 0.777946412563324, 0.33507877588272095, 0.0003386993776075542, 0.0027021332643926144, 6.795446552132489e-06, 3.2483640097780153e-06, 7.081935473252088e-05, 0.023382455110549927, 9.978687558032107e-06, 4.590428488882026e-06, 0.9554732441902161, 1.8073660612571985e-05, 0.00019632693147286773, 0.0003109263489022851, 3.382728209544439e-06, 0.0008480690885335207, 2.44715283770347e-06, 2.4201244741561823e-06, 3.280556120444089e-05, 2.4430864868918434e-06, 3.0472718208329752e-05, 2.2371737031789962e-06, 1.0510200809221715e-05, 2.5424460545764305e-06, 8.93811920832377e-06, 3.0220596727303928e-06, 1.0259032023895998e-05, 5.064467040938325e-05, 0.0017707186052575707, 2.5785632260522107e-06, 8.3678442024393e-06, 1.9668255845317617e-05, 3.838941847789101e-05, 4.082892246515257e-06, 1.7965820688914391e-06, 4.156089562457055e-05, 1.7610955183045007e-06, 1.699421954981517e-05, 1.1771411664085463e-05, 7.794673729222268e-05, 8.242892363341525e-06, 6.817413668613881e-05, 0.7081084847450256, 0.5752285122871399, 6.950891929591307e-06, 0.0184829980134964, 3.3700771382427774e-06, 2.3889358544693096e-06, 2.5390550945303403e-05, 8.452532711089589e-06, 0.002036128658801317, 4.084087777300738e-06, 3.585115791793214e-06, 6.726251740474254e-05, 9.686402336228639e-06, 0.004979254677891731, 4.386606178741204e-06, 1.3844051863998175e-05, 1.9333738237037323e-05, 7.380323495453922e-06, 0.885578453540802, 0.5720977187156677, 4.4437183532863855e-06, 3.626113311838708e-06, 1.983851461773156e-06, 6.262166607484687e-06, 0.0001664801238803193, 0.0034013271797448397, 4.6949598981882446e-06, 2.2488304239232093e-05, 3.443146124482155e-05, 0.006341640371829271, 3.3697267554089194e-06, 5.429002158052754e-06, 7.245594133564737e-06, 7.354054832831025e-05, 0.018038153648376465, 2.600085963422316e-06, 0.4422224462032318, 1.2391638847475406e-05, 0.00012239661009516567, 0.00013093835150357336, 7.166071736719459e-05, 2.210886350439978e-06, 6.615199708903674e-06, 3.1423367090610554e-06, 2.110416789946612e-05, 2.8581118840520503e-06, 0.0013660052791237831, 3.813367584371008e-05, 3.7061186048958916e-06, 1.2927249599670176e-06, 2.706751956793596e-06, 1.7564191239216598e-06, 5.874931503058178e-06, 2.5017450752784498e-05, 4.680145138991065e-06, 0.00011990529310423881, 0.9311376810073853, 1.1563226507860236e-05, 2.8479464617703343e-06, 0.00025513701257295907, 4.371285467641428e-05, 2.8143242616351927e-06, 2.1860817014385248e-06, 2.5021887267939746e-05, 1.4320545460577705e-06, 7.80451373429969e-05, 0.00014229773660190403, 1.1475803148641717e-05, 0.0014462274266406894, 5.935838089499157e-06, 2.7736589345295215e-06, 0.002560215536504984, 3.337319867569022e-06, 0.8252480030059814, 3.0713174055563286e-05, 0.0006001009605824947, 0.0009493047837167978, 2.646891061885981e-06, 0.0011679059825837612, 2.8466774892876856e-05, 6.6563643486006185e-06, 0.01642497070133686, 1.833008923313173e-06, 7.295250543393195e-05, 5.093345407658489e-06, 2.294802470714785e-05, 6.3094275901676156e-06, 6.0627044149441645e-06, 3.831866706605069e-06, 0.008181705139577389, 2.662321321622585e-06, 2.5668363377917558e-05, 0.0005444357520900667, 3.188553364452673e-06, 0.0029280860908329487, 0.00011630864173639566, 0.0002421067765681073, 0.00015735402121208608, 2.341690787943662e-06, 1.3395450878306292e-05, 1.232306476595113e-05, 1.915316943268408e-06, 6.476580892922357e-06, 0.0001629189937375486, 2.6505992991587846e-06, 5.255099495116156e-06, 0.009059654548764229, 3.1120753192226402e-06, 3.12273004965391e-06, 0.9263060092926025, 0.006039369851350784, 3.1466279324376956e-06, 0.00011875694326590747, 0.0010297833941876888, 0.00012803297431673855, 4.901779448118759e-06, 0.00012803297431673855, 3.188285518263001e-06, 2.367401293668081e-06, 2.6381592306279344e-06, 3.889648723998107e-05, 2.5269102934544208e-06, 0.00021287902200128883, 3.95232746086549e-05, 0.19231295585632324, 0.3455067574977875, 0.16444526612758636, 0.8882797956466675, 0.00031134934397414327, 3.0126559522614116e-06, 9.865830179478507e-06, 1.6876731478987494e-06, 4.022158009320265e-06, 3.2547241062275134e-06, 0.022877058014273643, 2.7487651095725596e-05, 1.0608442607917823e-05, 0.0022742804139852524, 1.5708899354649475e-06, 3.5120604024996283e-06, 0.00031962565844878554, 1.2697940974248922e-06, 0.010804347693920135, 1.9490113118081354e-05, 5.616388989437837e-06, 0.03553185984492302, 2.545073357396177e-06, 0.00010654878860805184, 3.418880805838853e-05, 1.2860001561421086e-06, 4.758367140311748e-05, 2.1864525479031727e-06, 0.00795808993279934, 0.00013580784434452653, 1.9357162273081485e-06, 4.5810498704668134e-05, 0.0006176908500492573, 1.808322281249275e-06, 0.00017951616609934717, 0.00035902485251426697, 2.4868470518413233e-06, 2.2226229702937417e-05, 4.134941264055669e-06, 2.578730118329986e-06, 0.04381624236702919, 3.708929398271721e-06, 2.9328315577004105e-05, 2.262526095364592e-06, 3.414764933040715e-06, 6.809848855482414e-06, 1.525688048786833e-06, 0.0003288121079094708, 3.155792728648521e-05, 0.003621534211561084, 3.2600362374068936e-06, 1.4705932699143887e-05, 5.042636075813789e-06, 0.0004242938302922994, 6.923457112861797e-05, 9.33702103793621e-05, 4.3638365241349675e-06, 0.0015651159919798374, 1.3740606163992197e-06, 0.3771493434906006, 1.6987764865916688e-06, 5.4038035159464926e-05, 5.68990708416095e-06, 7.940180694276933e-06, 0.2550536096096039, 1.827333471737802e-05, 0.0001513559400336817, 0.002456569578498602, 2.740695526881609e-05, 7.998550427146256e-05, 8.00190719019156e-06, 1.9552232970454497e-06, 2.3284987946681213e-06, 1.735592377372086e-05, 0.0009042994352057576, 0.0001756988203851506, 1.158487157226773e-05, 0.7307468056678772, 0.00021996490249875933, 2.9178507247706875e-05, 3.7192916352069005e-05, 0.000253797770710662, 1.1654859918053262e-05, 2.8204256523167714e-05, 2.6021202756965067e-06, 2.9362433906499064e-06, 2.804253426802461e-06, 0.0015187712851911783, 1.4310047845356166e-05, 1.8644029751158087e-06, 1.45648673424148e-05, 0.00014820134674664587, 2.8868762456113473e-05, 5.611447249975754e-06, 1.1582540537347086e-05, 1.891571082524024e-05, 0.0018935413099825382, 1.554727896291297e-05, 0.22324620187282562, 3.512549255901831e-06, 0.6811045408248901, 3.498955493341782e-06, 2.6963070922647603e-06, 4.121213351027109e-06, 0.9140765070915222, 8.226840691349935e-06, 2.754405613814015e-06, 0.00014487786393146962, 3.102050868619699e-06, 3.1552431210002396e-06, 0.00015406485181301832, 2.7761093406297732e-06, 1.821730620577e-05, 1.4681492757517844e-05, 5.502272415469633e-06, 3.2278589969791938e-06, 6.448790827562334e-06, 4.701726084022084e-06, 6.626033155043842e-06, 2.5317507606814615e-05, 5.472223710967228e-05, 9.677833077148534e-06, 2.5877103325910866e-05, 2.0492886960710166e-06, 1.818628220462415e-06, 1.215183874592185e-05, 0.00026497579528950155, 0.715893566608429, 1.933264456965844e-06, 6.12700532656163e-05, 0.0012413124786689878, 2.816445203279727e-06, 0.02481006272137165, 0.001704229973256588, 4.999723387300037e-05, 1.7477971141488524e-06, 2.3357839381787926e-05, 1.9978003820142476e-06, 0.022857919335365295, 1.8750370145426132e-05, 2.5579900466254912e-05, 2.2324285964714363e-05, 0.0005982244620099664, 5.173749741516076e-05, 4.083261956111528e-06, 2.2678316327073844e-06, 1.20088554922404e-06, 0.00013178838707972318, 3.172461219946854e-05, 1.4271613508753944e-06, 2.6119880658370676e-06, 6.5426334003859665e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 02:28:43.047500: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 02:28:45.303043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-04 02:28:54.156053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 02:28:55.973557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 02:29:03.119062: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 02:29:04.882238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 02:29:12.056823: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 02:29:13.895415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n",
      "Ready for predict()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking:  67%|██████▋   | 2/3 [02:33<01:16, 76.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0.9456437230110168, 0.996783971786499, 0.9666732549667358, 6.670789844065439e-06, 0.08171119540929794, 0.7397224307060242, 0.008762986399233341, 0.07190988212823868, 2.7369012514100177e-06, 4.171929685981013e-05, 0.03734675794839859, 0.9959157109260559, 7.160047516663326e-06, 0.0015542611945420504, 0.941811740398407, 7.144080427679e-06, 0.0004525045515038073, 0.0003630525025073439, 2.427274921501521e-05, 2.3689328372711316e-05, 0.9712644219398499, 0.24110570549964905, 0.0002644528285600245, 0.5568408966064453, 0.00040787571924738586, 0.9873769879341125, 0.036740224808454514, 0.824500322341919, 0.00019249400065746158, 0.8373112678527832, 0.5780090093612671, 7.149000884965062e-05, 0.9149550199508667, 0.00017964081780519336, 0.9679471850395203, 0.9127671718597412, 0.0001653720100875944, 0.001106139156036079, 1.948814133356791e-05, 5.129407327331137e-06, 0.9169193506240845, 0.058274175971746445, 1.4319725778477732e-06, 0.001757692196406424, 8.745161721890327e-06, 0.0010489268461242318, 3.536660369718447e-05, 6.532165571115911e-06, 4.790314778801985e-05, 0.6138888001441956, 0.0001232078648172319, 7.505494522774825e-06, 0.0007147459546104074, 0.0006743752164766192, 0.8066490292549133, 0.9184074401855469, 1.0099402970809024e-05, 0.07541836053133011, 3.299741820228519e-06, 6.844962626928464e-05, 4.490940409596078e-05, 3.3092271678469842e-06, 0.0005152150988578796, 5.137798871146515e-05, 0.00012851980864070356, 0.0007394591229967773, 3.333245331305079e-05, 0.8545377254486084, 6.515273526019882e-06, 0.004646587651222944, 1.5542209439445287e-05, 0.0004629068716894835, 0.7783974409103394, 1.3692613265448017e-06, 0.029263760894536972, 1.4042814655113034e-05, 0.38074594736099243, 4.45879231847357e-06, 0.40141087770462036, 0.0001166803267551586, 0.13233457505702972, 2.5520487270114245e-06, 0.00010193284833803773, 0.18372267484664917, 4.016360981040634e-05, 0.028063634410500526, 0.000487298471853137, 0.0020295388530939817, 0.8122107982635498, 2.898536877182778e-05, 0.9729106426239014, 1.0033318176283501e-05, 9.043745194503572e-06, 0.9085103869438171, 0.9395525455474854, 4.03796593673178e-06, 5.3482199291465804e-05, 3.2069729059003294e-06, 0.0008841798990033567, 5.266805146675324e-06, 0.15301185846328735, 8.766193786868826e-05, 2.1212299543549307e-05, 2.837841975633637e-06, 5.339566632756032e-05, 0.0002940376289188862, 0.2977893352508545, 1.692535079200752e-05, 1.6059093468356878e-06, 0.9923264980316162, 0.7416220307350159, 0.26026079058647156, 0.2967442274093628, 5.008721927879378e-06, 6.76236450090073e-05, 0.005059921182692051, 1.1709256568792625e-06, 4.8072374738694634e-06, 0.004437941126525402, 3.1165152449830202e-06, 7.974453910719603e-05, 0.00010986670531565323, 0.9684047698974609, 0.4841724932193756, 0.9601609110832214, 1.0421510523883626e-05, 2.091876240228885e-06, 0.07707155495882034, 0.006078449543565512, 0.032865870743989944, 4.1550629248376936e-05, 0.5383612513542175, 6.015719009155873e-06, 0.721458911895752, 6.679033504042309e-06, 2.7326521376380697e-05, 0.0013705923920497298, 0.4435144364833832, 3.0266483008745126e-05, 8.175540642696433e-06, 0.00011914133210666478, 1.0504578312975354e-05, 0.6746375560760498, 1.3126495105098002e-05, 0.00027759288786910474, 0.47848284244537354, 1.196409448311897e-05, 0.5526809692382812, 0.8529613018035889, 5.996479376335628e-05, 1.3229951036919374e-05, 0.004513005260378122, 0.0032347331289201975, 0.00015375447401311249, 0.00043240346712991595, 0.06437303870916367, 9.995547588914633e-05, 0.011491340585052967, 1.5969771993695758e-05, 4.874337264482165e-06, 4.147606887272559e-06, 8.16540778032504e-05, 0.009450147859752178, 0.015080757439136505, 0.5862635374069214, 0.05247582867741585, 0.14313480257987976, 1.6141299283845e-06, 4.408838321978692e-06, 1.9265213268226944e-05, 0.09130251407623291, 4.31789430876961e-06, 1.0881347407121211e-05, 1.1303448445687536e-05, 3.6755274777533486e-05, 0.291583389043808, 1.9262324713054113e-06, 0.8124517202377319, 0.03668743744492531, 3.2527382245461922e-06, 0.00020055084314662963, 1.4555160305462778e-05, 4.047334641654743e-06, 0.8030486106872559, 3.9543324419355486e-06, 0.00014850830484647304, 3.826817555818707e-05, 0.010829358361661434, 2.5191477107000537e-05, 0.11921456456184387, 0.0013164111878722906, 1.3246362868812867e-05, 0.0010917738545686007, 2.9166017156967428e-06, 0.5270065069198608, 0.005460525397211313, 0.021102987229824066, 1.5199053450487554e-05, 5.1492033890099265e-06, 0.00016637427324894816, 7.620984433742706e-06, 2.4333519831998274e-05, 0.00023968708410393447, 0.0007808119989931583, 5.8167174756817985e-06, 0.09666285663843155, 0.1301565319299698, 1.8492519302526489e-06, 0.010992353782057762, 0.08451837301254272, 2.2283298676484264e-05, 0.4895678460597992, 5.2465165936155245e-06, 3.908676717401249e-06, 0.15237028896808624, 0.00012293549661990255, 0.00018359489331487566, 9.710734593681991e-05, 0.00010540554649196565, 0.0002229888050351292, 2.2815765987616032e-06, 6.005743853165768e-05, 0.909376859664917, 0.15693658590316772, 0.005169334821403027, 5.430633791547734e-06, 0.0012005195021629333, 5.6187433074228466e-05, 0.2601551115512848, 1.757633981469553e-05, 0.0005290666013024747, 2.333126758458093e-06, 2.5384774744452443e-06, 6.283139555307571e-06, 0.587867796421051, 0.005134460050612688, 0.005941334646195173, 0.01501127053052187, 0.4553495943546295, 3.7805195916007506e-06, 2.1685214051103685e-06, 0.12018986791372299, 3.076430220971815e-05, 4.3298032323946245e-06, 2.3100472390069626e-05, 0.15881647169589996, 0.01835809275507927, 1.813547351048328e-05, 7.3542992140573915e-06, 0.5110127329826355, 0.09724298119544983, 4.946985427523032e-06, 6.90214892529184e-06, 1.3186392607167363e-05, 1.9363722458365373e-05, 0.03200453892350197, 3.334011580591323e-06, 0.0022033657878637314, 4.387280569062568e-05, 6.856808340671705e-06, 0.0002356082113692537, 5.7705339713720605e-05, 4.994298251403961e-06, 0.24010354280471802, 8.895337487047072e-06, 0.8982253074645996, 8.805566722003277e-06, 0.0005469495663419366, 0.0017618670826777816, 0.17616966366767883, 0.00033133200486190617, 0.828922688961029, 5.8634518609324005e-06, 7.476740574929863e-05, 3.0191934001777554e-06, 5.039660663896939e-06, 7.31702966731973e-05, 0.9779384136199951, 1.5970015738275833e-05, 2.7851172035298077e-06, 0.0006101481849327683, 0.05410273000597954, 0.04313826933503151, 0.006977430544793606, 0.05341092497110367, 0.22890318930149078, 0.5842357873916626, 0.018774308264255524, 3.1867170946497936e-06, 2.2481959604192525e-05, 0.016880756244063377, 0.48787039518356323, 0.0007156122010201216, 3.0940973374526948e-06, 3.389752237126231e-06, 0.5627657175064087, 0.0010783466277644038, 9.337962183053605e-06, 1.9869794414262287e-06, 0.0021602921187877655, 0.10414577275514603, 4.0827594602887984e-06, 0.00594684761017561, 2.4024798221944366e-06, 0.375841349363327, 3.608214683481492e-05, 0.8935145139694214, 5.230845636106096e-05, 0.04970772936940193, 2.5347158043587115e-06, 2.267375930387061e-05, 0.009156656451523304, 0.002069327514618635, 0.06262591481208801, 7.327251660171896e-05, 3.125229795841733e-06, 9.338506060885265e-06, 0.23718178272247314, 0.008252843283116817, 6.76922281854786e-05, 6.022031084285118e-05, 0.00016363935719709843, 1.796546894183848e-05, 9.478043807575887e-07, 3.482221245576511e-06, 0.02669490873813629, 0.0004805687931366265, 0.28404685854911804, 2.5000110781547846e-06, 8.418761717621237e-05, 5.752002107328735e-05, 0.9298338890075684, 2.6688344405556563e-06, 0.00022664949938189238, 0.5369706749916077, 2.2528138288180344e-06, 0.003270318964496255, 0.0003364959848113358, 0.0012723676627501845, 1.3386079444899224e-06, 0.7729904055595398, 0.0003375698288436979, 0.001476437784731388, 0.2890383005142212, 6.25413986199419e-06, 1.8097646261594491e-06, 0.3698892593383789, 0.7522964477539062, 2.719435542530846e-06, 2.610150659165811e-06, 2.4520342776668258e-05, 0.0004282427253201604, 1.9406408682698384e-05, 0.001914694090373814, 0.0619187094271183, 0.002502232091501355, 0.00012521864846348763, 2.5379085855092853e-06, 0.20914480090141296, 0.569483757019043, 0.5223606824874878, 4.610575342667289e-05, 0.0008525601588189602, 8.99779024621239e-06, 0.0006526526412926614, 5.5276403145398945e-05, 1.0951649528578855e-05, 9.051286906469613e-05, 6.165916602185462e-06, 1.0329331416869536e-05, 0.7801688313484192, 0.00019908857939299196, 2.8019919682265026e-06, 3.891986580129014e-06, 3.579311169232824e-06, 0.5865647792816162, 0.01719002053141594, 0.053603652864694595, 0.0016832799883559346, 7.212966738734394e-05, 0.9153780937194824, 1.7158629361802014e-06, 0.49475976824760437, 3.1626470445189625e-05, 6.641317304456607e-05, 0.7976750135421753, 1.4704309023727546e-06, 1.6354354102077195e-06, 0.37461793422698975, 0.026122737675905228, 0.7781143188476562, 5.221384981268784e-06, 1.6275406551358174e-06, 4.011443252238678e-06, 1.0373478289693594e-05, 1.0264149750582874e-05, 0.001240491052158177, 3.7209608763077995e-06, 0.7331744432449341, 1.1219242423976539e-06, 0.012662440538406372, 1.2686926993410452e-06, 2.0663860595959704e-06, 1.655599021432863e-06, 2.2934682419872843e-06, 0.9016116857528687, 3.557914851626265e-06, 0.00021554743580054492, 0.0003232995804864913, 0.004192987456917763, 0.10360796004533768, 6.35740980214905e-06, 0.10134816914796829, 0.1209702268242836, 0.0003271670138929039, 7.049903069855645e-05, 3.3460455597378314e-06, 5.135550964041613e-05, 3.311026603114442e-06, 0.9148072600364685, 0.880746066570282, 1.9831135432468727e-05, 0.0006903605535626411, 5.521200364455581e-06, 4.64937829747214e-06, 0.36896610260009766, 0.0013989787548780441, 0.6409865617752075, 0.0010858956957235932, 0.00042801257222890854, 0.592424213886261, 5.526431323232828e-06, 0.9031885862350464, 0.0009054979309439659, 0.0006766871083527803, 0.00012697497732006013, 0.0002399765799054876, 4.5625201892107725e-05, 0.11902172863483429, 0.7863047122955322, 3.4707795748545323e-06, 1.1462830116215628e-05, 9.826832865655888e-06, 2.9136740522517357e-06, 0.0001838019088609144, 0.0011622548336163163, 0.056815795600414276, 0.002419607248157263, 0.0018204759107902646, 0.09900157153606415, 9.58826831265469e-07, 3.1548279366688803e-06, 1.6442132618976757e-05, 0.754509449005127, 0.23203276097774506, 0.002650247421115637, 4.958882345817983e-05, 0.007971989922225475, 1.693955482551246e-06, 8.157643605954945e-06, 0.01961340941488743, 2.756231879175175e-06, 1.670330311753787e-05, 5.272668931866065e-05, 0.011056304909288883, 0.4897778034210205, 0.0002030543255386874, 1.1908793567272369e-05, 4.846322099183453e-06, 0.7552120089530945, 6.283794209593907e-05, 0.000152567881741561, 0.004580153618007898, 2.7532264539331663e-06, 1.651121124268684e-06, 1.2322311704338063e-05, 2.850483542715665e-05, 3.722978362930007e-05, 3.066314320676611e-06, 5.006431456422433e-05, 7.649646431673318e-05, 2.669028617674485e-05, 2.0191228031762876e-06, 0.0007994164479896426, 8.372818228963297e-06, 8.39130825625034e-06, 0.0005771824507974088, 0.19110539555549622, 1.2321078429522458e-05, 2.762245640042238e-05, 3.29065983351029e-06, 6.875061080791056e-05, 0.010282292030751705, 5.653323751175776e-05, 0.08237511664628983, 4.052989970659837e-05, 6.5601648202573415e-06, 2.3018342290015426e-06, 0.00552819250151515, 1.9506510398059618e-06, 2.6101381536136614e-06, 0.0014058983651921153, 2.368180503253825e-06, 3.904236109519843e-06, 8.379043720196933e-05, 0.6923182010650635, 3.7718909879913554e-06, 0.058592211455106735, 2.5735669623827562e-05, 0.0007383275660686195, 0.000621593848336488, 0.0007254636147990823, 0.00014276293222792447, 7.032128451101016e-06, 1.854067477324861e-06, 0.0008010620367713273, 3.7497904941119486e-06, 4.0553135477239266e-05, 2.7799040253739804e-05, 7.842468221497256e-06, 2.9020905003562802e-06, 6.656407640548423e-05, 0.3109641671180725, 2.299653260706691e-06, 8.219123992603272e-05, 2.476225972714019e-06, 0.00022720577544532716, 1.5313001995309605e-06, 4.412863927427679e-05, 3.723317831827444e-06, 3.21419611282181e-06, 2.107487034663791e-06, 2.0936962755513377e-05, 1.0240460142085794e-05, 6.238221249077469e-05, 0.00014543786528520286, 3.7110878565727035e-06, 4.233744675730122e-06, 1.6147349697348545e-06, 0.00031995950848795474, 8.365311805391684e-05, 4.7327262109320145e-06, 0.0010744326282292604, 2.940345075330697e-05, 3.7131833323655883e-06, 0.0016608451260253787, 0.00010282611765433103, 0.0005823518731631339, 3.237830014768406e-06, 1.2140682883909903e-05, 0.0007951193838380277, 5.326596237864578e-06, 2.449226258249837e-06, 8.249914571933914e-06, 0.020628951489925385, 2.2632577838521684e-06, 1.353093352918222e-06, 0.8998377919197083, 2.107213185809087e-05, 3.809589316006168e-06, 0.00034932486596517265, 8.405042535741813e-06, 3.925760211132001e-06, 1.8156492842535954e-06, 0.00317106069996953, 0.003937046974897385, 2.363050043641124e-06, 2.1613343506032834e-06, 8.830626939015929e-06, 4.0700244426261634e-05, 5.220424100116361e-06, 3.219693326173001e-06, 2.4870605557225645e-06, 0.006447302643209696, 0.0006164334481582046, 5.955294182058424e-05, 4.2387080611661077e-05, 0.37380123138427734, 0.8334941864013672, 0.7164615392684937, 4.33263267041184e-06, 7.975716289365664e-05, 0.003958462737500668, 0.00015579775208607316, 5.693180355592631e-06, 4.959025318385102e-06, 3.97794610762503e-06, 1.964699913514778e-05, 2.9389577775873477e-06, 2.375042822677642e-05, 1.3922557400292135e-06, 4.829564659303287e-06, 0.00024802639381960034, 1.9367596451047575e-06, 0.00017141882563009858, 3.9049509723554365e-06, 3.148809992126189e-05, 4.568653366732178e-06, 8.762249490246177e-05, 2.353428953938419e-06, 0.00016888834943529218, 1.3613431292469613e-05, 0.07665161788463593, 0.8080880641937256, 0.008205948397517204, 0.7479721307754517, 2.847041469067335e-05, 1.3424483995549963e-06, 0.0005213944241404533, 4.4906661059940234e-05, 2.404439146630466e-05, 1.9688015981955687e-06, 1.697736706773867e-06, 6.08987011219142e-06, 2.1003763777116546e-06, 4.468617134989472e-06, 6.938805017853156e-05, 0.19071099162101746, 2.509166506570182e-06, 3.795830025410396e-06, 4.5967422920512035e-05, 2.3391173726849956e-06, 0.0005978134577162564, 0.0011022567050531507, 9.416432476427872e-06, 2.6457807962287916e-06, 2.8714578093058662e-06, 1.980681190616451e-06, 4.295977487345226e-05, 0.0006686921115033329, 0.2713049650192261, 0.1484692543745041, 4.854388862440828e-06, 6.81710025673965e-06, 6.581840807484696e-06, 2.0186876099614892e-06, 1.794311515368463e-06, 0.00029226651531644166, 0.004967979155480862, 0.03999817743897438, 4.3657448259182274e-05, 0.0002265687071485445, 0.0007733248639851809, 0.004235796630382538, 8.888205229595769e-07, 4.5105465687811375e-05, 1.166413403552724e-05, 0.005436763633042574, 9.256981684302446e-06, 1.0949216630251613e-05, 4.377178811409976e-06, 2.0146890165051445e-06, 0.0007295964751392603, 1.8065509721054696e-05, 9.986446457332931e-06, 1.8613276324686012e-06, 6.292704256338766e-06, 1.1084399602623307e-06, 0.10544012486934662, 2.2195542896952247e-06, 7.272572474903427e-06, 0.00022976528271101415, 4.93104598717764e-05, 4.0009508666116744e-05, 3.639698479673825e-05, 6.535717147926334e-06, 0.0006167507963255048, 4.3821487452078145e-06, 0.00017233460675925016, 0.00011895766510860994, 1.4360931345436256e-05, 2.2613398868998047e-06, 4.887719569524052e-06, 6.16170436842367e-05, 6.498372385976836e-05, 0.00016588225844316185, 0.07517701387405396, 0.002209912985563278, 2.4504188331775367e-05, 6.621813099627616e-06, 3.331081416035886e-06, 2.8358533654682105e-06, 0.777946412563324, 0.33507877588272095, 0.0003386993776075542, 0.0027021332643926144, 6.795446552132489e-06, 3.2483640097780153e-06, 7.081935473252088e-05, 0.023382455110549927, 9.978687558032107e-06, 4.590428488882026e-06, 0.9554732441902161, 1.8073660612571985e-05, 0.00019632693147286773, 0.0003109263489022851, 3.382728209544439e-06, 0.0008480690885335207, 2.44715283770347e-06, 2.4201244741561823e-06, 3.280556120444089e-05, 2.4430864868918434e-06, 3.0472718208329752e-05, 2.2371737031789962e-06, 1.0510200809221715e-05, 2.5424460545764305e-06, 8.93811920832377e-06, 3.0220596727303928e-06, 1.0259032023895998e-05, 5.064467040938325e-05, 0.0017707186052575707, 2.5785632260522107e-06, 8.3678442024393e-06, 1.9668255845317617e-05, 3.838941847789101e-05, 4.082892246515257e-06, 1.7965820688914391e-06, 4.156089562457055e-05, 1.7610955183045007e-06, 1.699421954981517e-05, 1.1771411664085463e-05, 7.794673729222268e-05, 8.242892363341525e-06, 6.817413668613881e-05, 0.7081084847450256, 0.5752285122871399, 6.950891929591307e-06, 0.0184829980134964, 3.3700771382427774e-06, 2.3889358544693096e-06, 2.5390550945303403e-05, 8.452532711089589e-06, 0.002036128658801317, 4.084087777300738e-06, 3.585115791793214e-06, 6.726251740474254e-05, 9.686402336228639e-06, 0.004979254677891731, 4.386606178741204e-06, 1.3844051863998175e-05, 1.9333738237037323e-05, 7.380323495453922e-06, 0.885578453540802, 0.5720977187156677, 4.4437183532863855e-06, 3.626113311838708e-06, 1.983851461773156e-06, 6.262166607484687e-06, 0.0001664801238803193, 0.0034013271797448397, 4.6949598981882446e-06, 2.2488304239232093e-05, 3.443146124482155e-05, 0.006341640371829271, 3.3697267554089194e-06, 5.429002158052754e-06, 7.245594133564737e-06, 7.354054832831025e-05, 0.018038153648376465, 2.600085963422316e-06, 0.4422224462032318, 1.2391638847475406e-05, 0.00012239661009516567, 0.00013093835150357336, 7.166071736719459e-05, 2.210886350439978e-06, 6.615199708903674e-06, 3.1423367090610554e-06, 2.110416789946612e-05, 2.8581118840520503e-06, 0.0013660052791237831, 3.813367584371008e-05, 3.7061186048958916e-06, 1.2927249599670176e-06, 2.706751956793596e-06, 1.7564191239216598e-06, 5.874931503058178e-06, 2.5017450752784498e-05, 4.680145138991065e-06, 0.00011990529310423881, 0.9311376810073853, 1.1563226507860236e-05, 2.8479464617703343e-06, 0.00025513701257295907, 4.371285467641428e-05, 2.8143242616351927e-06, 2.1860817014385248e-06, 2.5021887267939746e-05, 1.4320545460577705e-06, 7.80451373429969e-05, 0.00014229773660190403, 1.1475803148641717e-05, 0.0014462274266406894, 5.935838089499157e-06, 2.7736589345295215e-06, 0.002560215536504984, 3.337319867569022e-06, 0.8252480030059814, 3.0713174055563286e-05, 0.0006001009605824947, 0.0009493047837167978, 2.646891061885981e-06, 0.0011679059825837612, 2.8466774892876856e-05, 6.6563643486006185e-06, 0.01642497070133686, 1.833008923313173e-06, 7.295250543393195e-05, 5.093345407658489e-06, 2.294802470714785e-05, 6.3094275901676156e-06, 6.0627044149441645e-06, 3.831866706605069e-06, 0.008181705139577389, 2.662321321622585e-06, 2.5668363377917558e-05, 0.0005444357520900667, 3.188553364452673e-06, 0.0029280860908329487, 0.00011630864173639566, 0.0002421067765681073, 0.00015735402121208608, 2.341690787943662e-06, 1.3395450878306292e-05, 1.232306476595113e-05, 1.915316943268408e-06, 6.476580892922357e-06, 0.0001629189937375486, 2.6505992991587846e-06, 5.255099495116156e-06, 0.009059654548764229, 3.1120753192226402e-06, 3.12273004965391e-06, 0.9263060092926025, 0.006039369851350784, 3.1466279324376956e-06, 0.00011875694326590747, 0.0010297833941876888, 0.00012803297431673855, 4.901779448118759e-06, 0.00012803297431673855, 3.188285518263001e-06, 2.367401293668081e-06, 2.6381592306279344e-06, 3.889648723998107e-05, 2.5269102934544208e-06, 0.00021287902200128883, 3.95232746086549e-05, 0.19231295585632324, 0.3455067574977875, 0.16444526612758636, 0.8882797956466675, 0.00031134934397414327, 3.0126559522614116e-06, 9.865830179478507e-06, 1.6876731478987494e-06, 4.022158009320265e-06, 3.2547241062275134e-06, 0.022877058014273643, 2.7487651095725596e-05, 1.0608442607917823e-05, 0.0022742804139852524, 1.5708899354649475e-06, 3.5120604024996283e-06, 0.00031962565844878554, 1.2697940974248922e-06, 0.010804347693920135, 1.9490113118081354e-05, 5.616388989437837e-06, 0.03553185984492302, 2.545073357396177e-06, 0.00010654878860805184, 3.418880805838853e-05, 1.2860001561421086e-06, 4.758367140311748e-05, 2.1864525479031727e-06, 0.00795808993279934, 0.00013580784434452653, 1.9357162273081485e-06, 4.5810498704668134e-05, 0.0006176908500492573, 1.808322281249275e-06, 0.00017951616609934717, 0.00035902485251426697, 2.4868470518413233e-06, 2.2226229702937417e-05, 4.134941264055669e-06, 2.578730118329986e-06, 0.04381624236702919, 3.708929398271721e-06, 2.9328315577004105e-05, 2.262526095364592e-06, 3.414764933040715e-06, 6.809848855482414e-06, 1.525688048786833e-06, 0.0003288121079094708, 3.155792728648521e-05, 0.003621534211561084, 3.2600362374068936e-06, 1.4705932699143887e-05, 5.042636075813789e-06, 0.0004242938302922994, 6.923457112861797e-05, 9.33702103793621e-05, 4.3638365241349675e-06, 0.0015651159919798374, 1.3740606163992197e-06, 0.3771493434906006, 1.6987764865916688e-06, 5.4038035159464926e-05, 5.68990708416095e-06, 7.940180694276933e-06, 0.2550536096096039, 1.827333471737802e-05, 0.0001513559400336817, 0.002456569578498602, 2.740695526881609e-05, 7.998550427146256e-05, 8.00190719019156e-06, 1.9552232970454497e-06, 2.3284987946681213e-06, 1.735592377372086e-05, 0.0009042994352057576, 0.0001756988203851506, 1.158487157226773e-05, 0.7307468056678772, 0.00021996490249875933, 2.9178507247706875e-05, 3.7192916352069005e-05, 0.000253797770710662, 1.1654859918053262e-05, 2.8204256523167714e-05, 2.6021202756965067e-06, 2.9362433906499064e-06, 2.804253426802461e-06, 0.0015187712851911783, 1.4310047845356166e-05, 1.8644029751158087e-06, 1.45648673424148e-05, 0.00014820134674664587, 2.8868762456113473e-05, 5.611447249975754e-06, 1.1582540537347086e-05, 1.891571082524024e-05, 0.0018935413099825382, 1.554727896291297e-05, 0.22324620187282562, 3.512549255901831e-06, 0.6811045408248901, 3.498955493341782e-06, 2.6963070922647603e-06, 4.121213351027109e-06, 0.9140765070915222, 8.226840691349935e-06, 2.754405613814015e-06, 0.00014487786393146962, 3.102050868619699e-06, 3.1552431210002396e-06, 0.00015406485181301832, 2.7761093406297732e-06, 1.821730620577e-05, 1.4681492757517844e-05, 5.502272415469633e-06, 3.2278589969791938e-06, 6.448790827562334e-06, 4.701726084022084e-06, 6.626033155043842e-06, 2.5317507606814615e-05, 5.472223710967228e-05, 9.677833077148534e-06, 2.5877103325910866e-05, 2.0492886960710166e-06, 1.818628220462415e-06, 1.215183874592185e-05, 0.00026497579528950155, 0.715893566608429, 1.933264456965844e-06, 6.12700532656163e-05, 0.0012413124786689878, 2.816445203279727e-06, 0.02481006272137165, 0.001704229973256588, 4.999723387300037e-05, 1.7477971141488524e-06, 2.3357839381787926e-05, 1.9978003820142476e-06, 0.022857919335365295, 1.8750370145426132e-05, 2.5579900466254912e-05, 2.2324285964714363e-05, 0.0005982244620099664, 5.173749741516076e-05, 4.083261956111528e-06, 2.2678316327073844e-06, 1.20088554922404e-06, 0.00013178838707972318, 3.172461219946854e-05, 1.4271613508753944e-06, 2.6119880658370676e-06, 6.5426334003859665e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 02:29:58.460006: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 02:30:00.547033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-04 02:30:07.935053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 02:30:08.809809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-04 02:30:15.125643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 02:30:16.034370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 02:30:22.406475: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 02:30:23.289269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for predict()\n",
      "Ready for predict()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 3/3 [03:43<00:00, 74.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0.9456437230110168, 0.996783971786499, 0.9666732549667358, 6.670789844065439e-06, 0.08171119540929794, 0.7397224307060242, 0.008762986399233341, 0.07190988212823868, 2.7369012514100177e-06, 4.171929685981013e-05, 0.03734675794839859, 0.9959157109260559, 7.160047516663326e-06, 0.0015542611945420504, 0.941811740398407, 7.144080427679e-06, 0.0004525045515038073, 0.0003630525025073439, 2.427274921501521e-05, 2.3689328372711316e-05, 0.9712644219398499, 0.24110570549964905, 0.0002644528285600245, 0.5568408966064453, 0.00040787571924738586, 0.9873769879341125, 0.036740224808454514, 0.824500322341919, 0.00019249400065746158, 0.8373112678527832, 0.5780090093612671, 7.149000884965062e-05, 0.9149550199508667, 0.00017964081780519336, 0.9679471850395203, 0.9127671718597412, 0.0001653720100875944, 0.001106139156036079, 1.948814133356791e-05, 5.129407327331137e-06, 0.9169193506240845, 0.058274175971746445, 1.4319725778477732e-06, 0.001757692196406424, 8.745161721890327e-06, 0.0010489268461242318, 3.536660369718447e-05, 6.532165571115911e-06, 4.790314778801985e-05, 0.6138888001441956, 0.0001232078648172319, 7.505494522774825e-06, 0.0007147459546104074, 0.0006743752164766192, 0.8066490292549133, 0.9184074401855469, 1.0099402970809024e-05, 0.07541836053133011, 3.299741820228519e-06, 6.844962626928464e-05, 4.490940409596078e-05, 3.3092271678469842e-06, 0.0005152150988578796, 5.137798871146515e-05, 0.00012851980864070356, 0.0007394591229967773, 3.333245331305079e-05, 0.8545377254486084, 6.515273526019882e-06, 0.004646587651222944, 1.5542209439445287e-05, 0.0004629068716894835, 0.7783974409103394, 1.3692613265448017e-06, 0.029263760894536972, 1.4042814655113034e-05, 0.38074594736099243, 4.45879231847357e-06, 0.40141087770462036, 0.0001166803267551586, 0.13233457505702972, 2.5520487270114245e-06, 0.00010193284833803773, 0.18372267484664917, 4.016360981040634e-05, 0.028063634410500526, 0.000487298471853137, 0.0020295388530939817, 0.8122107982635498, 2.898536877182778e-05, 0.9729106426239014, 1.0033318176283501e-05, 9.043745194503572e-06, 0.9085103869438171, 0.9395525455474854, 4.03796593673178e-06, 5.3482199291465804e-05, 3.2069729059003294e-06, 0.0008841798990033567, 5.266805146675324e-06, 0.15301185846328735, 8.766193786868826e-05, 2.1212299543549307e-05, 2.837841975633637e-06, 5.339566632756032e-05, 0.0002940376289188862, 0.2977893352508545, 1.692535079200752e-05, 1.6059093468356878e-06, 0.9923264980316162, 0.7416220307350159, 0.26026079058647156, 0.2967442274093628, 5.008721927879378e-06, 6.76236450090073e-05, 0.005059921182692051, 1.1709256568792625e-06, 4.8072374738694634e-06, 0.004437941126525402, 3.1165152449830202e-06, 7.974453910719603e-05, 0.00010986670531565323, 0.9684047698974609, 0.4841724932193756, 0.9601609110832214, 1.0421510523883626e-05, 2.091876240228885e-06, 0.07707155495882034, 0.006078449543565512, 0.032865870743989944, 4.1550629248376936e-05, 0.5383612513542175, 6.015719009155873e-06, 0.721458911895752, 6.679033504042309e-06, 2.7326521376380697e-05, 0.0013705923920497298, 0.4435144364833832, 3.0266483008745126e-05, 8.175540642696433e-06, 0.00011914133210666478, 1.0504578312975354e-05, 0.6746375560760498, 1.3126495105098002e-05, 0.00027759288786910474, 0.47848284244537354, 1.196409448311897e-05, 0.5526809692382812, 0.8529613018035889, 5.996479376335628e-05, 1.3229951036919374e-05, 0.004513005260378122, 0.0032347331289201975, 0.00015375447401311249, 0.00043240346712991595, 0.06437303870916367, 9.995547588914633e-05, 0.011491340585052967, 1.5969771993695758e-05, 4.874337264482165e-06, 4.147606887272559e-06, 8.16540778032504e-05, 0.009450147859752178, 0.015080757439136505, 0.5862635374069214, 0.05247582867741585, 0.14313480257987976, 1.6141299283845e-06, 4.408838321978692e-06, 1.9265213268226944e-05, 0.09130251407623291, 4.31789430876961e-06, 1.0881347407121211e-05, 1.1303448445687536e-05, 3.6755274777533486e-05, 0.291583389043808, 1.9262324713054113e-06, 0.8124517202377319, 0.03668743744492531, 3.2527382245461922e-06, 0.00020055084314662963, 1.4555160305462778e-05, 4.047334641654743e-06, 0.8030486106872559, 3.9543324419355486e-06, 0.00014850830484647304, 3.826817555818707e-05, 0.010829358361661434, 2.5191477107000537e-05, 0.11921456456184387, 0.0013164111878722906, 1.3246362868812867e-05, 0.0010917738545686007, 2.9166017156967428e-06, 0.5270065069198608, 0.005460525397211313, 0.021102987229824066, 1.5199053450487554e-05, 5.1492033890099265e-06, 0.00016637427324894816, 7.620984433742706e-06, 2.4333519831998274e-05, 0.00023968708410393447, 0.0007808119989931583, 5.8167174756817985e-06, 0.09666285663843155, 0.1301565319299698, 1.8492519302526489e-06, 0.010992353782057762, 0.08451837301254272, 2.2283298676484264e-05, 0.4895678460597992, 5.2465165936155245e-06, 3.908676717401249e-06, 0.15237028896808624, 0.00012293549661990255, 0.00018359489331487566, 9.710734593681991e-05, 0.00010540554649196565, 0.0002229888050351292, 2.2815765987616032e-06, 6.005743853165768e-05, 0.909376859664917, 0.15693658590316772, 0.005169334821403027, 5.430633791547734e-06, 0.0012005195021629333, 5.6187433074228466e-05, 0.2601551115512848, 1.757633981469553e-05, 0.0005290666013024747, 2.333126758458093e-06, 2.5384774744452443e-06, 6.283139555307571e-06, 0.587867796421051, 0.005134460050612688, 0.005941334646195173, 0.01501127053052187, 0.4553495943546295, 3.7805195916007506e-06, 2.1685214051103685e-06, 0.12018986791372299, 3.076430220971815e-05, 4.3298032323946245e-06, 2.3100472390069626e-05, 0.15881647169589996, 0.01835809275507927, 1.813547351048328e-05, 7.3542992140573915e-06, 0.5110127329826355, 0.09724298119544983, 4.946985427523032e-06, 6.90214892529184e-06, 1.3186392607167363e-05, 1.9363722458365373e-05, 0.03200453892350197, 3.334011580591323e-06, 0.0022033657878637314, 4.387280569062568e-05, 6.856808340671705e-06, 0.0002356082113692537, 5.7705339713720605e-05, 4.994298251403961e-06, 0.24010354280471802, 8.895337487047072e-06, 0.8982253074645996, 8.805566722003277e-06, 0.0005469495663419366, 0.0017618670826777816, 0.17616966366767883, 0.00033133200486190617, 0.828922688961029, 5.8634518609324005e-06, 7.476740574929863e-05, 3.0191934001777554e-06, 5.039660663896939e-06, 7.31702966731973e-05, 0.9779384136199951, 1.5970015738275833e-05, 2.7851172035298077e-06, 0.0006101481849327683, 0.05410273000597954, 0.04313826933503151, 0.006977430544793606, 0.05341092497110367, 0.22890318930149078, 0.5842357873916626, 0.018774308264255524, 3.1867170946497936e-06, 2.2481959604192525e-05, 0.016880756244063377, 0.48787039518356323, 0.0007156122010201216, 3.0940973374526948e-06, 3.389752237126231e-06, 0.5627657175064087, 0.0010783466277644038, 9.337962183053605e-06, 1.9869794414262287e-06, 0.0021602921187877655, 0.10414577275514603, 4.0827594602887984e-06, 0.00594684761017561, 2.4024798221944366e-06, 0.375841349363327, 3.608214683481492e-05, 0.8935145139694214, 5.230845636106096e-05, 0.04970772936940193, 2.5347158043587115e-06, 2.267375930387061e-05, 0.009156656451523304, 0.002069327514618635, 0.06262591481208801, 7.327251660171896e-05, 3.125229795841733e-06, 9.338506060885265e-06, 0.23718178272247314, 0.008252843283116817, 6.76922281854786e-05, 6.022031084285118e-05, 0.00016363935719709843, 1.796546894183848e-05, 9.478043807575887e-07, 3.482221245576511e-06, 0.02669490873813629, 0.0004805687931366265, 0.28404685854911804, 2.5000110781547846e-06, 8.418761717621237e-05, 5.752002107328735e-05, 0.9298338890075684, 2.6688344405556563e-06, 0.00022664949938189238, 0.5369706749916077, 2.2528138288180344e-06, 0.003270318964496255, 0.0003364959848113358, 0.0012723676627501845, 1.3386079444899224e-06, 0.7729904055595398, 0.0003375698288436979, 0.001476437784731388, 0.2890383005142212, 6.25413986199419e-06, 1.8097646261594491e-06, 0.3698892593383789, 0.7522964477539062, 2.719435542530846e-06, 2.610150659165811e-06, 2.4520342776668258e-05, 0.0004282427253201604, 1.9406408682698384e-05, 0.001914694090373814, 0.0619187094271183, 0.002502232091501355, 0.00012521864846348763, 2.5379085855092853e-06, 0.20914480090141296, 0.569483757019043, 0.5223606824874878, 4.610575342667289e-05, 0.0008525601588189602, 8.99779024621239e-06, 0.0006526526412926614, 5.5276403145398945e-05, 1.0951649528578855e-05, 9.051286906469613e-05, 6.165916602185462e-06, 1.0329331416869536e-05, 0.7801688313484192, 0.00019908857939299196, 2.8019919682265026e-06, 3.891986580129014e-06, 3.579311169232824e-06, 0.5865647792816162, 0.01719002053141594, 0.053603652864694595, 0.0016832799883559346, 7.212966738734394e-05, 0.9153780937194824, 1.7158629361802014e-06, 0.49475976824760437, 3.1626470445189625e-05, 6.641317304456607e-05, 0.7976750135421753, 1.4704309023727546e-06, 1.6354354102077195e-06, 0.37461793422698975, 0.026122737675905228, 0.7781143188476562, 5.221384981268784e-06, 1.6275406551358174e-06, 4.011443252238678e-06, 1.0373478289693594e-05, 1.0264149750582874e-05, 0.001240491052158177, 3.7209608763077995e-06, 0.7331744432449341, 1.1219242423976539e-06, 0.012662440538406372, 1.2686926993410452e-06, 2.0663860595959704e-06, 1.655599021432863e-06, 2.2934682419872843e-06, 0.9016116857528687, 3.557914851626265e-06, 0.00021554743580054492, 0.0003232995804864913, 0.004192987456917763, 0.10360796004533768, 6.35740980214905e-06, 0.10134816914796829, 0.1209702268242836, 0.0003271670138929039, 7.049903069855645e-05, 3.3460455597378314e-06, 5.135550964041613e-05, 3.311026603114442e-06, 0.9148072600364685, 0.880746066570282, 1.9831135432468727e-05, 0.0006903605535626411, 5.521200364455581e-06, 4.64937829747214e-06, 0.36896610260009766, 0.0013989787548780441, 0.6409865617752075, 0.0010858956957235932, 0.00042801257222890854, 0.592424213886261, 5.526431323232828e-06, 0.9031885862350464, 0.0009054979309439659, 0.0006766871083527803, 0.00012697497732006013, 0.0002399765799054876, 4.5625201892107725e-05, 0.11902172863483429, 0.7863047122955322, 3.4707795748545323e-06, 1.1462830116215628e-05, 9.826832865655888e-06, 2.9136740522517357e-06, 0.0001838019088609144, 0.0011622548336163163, 0.056815795600414276, 0.002419607248157263, 0.0018204759107902646, 0.09900157153606415, 9.58826831265469e-07, 3.1548279366688803e-06, 1.6442132618976757e-05, 0.754509449005127, 0.23203276097774506, 0.002650247421115637, 4.958882345817983e-05, 0.007971989922225475, 1.693955482551246e-06, 8.157643605954945e-06, 0.01961340941488743, 2.756231879175175e-06, 1.670330311753787e-05, 5.272668931866065e-05, 0.011056304909288883, 0.4897778034210205, 0.0002030543255386874, 1.1908793567272369e-05, 4.846322099183453e-06, 0.7552120089530945, 6.283794209593907e-05, 0.000152567881741561, 0.004580153618007898, 2.7532264539331663e-06, 1.651121124268684e-06, 1.2322311704338063e-05, 2.850483542715665e-05, 3.722978362930007e-05, 3.066314320676611e-06, 5.006431456422433e-05, 7.649646431673318e-05, 2.669028617674485e-05, 2.0191228031762876e-06, 0.0007994164479896426, 8.372818228963297e-06, 8.39130825625034e-06, 0.0005771824507974088, 0.19110539555549622, 1.2321078429522458e-05, 2.762245640042238e-05, 3.29065983351029e-06, 6.875061080791056e-05, 0.010282292030751705, 5.653323751175776e-05, 0.08237511664628983, 4.052989970659837e-05, 6.5601648202573415e-06, 2.3018342290015426e-06, 0.00552819250151515, 1.9506510398059618e-06, 2.6101381536136614e-06, 0.0014058983651921153, 2.368180503253825e-06, 3.904236109519843e-06, 8.379043720196933e-05, 0.6923182010650635, 3.7718909879913554e-06, 0.058592211455106735, 2.5735669623827562e-05, 0.0007383275660686195, 0.000621593848336488, 0.0007254636147990823, 0.00014276293222792447, 7.032128451101016e-06, 1.854067477324861e-06, 0.0008010620367713273, 3.7497904941119486e-06, 4.0553135477239266e-05, 2.7799040253739804e-05, 7.842468221497256e-06, 2.9020905003562802e-06, 6.656407640548423e-05, 0.3109641671180725, 2.299653260706691e-06, 8.219123992603272e-05, 2.476225972714019e-06, 0.00022720577544532716, 1.5313001995309605e-06, 4.412863927427679e-05, 3.723317831827444e-06, 3.21419611282181e-06, 2.107487034663791e-06, 2.0936962755513377e-05, 1.0240460142085794e-05, 6.238221249077469e-05, 0.00014543786528520286, 3.7110878565727035e-06, 4.233744675730122e-06, 1.6147349697348545e-06, 0.00031995950848795474, 8.365311805391684e-05, 4.7327262109320145e-06, 0.0010744326282292604, 2.940345075330697e-05, 3.7131833323655883e-06, 0.0016608451260253787, 0.00010282611765433103, 0.0005823518731631339, 3.237830014768406e-06, 1.2140682883909903e-05, 0.0007951193838380277, 5.326596237864578e-06, 2.449226258249837e-06, 8.249914571933914e-06, 0.020628951489925385, 2.2632577838521684e-06, 1.353093352918222e-06, 0.8998377919197083, 2.107213185809087e-05, 3.809589316006168e-06, 0.00034932486596517265, 8.405042535741813e-06, 3.925760211132001e-06, 1.8156492842535954e-06, 0.00317106069996953, 0.003937046974897385, 2.363050043641124e-06, 2.1613343506032834e-06, 8.830626939015929e-06, 4.0700244426261634e-05, 5.220424100116361e-06, 3.219693326173001e-06, 2.4870605557225645e-06, 0.006447302643209696, 0.0006164334481582046, 5.955294182058424e-05, 4.2387080611661077e-05, 0.37380123138427734, 0.8334941864013672, 0.7164615392684937, 4.33263267041184e-06, 7.975716289365664e-05, 0.003958462737500668, 0.00015579775208607316, 5.693180355592631e-06, 4.959025318385102e-06, 3.97794610762503e-06, 1.964699913514778e-05, 2.9389577775873477e-06, 2.375042822677642e-05, 1.3922557400292135e-06, 4.829564659303287e-06, 0.00024802639381960034, 1.9367596451047575e-06, 0.00017141882563009858, 3.9049509723554365e-06, 3.148809992126189e-05, 4.568653366732178e-06, 8.762249490246177e-05, 2.353428953938419e-06, 0.00016888834943529218, 1.3613431292469613e-05, 0.07665161788463593, 0.8080880641937256, 0.008205948397517204, 0.7479721307754517, 2.847041469067335e-05, 1.3424483995549963e-06, 0.0005213944241404533, 4.4906661059940234e-05, 2.404439146630466e-05, 1.9688015981955687e-06, 1.697736706773867e-06, 6.08987011219142e-06, 2.1003763777116546e-06, 4.468617134989472e-06, 6.938805017853156e-05, 0.19071099162101746, 2.509166506570182e-06, 3.795830025410396e-06, 4.5967422920512035e-05, 2.3391173726849956e-06, 0.0005978134577162564, 0.0011022567050531507, 9.416432476427872e-06, 2.6457807962287916e-06, 2.8714578093058662e-06, 1.980681190616451e-06, 4.295977487345226e-05, 0.0006686921115033329, 0.2713049650192261, 0.1484692543745041, 4.854388862440828e-06, 6.81710025673965e-06, 6.581840807484696e-06, 2.0186876099614892e-06, 1.794311515368463e-06, 0.00029226651531644166, 0.004967979155480862, 0.03999817743897438, 4.3657448259182274e-05, 0.0002265687071485445, 0.0007733248639851809, 0.004235796630382538, 8.888205229595769e-07, 4.5105465687811375e-05, 1.166413403552724e-05, 0.005436763633042574, 9.256981684302446e-06, 1.0949216630251613e-05, 4.377178811409976e-06, 2.0146890165051445e-06, 0.0007295964751392603, 1.8065509721054696e-05, 9.986446457332931e-06, 1.8613276324686012e-06, 6.292704256338766e-06, 1.1084399602623307e-06, 0.10544012486934662, 2.2195542896952247e-06, 7.272572474903427e-06, 0.00022976528271101415, 4.93104598717764e-05, 4.0009508666116744e-05, 3.639698479673825e-05, 6.535717147926334e-06, 0.0006167507963255048, 4.3821487452078145e-06, 0.00017233460675925016, 0.00011895766510860994, 1.4360931345436256e-05, 2.2613398868998047e-06, 4.887719569524052e-06, 6.16170436842367e-05, 6.498372385976836e-05, 0.00016588225844316185, 0.07517701387405396, 0.002209912985563278, 2.4504188331775367e-05, 6.621813099627616e-06, 3.331081416035886e-06, 2.8358533654682105e-06, 0.777946412563324, 0.33507877588272095, 0.0003386993776075542, 0.0027021332643926144, 6.795446552132489e-06, 3.2483640097780153e-06, 7.081935473252088e-05, 0.023382455110549927, 9.978687558032107e-06, 4.590428488882026e-06, 0.9554732441902161, 1.8073660612571985e-05, 0.00019632693147286773, 0.0003109263489022851, 3.382728209544439e-06, 0.0008480690885335207, 2.44715283770347e-06, 2.4201244741561823e-06, 3.280556120444089e-05, 2.4430864868918434e-06, 3.0472718208329752e-05, 2.2371737031789962e-06, 1.0510200809221715e-05, 2.5424460545764305e-06, 8.93811920832377e-06, 3.0220596727303928e-06, 1.0259032023895998e-05, 5.064467040938325e-05, 0.0017707186052575707, 2.5785632260522107e-06, 8.3678442024393e-06, 1.9668255845317617e-05, 3.838941847789101e-05, 4.082892246515257e-06, 1.7965820688914391e-06, 4.156089562457055e-05, 1.7610955183045007e-06, 1.699421954981517e-05, 1.1771411664085463e-05, 7.794673729222268e-05, 8.242892363341525e-06, 6.817413668613881e-05, 0.7081084847450256, 0.5752285122871399, 6.950891929591307e-06, 0.0184829980134964, 3.3700771382427774e-06, 2.3889358544693096e-06, 2.5390550945303403e-05, 8.452532711089589e-06, 0.002036128658801317, 4.084087777300738e-06, 3.585115791793214e-06, 6.726251740474254e-05, 9.686402336228639e-06, 0.004979254677891731, 4.386606178741204e-06, 1.3844051863998175e-05, 1.9333738237037323e-05, 7.380323495453922e-06, 0.885578453540802, 0.5720977187156677, 4.4437183532863855e-06, 3.626113311838708e-06, 1.983851461773156e-06, 6.262166607484687e-06, 0.0001664801238803193, 0.0034013271797448397, 4.6949598981882446e-06, 2.2488304239232093e-05, 3.443146124482155e-05, 0.006341640371829271, 3.3697267554089194e-06, 5.429002158052754e-06, 7.245594133564737e-06, 7.354054832831025e-05, 0.018038153648376465, 2.600085963422316e-06, 0.4422224462032318, 1.2391638847475406e-05, 0.00012239661009516567, 0.00013093835150357336, 7.166071736719459e-05, 2.210886350439978e-06, 6.615199708903674e-06, 3.1423367090610554e-06, 2.110416789946612e-05, 2.8581118840520503e-06, 0.0013660052791237831, 3.813367584371008e-05, 3.7061186048958916e-06, 1.2927249599670176e-06, 2.706751956793596e-06, 1.7564191239216598e-06, 5.874931503058178e-06, 2.5017450752784498e-05, 4.680145138991065e-06, 0.00011990529310423881, 0.9311376810073853, 1.1563226507860236e-05, 2.8479464617703343e-06, 0.00025513701257295907, 4.371285467641428e-05, 2.8143242616351927e-06, 2.1860817014385248e-06, 2.5021887267939746e-05, 1.4320545460577705e-06, 7.80451373429969e-05, 0.00014229773660190403, 1.1475803148641717e-05, 0.0014462274266406894, 5.935838089499157e-06, 2.7736589345295215e-06, 0.002560215536504984, 3.337319867569022e-06, 0.8252480030059814, 3.0713174055563286e-05, 0.0006001009605824947, 0.0009493047837167978, 2.646891061885981e-06, 0.0011679059825837612, 2.8466774892876856e-05, 6.6563643486006185e-06, 0.01642497070133686, 1.833008923313173e-06, 7.295250543393195e-05, 5.093345407658489e-06, 2.294802470714785e-05, 6.3094275901676156e-06, 6.0627044149441645e-06, 3.831866706605069e-06, 0.008181705139577389, 2.662321321622585e-06, 2.5668363377917558e-05, 0.0005444357520900667, 3.188553364452673e-06, 0.0029280860908329487, 0.00011630864173639566, 0.0002421067765681073, 0.00015735402121208608, 2.341690787943662e-06, 1.3395450878306292e-05, 1.232306476595113e-05, 1.915316943268408e-06, 6.476580892922357e-06, 0.0001629189937375486, 2.6505992991587846e-06, 5.255099495116156e-06, 0.009059654548764229, 3.1120753192226402e-06, 3.12273004965391e-06, 0.9263060092926025, 0.006039369851350784, 3.1466279324376956e-06, 0.00011875694326590747, 0.0010297833941876888, 0.00012803297431673855, 4.901779448118759e-06, 0.00012803297431673855, 3.188285518263001e-06, 2.367401293668081e-06, 2.6381592306279344e-06, 3.889648723998107e-05, 2.5269102934544208e-06, 0.00021287902200128883, 3.95232746086549e-05, 0.19231295585632324, 0.3455067574977875, 0.16444526612758636, 0.8882797956466675, 0.00031134934397414327, 3.0126559522614116e-06, 9.865830179478507e-06, 1.6876731478987494e-06, 4.022158009320265e-06, 3.2547241062275134e-06, 0.022877058014273643, 2.7487651095725596e-05, 1.0608442607917823e-05, 0.0022742804139852524, 1.5708899354649475e-06, 3.5120604024996283e-06, 0.00031962565844878554, 1.2697940974248922e-06, 0.010804347693920135, 1.9490113118081354e-05, 5.616388989437837e-06, 0.03553185984492302, 2.545073357396177e-06, 0.00010654878860805184, 3.418880805838853e-05, 1.2860001561421086e-06, 4.758367140311748e-05, 2.1864525479031727e-06, 0.00795808993279934, 0.00013580784434452653, 1.9357162273081485e-06, 4.5810498704668134e-05, 0.0006176908500492573, 1.808322281249275e-06, 0.00017951616609934717, 0.00035902485251426697, 2.4868470518413233e-06, 2.2226229702937417e-05, 4.134941264055669e-06, 2.578730118329986e-06, 0.04381624236702919, 3.708929398271721e-06, 2.9328315577004105e-05, 2.262526095364592e-06, 3.414764933040715e-06, 6.809848855482414e-06, 1.525688048786833e-06, 0.0003288121079094708, 3.155792728648521e-05, 0.003621534211561084, 3.2600362374068936e-06, 1.4705932699143887e-05, 5.042636075813789e-06, 0.0004242938302922994, 6.923457112861797e-05, 9.33702103793621e-05, 4.3638365241349675e-06, 0.0015651159919798374, 1.3740606163992197e-06, 0.3771493434906006, 1.6987764865916688e-06, 5.4038035159464926e-05, 5.68990708416095e-06, 7.940180694276933e-06, 0.2550536096096039, 1.827333471737802e-05, 0.0001513559400336817, 0.002456569578498602, 2.740695526881609e-05, 7.998550427146256e-05, 8.00190719019156e-06, 1.9552232970454497e-06, 2.3284987946681213e-06, 1.735592377372086e-05, 0.0009042994352057576, 0.0001756988203851506, 1.158487157226773e-05, 0.7307468056678772, 0.00021996490249875933, 2.9178507247706875e-05, 3.7192916352069005e-05, 0.000253797770710662, 1.1654859918053262e-05, 2.8204256523167714e-05, 2.6021202756965067e-06, 2.9362433906499064e-06, 2.804253426802461e-06, 0.0015187712851911783, 1.4310047845356166e-05, 1.8644029751158087e-06, 1.45648673424148e-05, 0.00014820134674664587, 2.8868762456113473e-05, 5.611447249975754e-06, 1.1582540537347086e-05, 1.891571082524024e-05, 0.0018935413099825382, 1.554727896291297e-05, 0.22324620187282562, 3.512549255901831e-06, 0.6811045408248901, 3.498955493341782e-06, 2.6963070922647603e-06, 4.121213351027109e-06, 0.9140765070915222, 8.226840691349935e-06, 2.754405613814015e-06, 0.00014487786393146962, 3.102050868619699e-06, 3.1552431210002396e-06, 0.00015406485181301832, 2.7761093406297732e-06, 1.821730620577e-05, 1.4681492757517844e-05, 5.502272415469633e-06, 3.2278589969791938e-06, 6.448790827562334e-06, 4.701726084022084e-06, 6.626033155043842e-06, 2.5317507606814615e-05, 5.472223710967228e-05, 9.677833077148534e-06, 2.5877103325910866e-05, 2.0492886960710166e-06, 1.818628220462415e-06, 1.215183874592185e-05, 0.00026497579528950155, 0.715893566608429, 1.933264456965844e-06, 6.12700532656163e-05, 0.0012413124786689878, 2.816445203279727e-06, 0.02481006272137165, 0.001704229973256588, 4.999723387300037e-05, 1.7477971141488524e-06, 2.3357839381787926e-05, 1.9978003820142476e-06, 0.022857919335365295, 1.8750370145426132e-05, 2.5579900466254912e-05, 2.2324285964714363e-05, 0.0005982244620099664, 5.173749741516076e-05, 4.083261956111528e-06, 2.2678316327073844e-06, 1.20088554922404e-06, 0.00013178838707972318, 3.172461219946854e-05, 1.4271613508753944e-06, 2.6119880658370676e-06, 6.5426334003859665e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# to add to evaluation.py\n",
    "import torch.multiprocessing as mp\n",
    "from multiprocessing import Manager\n",
    "\n",
    "# DDP related configuration\n",
    "world_size = torch.cuda.device_count()\n",
    "print(\"world_size\", world_size)\n",
    "\n",
    "manager = Manager()\n",
    "output_list = manager.list()\n",
    "\n",
    "\n",
    "for qid, hit in tqdm(hits.items(), total=len(hits), desc=\"Reranking\"):\n",
    "\n",
    "    reranking_query = query_dict[qid]\n",
    "\n",
    "    mp.spawn(rerank_t5_DDP,\n",
    "            args=(\n",
    "                world_size, \n",
    "                reranking_query,\n",
    "                [json.loads(searcher.doc(doc_object.docid).raw())[\"contents\"] for doc_object in hit],\n",
    "                tokenizer,\n",
    "                decoder_stard_id,\n",
    "                targeted_ids,\n",
    "                output_list\n",
    "                ),\n",
    "            nprocs=world_size,\n",
    "            join=True)\n",
    "\n",
    "\n",
    "    reranked_score = list(output_list)\n",
    "    print(len(reranked_score))\n",
    "    print(reranked_score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "print(a[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents with \"same\" T5 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rech/huiyuche/envs/trec_ikat/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Jul 12, 2024 12:05:21 P.M. org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "retrieval_query_list = [\n",
    "    \"Can you compare mozzarella with plant-based cheese?\",\n",
    "    \"No, I mean Academy Awards and Golden Globe Awards.\"\n",
    "]\n",
    "qid_list_string = [\n",
    "    \"10-1-6\",\n",
    "    \"15-1-5\"\n",
    "]\n",
    "\n",
    "query_dict = {\n",
    "    qid: query for qid, query in zip(qid_list_string, retrieval_query_list)\n",
    "}\n",
    "\n",
    "searcher = LuceneSearcher(\"/part/01/Tmp/yuchen/indexes/clueweb22b_ikat23_fengran_sparse_index_2/\")\n",
    "#searcher = LuceneSearcher(\"/data/rech/huiyuche/TREC_iKAT_2024/data/indexes/clueweb22b_ikat23_fengran_sparse_index_2\")\n",
    "searcher.set_bm25(0.9, 0.4)\n",
    "hits = searcher.batch_search(retrieval_query_list, qid_list_string, k = 50, threads = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'org.apache.lucene.document.Document' object has no attribute 'raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m doc_object\u001b[38;5;241m.\u001b[39mlucene_document\u001b[38;5;241m.\u001b[39mraw\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;66;03m#return json.loads(searcher.doc(doc_object.docid).raw())[\"contents\"]\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_doc_content_by_docid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclueweb22-en0012-49-03406:9\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10-1-6\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mget_doc_content_by_docid\u001b[0;34m(docid, hit_list)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc_object \u001b[38;5;129;01min\u001b[39;00m hit_list:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m doc_object\u001b[38;5;241m.\u001b[39mdocid \u001b[38;5;241m==\u001b[39m docid:\n\u001b[0;32m----> 5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdoc_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlucene_document\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'org.apache.lucene.document.Document' object has no attribute 'raw'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def get_doc_content_by_docid(docid, hit_list):\n",
    "    for doc_object in hit_list:\n",
    "        if doc_object.docid == docid:\n",
    "            return doc_object.lucene_document.raw\n",
    "            #return json.loads(searcher.doc(doc_object.docid).raw())[\"contents\"]\n",
    "print(get_doc_content_by_docid(\"clueweb22-en0012-49-03406:9\", hits[\"10-1-6\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRA LARGE MARGHERITA $7.00 Diced tomatoes & stretchy mozzarella, topped with oregano. 0 EXTRA LARGE CHEESY GARLIC WITH CRÈME FRAICHE $7.00 Stretchy mozzarella, classic garlic sauce & oregano on a crème fraiche base. 0 MINI PEPPERONI $3.00 A Domino's classic. Slices of crispy pepperoni & creamy mozzarella, made on Deep Pan crust. 0 MINI CHEESY GARLIC WITH CRÈME FRAICHE $3.00 Stretchy mozzarella, classic garlic sauce & oregano, made on Deep Pan crust with crème fraiche. 0 MINI MARGHERITA $3.00 Diced tomatoes & stretchy mozzarella, made on Deep Pan crust & topped with oregano. 0 MINI HAM & CHEESE $3.00 Strips of smokey leg ham & creamy mozzarella, made on Deep Pan crust. 0 MINI SIMPLY CHEESE $3.00 Simply topped with lots of melted mozzarella goodness. Made on Deep Pan crust. 0 VEGETARIAN PLANT-BASED FIRE BREATHER $14.45 Plant-based beef, fiery jalapenos, diced tomato & sliced red onion with a spicy hit of chilli flakes. 0 VEGETARIAN PLANT-BASED GODFATHER $16.65 Plant-based beef, vibrant capsicum, diced tomato & Kalamata olives on a zesty garlic & pizza sauce base, topped with oregano. 0 VEGAN FIRE BREATHER $14.95 Vegan cheese, plant-based beef, fiery jalapenos, diced tomato & sliced red onion with a spicy hit of chilli flakes. 0 VEGAN GODFATHER $14.95 Vegan cheese, plant-based beef, vibrant capsicum, diced tomato & Kalamata olives on a zesty garlic & pizza sauce base, topped with oregano. 0\n"
     ]
    }
   ],
   "source": [
    "print(get_doc_content_by_docid(\"clueweb22-en0000-88-08679:9\", hits[\"10-1-6\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRA LARGE MARGHERITA $7.00 Diced tomatoes & stretchy mozzarella, topped with oregano. 0 EXTRA LARGE CHEESY GARLIC WITH CRÈME FRAICHE $7.00 Stretchy mozzarella, classic garlic sauce & oregano on a crème fraiche base. 0 MINI PEPPERONI $3.00 A Domino's classic. Slices of crispy pepperoni & creamy mozzarella, made on Deep Pan crust. 0 MINI CHEESY GARLIC WITH CRÈME FRAICHE $3.00 Stretchy mozzarella, classic garlic sauce & oregano, made on Deep Pan crust with crème fraiche. 0 MINI MARGHERITA $3.00 Diced tomatoes & stretchy mozzarella, made on Deep Pan crust & topped with oregano. 0 MINI HAM & CHEESE $3.00 Strips of smokey leg ham & creamy mozzarella, made on Deep Pan crust. 0 MINI SIMPLY CHEESE $3.00 Simply topped with lots of melted mozzarella goodness. Made on Deep Pan crust. 0 VEGETARIAN PLANT-BASED FIRE BREATHER $14.45 Plant-based beef, fiery jalapenos, diced tomato & sliced red onion with a spicy hit of chilli flakes. 0 VEGETARIAN PLANT-BASED GODFATHER $16.65 Plant-based beef, vibrant capsicum, diced tomato & Kalamata olives on a zesty garlic & pizza sauce base, topped with oregano. 0 VEGAN FIRE BREATHER $14.95 Vegan cheese, plant-based beef, fiery jalapenos, diced tomato & sliced red onion with a spicy hit of chilli flakes. 0 VEGAN GODFATHER $14.95 Vegan cheese, plant-based beef, vibrant capsicum, diced tomato & Kalamata olives on a zesty garlic & pizza sauce base, topped with oregano. 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(get_doc_content_by_docid(\"clueweb22-en0034-01-09394:9\", hits[\"10-1-6\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rech/huiyuche/envs/trec_ikat/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-15 01:43:14.508614: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-15 01:43:14.559720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-15 01:43:19.710949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/data/rech/huiyuche/TREC_iKAT_2024/src/')\n",
    "from llm import RepllamaDocumentEncoder\n",
    "import importlib\n",
    "importlib.reload(sys.modules['llm'])\n",
    "from llm import RepllamaDocumentEncoder\n",
    "\n",
    "cache_dir = \"/data/rech/huiyuche/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12823"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "del repllama_encoder\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "repllama_encoder = RepllamaDocumentEncoder(\n",
    "    cache_dir=cache_dir,\n",
    "    device_map = \"cuda:0\",\n",
    "    quant_4bit = False,\n",
    "    quant_8bit = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01464844 -0.01672363  0.01953125 ...  0.00439453  0.00958252\n",
      "   0.02478027]\n",
      " [ 0.02319336 -0.00154114 -0.00854492 ... -0.01031494  0.00466919\n",
      "   0.00592041]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rech/huiyuche/TREC_iKAT_2024/src/llm.py:460: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  passage_embeddings = torch.tensor(passage_embeddings, dtype=torch.float32).numpy()\n"
     ]
    }
   ],
   "source": [
    "outputs = repllama_encoder.encode(\n",
    "    [\n",
    "       \"The capital of France is Paris and Beijing.\",\n",
    "       \"Paris is the capital of France.\" \n",
    "        ]\n",
    ")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4096])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "passage_embeddings = outputs.last_hidden_state[:,-1,:].detach().cpu()\n",
    "passage_embeddings = torch.nn.functional.normalize(passage_embeddings, p=2, dim=1)\n",
    "print(passage_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01452637 -0.01660156  0.01940918 ...  0.0043335   0.00952148\n",
      "   0.0246582 ]\n",
      " [ 0.02331543 -0.00151825 -0.00854492 ... -0.01037598  0.00466919\n",
      "   0.00595093]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_946801/2741188116.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  passage_embeddings = torch.tensor(passage_embeddings, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "passage_embeddings = torch.tensor(passage_embeddings, dtype=torch.float32)\n",
    "print(passage_embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2673, 0.5345, 0.8018],\n",
      "        [0.4558, 0.5698, 0.6838],\n",
      "        [0.5026, 0.5744, 0.6462]])\n"
     ]
    }
   ],
   "source": [
    "test_tensor = torch.tensor([[1,2,3],[4,5,6],[7,8,9]], dtype=torch.float32)\n",
    "print(torch.nn.functional.normalize(test_tensor, p=2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.00002278"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2673**2 + 0.5345**2 + 0.8018**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "repllama_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test prompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/data/rech/huiyuche/TREC_iKAT_2024/src/')\n",
    "import importlib\n",
    "import prompter\n",
    "import topics\n",
    "import llm\n",
    "importlib.reload(sys.modules['prompter'])\n",
    "importlib.reload(sys.modules['topics'])\n",
    "importlib.reload(sys.modules['llm'])\n",
    "\n",
    "from prompter import RewriteAndResponsePromptor\n",
    "from llm import OpenAILM\n",
    "\n",
    "from topics import (\n",
    "    Turn, \n",
    "    load_turns_from_ikat_topic_files, \n",
    "    save_turns_to_json, \n",
    "    load_turns_from_json,\n",
    "    Result,\n",
    "    Reformulation,\n",
    "    get_context_by_qid,\n",
    "    get_turn_by_qid\n",
    ")\n",
    "from dataclasses import asdict\n",
    "cache_dir = \"/data/rech/huiyuche/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "rewriter = OpenAILM(\n",
    "        api_key=os.environ['openai_key'],\n",
    "        model_name= model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = RewriteAndResponsePromptor(\n",
    "    demo_file = \"/data/rech/huiyuche/TREC_iKAT_2024/data/topics/ikat23/original_demonstration.json\",\n",
    "    enable_cot = False\n",
    ")\n",
    "turn_list = load_turns_from_json(\"/data/rech/huiyuche/TREC_iKAT_2024/data/topics/ikat23/ikat_2023_test.json\")\n",
    "context = get_context_by_qid( \"9-1-3\",turn_list)\n",
    "current_turn = get_turn_by_qid(\"9-1-3\",turn_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rewrite: What about the DASH diet? I heard it is a healthy diet.\\nResponse: The DASH (Dietary Approaches to Stop Hypertension) diet is indeed a healthy eating plan that is designed to help lower blood pressure and improve overall health. It emphasizes consuming fruits, vegetables, whole grains, lean proteins, and low-fat dairy while limiting sodium, saturated fats, and added sugars. The DASH diet has been shown to be effective in reducing blood pressure and promoting heart health. It is considered a well-balanced and sustainable diet that can benefit individuals looking to improve their overall health and well-being.']\n"
     ]
    }
   ],
   "source": [
    "prompt = prompter.build_turn_prompt(context,current_turn)\n",
    "\n",
    "print(rewriter.generate(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #1:\n",
      "Question: What should I consider when buying a phone?\n",
      "Rewrite: What should I consider when buying a phone?\n",
      "Response: The design of the phone and the overall look and feel of the phone are very important. You should be comfortable with the way the phone looks and feels when you hold it in your hand. In addition, don't be afraid to get a phone from a different manufacturer than you're used to. Consider an older, used, or refurbished phone to save money. Sometimes a year-old or even a two-year-old device might give you everything you need.\n",
      "\n",
      "Question: Cool. Which one would you recommend?\n",
      "Rewrite: Cool. Which smartphone would you recommend for me?\n",
      "Response: Just because a phone has everything and the kitchen sink doesn't mean you have to buy it — it's just an option for those who might have a larger budget. The same logic applies to pretty much anything you can buy. If you want something more affordable, cheaper, but has the latest in Android and that comes with 5000mAh of battery, then the Motorola Moto G7 Power is an excellent phone to consider. If you want an Android smartphone that's cheaper, more affordable, and you still find the Moto G7 Power a bit off your budget, then I highly recommend the Samsung Galaxy A10e.\n",
      "\n",
      "Question: Tell me more about the first one.\n",
      "Rewrite: Tell me more about the Motorola Moto G7 Power.\n",
      "Response: It sports an 8-megapixel camera on the front for selfies with an f/2.2 aperture and a pixel size of 1.12-micron. Motorola Moto G7 Power is based on Android 9.0 and packs 32GB of inbuilt storage that can be expanded via a microSD card (up to 512GB). The Motorola Moto G7 Power is a dual-SIM (GSM and GSM) smartphone that accepts Nano-SIM and Nano-SIM cards. The Motorola Moto G7 Power measures 160.83 x 76.00 x 9.40mm (height x width x thickness) and weighs 198.00 grams. It was launched in Marine Blue color. Connectivity options on the Motorola Moto G7 Power include Wi-Fi 802.11 a/b/g/n, GPS, Bluetooth v4.20, USB Type-C, FM radio, 3G, and 4G. Sensors on the phone include an accelerometer, ambient light sensor, compass/ magnetometer, gyroscope, proximity sensor, and fingerprint sensor. The Motorola Moto G7 Power supports face unlock. As of 1st June 2021, Motorola Moto G7 Power price in India starts at Rs. 14,990.\n",
      "\n",
      "Question: How much cheaper is the A10e?\n",
      "Rewrite: How much cheaper is the Samsung Galaxy A10e than the Motorola Moto G7 Power?\n",
      "Response: The current lowest price found for Samsung Galaxy A10e is 7,499 and for Motorola Moto G7 is 13990. The details of both of these products were last updated on Jun 02, 2021.\n",
      "\n",
      "Question: Wow, that's almost half the cost! Can you compare them?\n",
      "Rewrite: Wow, that's almost half the cost of a Samsung Galaxy A10e. Can you compare the Samsung Galaxy A10e with the Motorola Moto G7 Power?\n",
      "Response: For the past few years, Motorola's G-series has consistently ranked as a favourite budget phone among users and the G7 continues this legacy as an exceptional phone for its price. All in all, people still love the Moto G7. It's a fantastic, affordable phone, and it won awards for its inexpensive price, good camera and reliable performance. But the Galaxy A50 is better, even if it's a bit more expensive. For that, it too earned a few awards. If you have an extra $70 to spare, go for the Galaxy A50 over the Moto G7.\n",
      "\n",
      "Question: Interesting. What makes the G7 so good?\n",
      "Rewrite: Interesting. What makes the Motorola G7 such a good award-winning phone?\n",
      "Response: Moto G7 and Moto G7 Power deliver a no-frills experience with few compromises. You're getting a great design, modern features, and clean and fast software. For the price, Motorola elegantly balances performance and features without making too many compromises. The biggest selling point of the Moto G 7 Power is the battery capacity and battery life.\n",
      "\n",
      "Question: Interesting. Why is the A50 better?\n",
      "Rewrite: Why is the Samsung Galaxy A50 better than the Moto G7?\n",
      "Response: The Galaxy A50's camera is way more flushed, it looks flatter and sleeker. The on-screen camera notch for the front-facing camera is also smaller. The A50 has an AMOLED display compared to the Moto G7's LCD screen. Though it doesn't shoot 4K video and Samsung's One UI is not everyone's favorite.\n",
      "\n",
      "Example #2:\n",
      "Question: I took a year-long break from running due to work commitments. Tell me, how do I get back into it?\n",
      "Rewrite: How do I get back into running after a year-long break due to work commitments?\n",
      "Response: Don't rush getting back into running and you'll drastically avoid injury. Better to start with a few months of short, easy runs rather than rushing straight in with long and hard runs. Re-asses yourself every month and then increase mileage slowly if you are feeling strong.\n",
      "\n",
      "Question: Cool. What exercises can I mix with it?\n",
      "Rewrite: Cool. What exercises can I mix with running?\n",
      "Response: If you want to lift heavier weights in the gym, that can be done twice per week. Strength workouts should complement your running, not detract from it. If you're so sore from strength exercises that your long run or faster workout is compromised, tone down the intensity. You can lift less weight or reduce the number of reps or sets that you're completing. But no matter what, remember that strength exercises should make your running better.\n",
      "\n",
      "Question: Which ones do you recommend?\n",
      "Rewrite: What strength exercises do you recommend to mix with running?\n",
      "Response: Strength Training increases your running speed Weight lifting helps you build strength or improve your muscle strength, which helps you run faster. Your legs get more robust with these exercises. You can cover a longer distance with each stride if you have stronger legs. Sufficient muscle strength will act as a foundation for power training, which depends on this strength and converts it into speed. Specialized weight training improves your running performance. Half Marathon Training with Weight Lifting You need at least two weight training sessions per week. Here is a list of great strength training exercises for runners. Compound workouts such as Trap Bar and Squats and single-leg workouts such as lunges are great ways to work out multiple muscle groups at the same time. The lunge matrix is a great strength training exercise that can be done with or without weights. To start, your body's weight is sufficient enough as a great starting point for the lunge matrix. Take complete rest for at least 45 seconds up to 3 minutes between sets. Strength training needs a longer rest period because it is more exhausting and demanding on your overall body.\n",
      "\n",
      "Question: Awesome. Can I do these and run on the same day?\n",
      "Rewrite: Awesome. Can I do these strength routines and run on the same day?\n",
      "Response: You can combine your strength training and easy run or long run sessions on the same day. Give yourself a 6-hour break between the exercises if possible. Remember that running is a priority and when push comes to shove you need to select runner-specific training (i.e. running) over strength training and cross-training. However, I would make it mandatory that you strength train at least once per week.\n",
      "\n",
      "Example #3:\n",
      "Question: What was the basis of the Watergate scandal?\n",
      "Rewrite: What was the basis of the Watergate scandal?\n",
      "Response: The Watergate Scandal erupted due to the Nixon administration's attempts to cover up its involvement in the break-in at the Democratic National Committee (DNC) on June 17, 1972, in the headquarters of the party's office at the Watergate complex in Washington, D.C. Details of the Watergate Scandal were exposed in investigations prompted by the media, especially by two reporters from the Washington Post, Bob Woodward and Carl Bernstein, along with their mysterious informant, \"Deep Throat\". The scandal surrounding the Watergate break-in and the subsequent cover-up by Nixon and his aides culminated in the president's resignation on August 9, 1974.\n",
      "\n",
      "Question: Oh wow. Why did they break in?\n",
      "Rewrite: Oh wow. Why did the Nixon administration break in at Democratic National Committee?\n",
      "Response: In June 1972, Nixon authorized members of his secret unit, as well as CREEP, to pay a team of burglars to infiltrate the Democratic Party's headquarters at the Watergate complex. This was before the election, and the burglars were meant to acquire information on his opponents. These undercover individuals broke into several Democratic offices within the Watergate and installed wire taps and recording equipment. Unfortunately for Nixon, the equipment was installed haphazardly and had to be readjusted. As the intruders attempted to return to Watergate and resolve the problem, they were arrested. The White House immediately attempted to downplay the break-in, but behind the scenes, Nixon and his secret team were quickly using funds to pay for the silence and cooperation of the intruders. Nixon also made sure that the FBI remained out of the incident by forcing the Central Intelligence Agency (CIA) to persuade the intelligence community that the break-in was classified as a matter of national security. The Nation Reacts While Nixon thought he was able to successfully cover up the break-in at the Watergate complex, members of the press, Congress and legal experts began questioning the event. By January 1973, the Watergate intruders stood trial for their participation in the break-in.\n",
      "\n",
      "Question: What happened next?\n",
      "Rewrite: What happened next after the Watergate intruders stood trial for their participation in the break-in?\n",
      "Response: The presiding judge, John Sirica, was unconvinced by the intruders' testimony that they acted solely in their own interest and that there was no connection to Nixon. Meanwhile, Congress intervened when the Senate organized the Watergate Committee to investigate the event. Needless to say, the walls were closing in around Nixon, and it became much worse. John Mitchell, the head of CREEP, was convicted of engaging in illegal activities by federal prosecutors. When that information became public, Judge Sirica increased the pressure on the Watergate intruders. In March, Sirica finally achieved his desired outcome when one of the intruders released information linking CREEP to the break-in. The dominoes were now falling one by one. In May, another convicted intruder testified before the Watergate Committee and revealed additional information linking not only CREEP, but the Nixon Administration to the events at Watergate. Eventually, the Watergate Committee called on John Dean, who was Nixon's lead legal counsel, who testified to Nixon's involvement in the break-in and cover-up. Simultaneously, the one piece of evidence that Congress, as well as the federal prosecution, needed was Nixon's taped conversations in the Oval Office.\n",
      "\n",
      "Question: Did they get it?\n",
      "Rewrite: Did Congress and federal prosecution get Nixon's taped conversations in the Oval Office?\n",
      "Response: Nixon initially refused to turn over the tapes claiming executive privilege and instead campaigned for the Stennis Compromise, which called for Senator John Stennis, who was essentially deaf, to translate the tapes for Congress! Judge Sirica and Congress both refused the compromise and instead sent a special prosecutor for the case, Archibald Cox, to present Nixon with a subpoena for the tapes. In another outlandish move, Nixon used his executive powers to terminate Cox as a prosecutor in what became known as the ' Saturday Night Massacre .' Nixon Surrenders Nixon's intransigence and outright defiance failed to pay off. While Nixon attempted to release sanitized versions of his tapes, many had had enough. On July 24, 1974, the United States Supreme Court ruled in the case of U.S. v. Nixon that Nixon's use of executive privilege was not applicable in a criminal investigation. Within weeks of the decision, the House of Representatives approved the impeachment of Nixon on the grounds of obstruction of justice, violation of the constitution and the unlawful withholding of evidence. To make matters worse, a tape was released, known as the 'smoking gun,' which unveiled Nixon agreeing to a CIA cover-up of the Watergate break-in during a phone conversation in 1972.\n",
      "\n",
      "Question: So what happened to Nixon?\n",
      "Rewrite: So what happened to Nixon after the events of the Watergate scandal?\n",
      "Response: With the mounting evidence and loss of both political and public support, Nixon resigned as president on August 9, 1974. Notably, his successor, President Gerald Ford, pardoned Nixon for his involvement in Watergate soon after his resignation.\n",
      "\n",
      "Question: What effects did this all have on the country's politics?\n",
      "Rewrite: What effects did the Watergate scandal have on US politics?\n",
      "Response: The Watergate scandal not only had a profound impact on America but also on the world. Due to this scandal, the republican party suffered and moved to the right on the political scale. Congress tried to limit presidential power in an effort to ensure a similar scandal never occurs again. Further, the American people also grew weary of big government. Further, as a result of Watergate, the presidential office was weakened, allowing the Soviets to expand. In addition, Watergate also led to the North Vietnamese regaining territory because the president was too busy with Watergate. Not only did this scandal affect the world, but it also hit the homeland hard. The Republican party was severely damaged; the American people lost all trust in Republicans. The Democrats gained 49 seats in the House and five in the Senate. The scandal helped wipe out a generation of Republican politicians. Leading to a democratic majority, which advocated less government, lower taxes, and more libertarian views. Another element that came out of Watergate was Congress's attempt to limit the campaign funding of presidents. Congress attempted to make the government more transparent with new laws and amending the Freedom of Information Act. Additionally, they passed the War Powers Act in 1973 to limit the president's ability to wage war.\n",
      "\n",
      "Question: What about Nixon's legacy?\n",
      "Rewrite: What effects did the Watergate scandal have on President Nixon's legacy?\n",
      "Response: Nixon signed into law Title IX, a section of the Education Amendment that says, \"No person in the United States shall based on sex, be excluded from participation in, be denied the benefits of, or be subjected to discrimination under any education program or activity receiving Federal financial assistance.\" The 40th anniversary of the law was celebrated this year. ABC's Serena Marshall reported on that anniversary: \"Since its passage, girls' participation in sports has gone from 1 in 27 to 2 in 5 at the high school level.\" Bostock said this and Nixon's other positive accomplishments did enough to outweigh the failures of his administration. \"I think if you look back -- and what better time to look back on a man's life than on the 100th anniversary of his birth? -- at the totality of his record and the influence he had on the country and the world, it was really extraordinary and on balance for the good,\" he said. Fulsom disagreed, saying Nixon's greatest legacy was one of \"distrust in the president and the government.\" \"I think he left definitely a negative impression on all those who lived during the time, and it continues to this day,\" Fulsom said. \"Just about every book that's ever been written about him paints a rather dim portrait as a leader.\" Love him or hate him, there's no denying, President Richard Nixon changed the course of American history.\n"
     ]
    }
   ],
   "source": [
    "print(prompter.demo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
